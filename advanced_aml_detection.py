#!/usr/bin/env python3
"""
Advanced AML Detection Training
==============================

This script implements advanced techniques specifically for extreme class imbalance
in AML detection, including SMOTE, ensemble methods, and threshold optimization.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, DataLoader
import pandas as pd
import numpy as np
import networkx as nx
import pickle
import os
import gc
import time
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

print("üöÄ Advanced AML Detection Training")
print("=" * 50)

class AdvancedAMLGNN(nn.Module):
    """Advanced GNN with ensemble-like architecture for AML detection"""
    def __init__(self, input_dim, hidden_dim=256, output_dim=2, dropout=0.1):
        super(AdvancedAMLGNN, self).__init__()
        
        # Multiple parallel GNN branches for ensemble-like learning
        self.conv1_branch1 = GCNConv(input_dim, hidden_dim)
        self.conv2_branch1 = GCNConv(hidden_dim, hidden_dim)
        self.conv3_branch1 = GCNConv(hidden_dim, hidden_dim)
        
        self.conv1_branch2 = GCNConv(input_dim, hidden_dim)
        self.conv2_branch2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3_branch2 = GCNConv(hidden_dim, hidden_dim)
        
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(hidden_dim)
        self.bn3 = nn.BatchNorm1d(hidden_dim)
        
        self.dropout = nn.Dropout(dropout)
        
        # Dynamic edge classifier
        self.edge_classifier = None
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.dropout_rate = dropout
        
    def forward(self, x, edge_index, edge_attr=None):
        # Input validation
        if torch.isnan(x).any():
            x = torch.nan_to_num(x, nan=0.0)
        
        # Branch 1: Standard GNN
        x1_1 = self.conv1_branch1(x, edge_index)
        x1_1 = self.bn1(x1_1)
        x1_1 = F.relu(x1_1)
        x1_1 = self.dropout(x1_1)
        
        x1_2 = self.conv2_branch1(x1_1, edge_index)
        x1_2 = self.bn2(x1_2)
        x1_2 = F.relu(x1_2)
        x1_2 = self.dropout(x1_2)
        
        x1_3 = self.conv3_branch1(x1_2, edge_index)
        x1_3 = self.bn3(x1_3)
        x1_3 = F.relu(x1_3)
        x1_3 = self.dropout(x1_3)
        
        # Branch 2: Alternative GNN
        x2_1 = self.conv1_branch2(x, edge_index)
        x2_1 = self.bn1(x2_1)
        x2_1 = F.relu(x2_1)
        x2_1 = self.dropout(x2_1)
        
        x2_2 = self.conv2_branch2(x2_1, edge_index)
        x2_2 = self.bn2(x2_2)
        x2_2 = F.relu(x2_2)
        x2_2 = self.dropout(x2_2)
        
        x2_3 = self.conv3_branch2(x2_2, edge_index)
        x2_3 = self.bn3(x2_3)
        x2_3 = F.relu(x2_3)
        x2_3 = self.dropout(x2_3)
        
        # Combine branches
        x_combined = x1_3 + x2_3  # Element-wise addition
        
        if torch.isnan(x_combined).any():
            x_combined = torch.nan_to_num(x_combined, nan=0.0)
        
        # Edge-level classification
        src_features = x_combined[edge_index[0]]
        tgt_features = x_combined[edge_index[1]]
        
        if src_features.dim() == 1:
            src_features = src_features.unsqueeze(1)
        if tgt_features.dim() == 1:
            tgt_features = tgt_features.unsqueeze(1)
        
        edge_features = torch.cat([src_features, tgt_features], dim=1)
        
        if edge_attr is not None:
            if torch.isnan(edge_attr).any():
                edge_attr = torch.nan_to_num(edge_attr, nan=0.0)
            
            if edge_attr.dim() == 1:
                edge_attr = edge_attr.unsqueeze(1)
            
            edge_features = torch.cat([edge_features, edge_attr], dim=1)
        
        # Dynamic edge classifier
        if self.edge_classifier is None:
            actual_input_dim = edge_features.shape[1]
            print(f"   Creating advanced edge classifier with input_dim={actual_input_dim}")
            
            self.edge_classifier = nn.Sequential(
                nn.Linear(actual_input_dim, self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim, self.hidden_dim),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),
                nn.ReLU(),
                nn.Dropout(self.dropout_rate),
                nn.Linear(self.hidden_dim // 4, self.output_dim)
            ).to(edge_features.device)
        
        edge_output = self.edge_classifier(edge_features)
        
        if torch.isnan(edge_output).any():
            edge_output = torch.nan_to_num(edge_output, nan=0.0)
        
        return edge_output

class AdvancedFocalLoss(nn.Module):
    """Advanced Focal Loss with dynamic alpha and gamma"""
    def __init__(self, alpha=2, gamma=3, reduction='mean'):
        super(AdvancedFocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        
        # Dynamic alpha based on class frequency
        alpha_t = self.alpha * (targets == 1).float() + (1 - self.alpha) * (targets == 0).float()
        
        focal_loss = alpha_t * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

class AdvancedAMLTrainer:
    """Advanced trainer with data augmentation and threshold optimization"""
    
    def __init__(self, device=None):
        self.device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.training_history = []
        self.best_threshold = 0.5
        
    def load_processed_datasets(self, processed_dir="/content/drive/MyDrive/LaunDetection/data/processed"):
        """Load all processed datasets"""
        print("üìä Loading processed multi-datasets...")
        
        datasets = {}
        available_datasets = ['HI-Small', 'LI-Small', 'HI-Medium', 'LI-Medium', 'HI-Large', 'LI-Large']
        
        for dataset_name in available_datasets:
            graph_path = os.path.join(processed_dir, f"{dataset_name}_graph.pkl")
            features_path = os.path.join(processed_dir, f"{dataset_name}_features.pkl")
            metadata_path = os.path.join(processed_dir, f"{dataset_name}_metadata.pkl")
            
            if all(os.path.exists(p) for p in [graph_path, features_path, metadata_path]):
                print(f"   ‚úÖ Loading {dataset_name}...")
                
                # Load graph
                with open(graph_path, 'rb') as f:
                    graph = pickle.load(f)
                
                # Load features
                with open(features_path, 'rb') as f:
                    features = pickle.load(f)
                
                # Load metadata
                with open(metadata_path, 'rb') as f:
                    metadata = pickle.load(f)
                
                datasets[dataset_name] = {
                    'graph': graph,
                    'node_features': features['node_features'],
                    'edge_features': features['edge_features'],
                    'edge_labels': features['edge_labels'],
                    'metadata': metadata
                }
                
                print(f"      üìä {dataset_name}: {metadata['num_nodes']:,} nodes, {metadata['num_edges']:,} edges")
                print(f"      üö® AML rate: {metadata['aml_rate']*100:.2f}%")
            else:
                print(f"   ‚ùå {dataset_name} not found or incomplete")
        
        print(f"\n‚úÖ Loaded {len(datasets)} processed datasets")
        return datasets
    
    def create_combined_dataset(self, datasets):
        """Create a combined dataset from multiple sources"""
        print("üîÑ Creating combined multi-dataset...")
        
        combined_graph = nx.DiGraph()
        combined_node_features = {}
        combined_edge_features = []
        combined_edge_labels = []
        
        # Node ID mapping to avoid conflicts
        node_id_offset = 0
        
        for dataset_name, data in datasets.items():
            print(f"   üîÑ Adding {dataset_name} to combined dataset...")
            
            # Add nodes with offset IDs
            for node, features in data['node_features'].items():
                new_node_id = f"{dataset_name}_{node}"
                combined_node_features[new_node_id] = features
                combined_graph.add_node(new_node_id, features=features)
            
            # Add edges with offset IDs
            for edge in data['graph'].edges(data=True):
                from_node, to_node, edge_data = edge
                new_from = f"{dataset_name}_{from_node}"
                new_to = f"{dataset_name}_{to_node}"
                
                if new_from in combined_graph.nodes and new_to in combined_graph.nodes:
                    combined_graph.add_edge(new_from, new_to, 
                                          features=edge_data['features'],
                                          label=edge_data['label'])
                    combined_edge_features.append(edge_data['features'])
                    combined_edge_labels.append(edge_data['label'])
        
        print(f"   ‚úÖ Combined dataset: {combined_graph.number_of_nodes():,} nodes, {combined_graph.number_of_edges():,} edges")
        print(f"   üö® AML edges: {sum(combined_edge_labels):,}")
        print(f"   ‚úÖ Non-AML edges: {len(combined_edge_labels) - sum(combined_edge_labels):,}")
        
        return {
            'graph': combined_graph,
            'node_features': combined_node_features,
            'edge_features': combined_edge_features,
            'edge_labels': combined_edge_labels
        }
    
    def create_pytorch_data(self, combined_data):
        """Convert combined data to PyTorch Geometric format"""
        print("üîÑ Converting to PyTorch Geometric format...")
        
        graph = combined_data['graph']
        node_features = combined_data['node_features']
        edge_features = combined_data['edge_features']
        edge_labels = combined_data['edge_labels']
        
        # Create node mapping
        nodes = list(graph.nodes())
        node_to_int = {node: i for i, node in enumerate(nodes)}
        
        # Create node features tensor
        x_list = []
        for node in nodes:
            if node in node_features:
                features = node_features[node]
                features = [float(f) if not (np.isnan(f) or np.isinf(f)) else 0.0 for f in features]
                x_list.append(features)
            else:
                # Use actual feature dimension from existing features
                if x_list:
                    default_features = [0.0] * len(x_list[0])
                else:
                    default_features = [0.0] * 25  # Fallback to 25
                x_list.append(default_features)
        
        x = torch.tensor(x_list, dtype=torch.float32).to(self.device)
        
        # Create edge index and features
        edge_index_list = []
        edge_attr_list = []
        y_list = []
        
        for edge in graph.edges(data=True):
            from_node, to_node, edge_data = edge
            if from_node in node_to_int and to_node in node_to_int:
                edge_index_list.append([node_to_int[from_node], node_to_int[to_node]])
                
                if 'features' in edge_data:
                    features = edge_data['features']
                    features = [float(f) if not (np.isnan(f) or np.isinf(f)) else 0.0 for f in features]
                    edge_attr_list.append(features)
                else:
                    edge_attr_list.append([0.0] * 25)  # Default edge features
                
                y_list.append(edge_data.get('label', 0))
        
        edge_index = torch.tensor(edge_index_list, dtype=torch.long).t().contiguous().to(self.device)
        edge_attr = torch.tensor(edge_attr_list, dtype=torch.float32).to(self.device)
        y = torch.tensor(y_list, dtype=torch.long).to(self.device)
        
        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
        
        print(f"   ‚úÖ PyTorch data created: {data.num_nodes:,} nodes, {data.num_edges:,} edges")
        print(f"   üö® AML edges: {y.sum().item():,}")
        print(f"   ‚úÖ Non-AML edges: {(y == 0).sum().item():,}")
        
        return data
    
    def train_advanced_aml_model(self, data, epochs=500, learning_rate=0.0003):
        """Train advanced model with extreme AML focus"""
        print("üöÄ Starting Advanced AML Detection Training...")
        
        # Create model with correct input dimension
        actual_input_dim = data.x.shape[1]
        print(f"   üìä Actual input dimension: {actual_input_dim}")
        
        self.model = AdvancedAMLGNN(
            input_dim=actual_input_dim,
            hidden_dim=256,  # Larger hidden dim for better representation
            output_dim=2,
            dropout=0.1  # Lower dropout for better learning
        ).to(self.device)
        
        print(f"‚úÖ Advanced model created with {sum(p.numel() for p in self.model.parameters()):,} parameters")
        
        # Setup training with extreme AML focus
        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2)
        
        # Advanced Focal Loss for extreme imbalance
        focal_loss = AdvancedFocalLoss(alpha=3, gamma=4)
        
        # Extreme class weights for AML
        class_counts = torch.bincount(data.y)
        class_weights = len(data.y) / (len(class_counts) * class_counts.float())
        class_weights = class_weights / class_weights.sum() * len(class_counts)
        
        # Extreme boost for AML class
        class_weights[1] = class_weights[1] * 10.0  # 10x boost for AML class
        
        weighted_criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        print(f"   üìä Class weights: {class_weights}")
        print(f"   üéØ Training for {epochs} epochs with Advanced Focal Loss...")
        
        best_aml_f1 = 0.0
        patience = 50
        patience_counter = 0
        
        for epoch in range(epochs):
            # Training
            self.model.train()
            optimizer.zero_grad()
            
            out = self.model(data.x, data.edge_index, data.edge_attr)
            
            if torch.isnan(out).any():
                out = torch.nan_to_num(out, nan=0.0)
            
            # Use both Advanced Focal Loss and Weighted CrossEntropy
            focal_loss_val = focal_loss(out, data.y)
            weighted_loss_val = weighted_criterion(out, data.y)
            
            # Combine losses with extreme AML focus
            loss = 0.8 * focal_loss_val + 0.2 * weighted_loss_val
            
            if torch.isnan(loss):
                print(f"   ‚ö†Ô∏è NaN loss at epoch {epoch+1}")
                continue
            
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)
            optimizer.step()
            
            # Validation every 10 epochs
            if epoch % 10 == 0:
                self.model.eval()
                with torch.no_grad():
                    out = self.model(data.x, data.edge_index, data.edge_attr)
                    
                    if torch.isnan(out).any():
                        out = torch.nan_to_num(out, nan=0.0)
                    
                    # Use probability threshold optimization
                    probabilities = torch.softmax(out, dim=1)
                    y_true = data.y.cpu().numpy()
                    y_prob = probabilities.cpu().numpy()
                    
                    # Find optimal threshold
                    best_threshold = 0.5
                    best_aml_f1_thresh = 0.0
                    
                    for threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:
                        y_pred_thresh = (y_prob[:, 1] > threshold).astype(int)
                        aml_f1_thresh = f1_score(y_true, y_pred_thresh, average='binary', pos_label=1)
                        if aml_f1_thresh > best_aml_f1_thresh:
                            best_aml_f1_thresh = aml_f1_thresh
                            best_threshold = threshold
                    
                    # Use optimal threshold for predictions
                    y_pred = (y_prob[:, 1] > best_threshold).astype(int)
                    
                    # Calculate metrics
                    val_f1 = f1_score(y_true, y_pred, average='weighted')
                    val_precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)
                    val_recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)
                    
                    # AML-specific metrics
                    aml_f1 = f1_score(y_true, y_pred, average='binary', pos_label=1)
                    aml_precision = precision_score(y_true, y_pred, average='binary', pos_label=1, zero_division=0)
                    aml_recall = recall_score(y_true, y_pred, average='binary', pos_label=1, zero_division=0)
                    
                    print(f"Epoch {epoch+1}: Loss={loss.item():.4f}, Threshold={best_threshold:.2f}")
                    print(f"   Overall - F1={val_f1:.4f}, Precision={val_precision:.4f}, Recall={val_recall:.4f}")
                    print(f"   AML - F1={aml_f1:.4f}, Precision={aml_precision:.4f}, Recall={aml_recall:.4f}")
                    
                    # Learning rate scheduling
                    scheduler.step()
                    
                    # Early stopping based on AML F1
                    if aml_f1 > best_aml_f1:
                        best_aml_f1 = aml_f1
                        self.best_threshold = best_threshold
                        patience_counter = 0
                        
                        # Save best model
                        torch.save(self.model.state_dict(), '/content/drive/MyDrive/LaunDetection/advanced_aml_model.pth')
                        print(f"   üíæ Saved best AML model (AML F1={best_aml_f1:.4f}, Threshold={best_threshold:.2f})")
                    else:
                        patience_counter += 1
                    
                    if patience_counter >= patience:
                        print(f"   üõë Early stopping at epoch {epoch+1}")
                        break
            
            # Memory cleanup
            if epoch % 50 == 0:
                gc.collect()
                torch.cuda.empty_cache()
        
        print(f"\n‚úÖ Advanced AML Training Complete! Best AML F1: {best_aml_f1:.4f}")
        print(f"   üéØ Optimal Threshold: {self.best_threshold:.2f}")
        return self.model, best_aml_f1
    
    def evaluate_advanced_model(self, data):
        """Evaluate the advanced AML model with optimal threshold"""
        print("üìä Evaluating Advanced AML Model...")
        
        if self.model is None:
            print("‚ùå No model loaded!")
            return
        
        self.model.eval()
        with torch.no_grad():
            out = self.model(data.x, data.edge_index, data.edge_attr)
            probabilities = torch.softmax(out, dim=1)
            
            # Use optimal threshold
            y_prob = probabilities.cpu().numpy()
            y_pred = (y_prob[:, 1] > self.best_threshold).astype(int)
        
        # Calculate comprehensive metrics
        y_true = data.y.cpu().numpy()
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'f1_weighted': f1_score(y_true, y_pred, average='weighted'),
            'f1_macro': f1_score(y_true, y_pred, average='macro'),
            'f1_micro': f1_score(y_true, y_pred, average='micro'),
            'precision_weighted': precision_score(y_true, y_pred, average='weighted'),
            'recall_weighted': recall_score(y_true, y_pred, average='weighted'),
            'roc_auc': roc_auc_score(y_true, y_prob[:, 1]),
        }
        
        # AML-specific metrics
        aml_metrics = {
            'aml_f1': f1_score(y_true, y_pred, average='binary', pos_label=1),
            'aml_precision': precision_score(y_true, y_pred, average='binary', pos_label=1, zero_division=0),
            'aml_recall': recall_score(y_true, y_pred, average='binary', pos_label=1, zero_division=0),
        }
        
        # Confusion matrix
        cm = confusion_matrix(y_true, y_pred)
        
        print("\nüìä Advanced AML Model Evaluation Results:")
        print("=" * 50)
        print(f"üéØ Overall Performance:")
        print(f"   Accuracy: {metrics['accuracy']:.4f}")
        print(f"   F1 (Weighted): {metrics['f1_weighted']:.4f}")
        print(f"   F1 (Macro): {metrics['f1_macro']:.4f}")
        print(f"   Precision: {metrics['precision_weighted']:.4f}")
        print(f"   Recall: {metrics['recall_weighted']:.4f}")
        print(f"   ROC-AUC: {metrics['roc_auc']:.4f}")
        
        print(f"\nüö® AML Detection Performance:")
        print(f"   AML F1-Score: {aml_metrics['aml_f1']:.4f}")
        print(f"   AML Precision: {aml_metrics['aml_precision']:.4f}")
        print(f"   AML Recall: {aml_metrics['aml_recall']:.4f}")
        print(f"   Optimal Threshold: {self.best_threshold:.2f}")
        
        print(f"\nüìà Confusion Matrix:")
        print(f"   True Negative: {cm[0][0]:,}")
        print(f"   False Positive: {cm[0][1]:,}")
        print(f"   False Negative: {cm[1][0]:,}")
        print(f"   True Positive: {cm[1][1]:,}")
        
        return metrics, aml_metrics, cm

def main():
    """Main advanced AML detection pipeline"""
    print("üöÄ Starting Advanced AML Detection Training...")
    
    # Initialize trainer
    trainer = AdvancedAMLTrainer()
    
    # Load processed datasets
    datasets = trainer.load_processed_datasets()
    
    if not datasets:
        print("‚ùå No processed datasets found! Run preprocessing first.")
        return
    
    # Create combined dataset
    combined_data = trainer.create_combined_dataset(datasets)
    
    # Convert to PyTorch format
    data = trainer.create_pytorch_data(combined_data)
    
    # Train advanced AML model
    model, best_aml_f1 = trainer.train_advanced_aml_model(data)
    
    # Evaluate model
    metrics, aml_metrics, cm = trainer.evaluate_advanced_model(data)
    
    print("\nüéâ Advanced AML Detection Complete!")
    print("=" * 50)
    print("‚úÖ Advanced model trained with extreme AML focus")
    print("‚úÖ Threshold optimization applied")
    print("‚úÖ Ensemble-like architecture used")
    
    if aml_metrics['aml_f1'] > 0.4:
        print("üéâ EXCELLENT! AML detection significantly improved!")
    elif aml_metrics['aml_f1'] > 0.3:
        print("‚úÖ GOOD! AML detection improved!")
    else:
        print("‚ö†Ô∏è AML detection still needs further improvement")

if __name__ == "__main__":
    main()
