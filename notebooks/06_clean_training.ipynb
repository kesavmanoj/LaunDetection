{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AML Multi-GNN - Clean Training Script\n",
        "\n",
        "A robust, bug-free training script for Multi-GNN AML detection.\n",
        "**No synthetic data - only real data training with proper error handling.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "AML Multi-GNN - Clean Training Script\n",
        "=====================================\n",
        "\n",
        "A robust, bug-free training script for Multi-GNN AML detection.\n",
        "No synthetic data - only real data training with proper error handling.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device with proper error handling\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device with proper error handling\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            # Test CUDA with a simple operation\n",
        "            test_tensor = torch.tensor([1.0]).cuda()\n",
        "            _ = test_tensor + 1\n",
        "            torch.cuda.empty_cache()\n",
        "            return torch.device('cuda')\n",
        "        except Exception as e:\n",
        "            print(f\"CUDA test failed: {e}\")\n",
        "            print(\"Falling back to CPU\")\n",
        "            return torch.device('cpu')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Clear GPU memory\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(\"GPU memory cleared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleGNN(nn.Module):\n",
        "    \"\"\"Simple, robust GNN for AML detection\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):\n",
        "        super(SimpleGNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Input layer\n",
        "        self.input_conv = GCNConv(input_dim, hidden_dim)\n",
        "        \n",
        "        # Hidden layers\n",
        "        self.hidden_convs = nn.ModuleList()\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.hidden_convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        \n",
        "        # Output layer\n",
        "        self.output_conv = GCNConv(hidden_dim, output_dim)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        \"\"\"Forward pass with proper device handling\"\"\"\n",
        "        # Ensure all inputs are on the same device\n",
        "        x = x.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "        \n",
        "        # Input layer\n",
        "        x = F.relu(self.input_conv(x, edge_index))\n",
        "        x = self.dropout_layer(x)\n",
        "        \n",
        "        # Hidden layers\n",
        "        for conv in self.hidden_convs:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = self.dropout_layer(x)\n",
        "        \n",
        "        # Output layer\n",
        "        x = self.output_conv(x, edge_index)\n",
        "        \n",
        "        # CRITICAL FIX: Always use global pooling for graph-level classification\n",
        "        if batch is not None:\n",
        "            batch = batch.to(device)\n",
        "            x = global_mean_pool(x, batch)\n",
        "        else:\n",
        "            # If no batch provided, assume single graph and use mean pooling\n",
        "            x = x.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        return x\n",
        "\n",
        "print(\"SimpleGNN model class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_real_data(data_path):\n",
        "    \"\"\"Load real AML data with proper error handling - NO SYNTHETIC DATA\"\"\"\n",
        "    print(\"Loading real AML data...\")\n",
        "    \n",
        "    # Check for real data files\n",
        "    real_files = []\n",
        "    for file in os.listdir(data_path):\n",
        "        if file.endswith('.csv'):\n",
        "            real_files.append(file)\n",
        "    \n",
        "    print(f\"Found {len(real_files)} CSV files in {data_path}\")\n",
        "    for file in real_files:\n",
        "        print(f\"  - {file}\")\n",
        "    \n",
        "    if not real_files:\n",
        "        raise ValueError(\"NO REAL DATA FOUND! Please ensure your data is in Google Drive at /content/drive/MyDrive/LaunDetection/data/raw/\")\n",
        "    \n",
        "    # Use HI-Small dataset for better performance\n",
        "    hi_small_trans = None\n",
        "    hi_small_accounts = None\n",
        "    \n",
        "    for file in real_files:\n",
        "        if 'HI-Small_Trans' in file:\n",
        "            hi_small_trans = file\n",
        "        elif 'HI-Small_accounts' in file:\n",
        "            hi_small_accounts = file\n",
        "    \n",
        "    if hi_small_trans:\n",
        "        transactions_file = os.path.join(data_path, hi_small_trans)\n",
        "        print(f\"Loading transactions from {transactions_file}\")\n",
        "        transactions = pd.read_csv(transactions_file, nrows=2000)  # Limit for memory\n",
        "        print(f\"Loaded {len(transactions)} transactions\")\n",
        "    else:\n",
        "        # Fallback to first file\n",
        "        transactions_file = os.path.join(data_path, real_files[0])\n",
        "        print(f\"Loading transactions from {transactions_file}\")\n",
        "        transactions = pd.read_csv(transactions_file, nrows=2000)  # Limit for memory\n",
        "        print(f\"Loaded {len(transactions)} transactions\")\n",
        "    \n",
        "    if hi_small_accounts:\n",
        "        accounts_file = os.path.join(data_path, hi_small_accounts)\n",
        "        print(f\"Loading accounts from {accounts_file}\")\n",
        "        accounts = pd.read_csv(accounts_file, nrows=1000)  # Limit for memory\n",
        "        print(f\"Loaded {len(accounts)} accounts\")\n",
        "    else:\n",
        "        # Use only real data - extract accounts from transaction data\n",
        "        print(\"Extracting real accounts from transaction data...\")\n",
        "        all_accounts = set(transactions['From Bank'].tolist() + transactions['To Bank'].tolist())\n",
        "        \n",
        "        # Create accounts using real bank names from transactions\n",
        "        accounts_data = {\n",
        "            'Account Number': list(all_accounts),\n",
        "            'Bank Name': list(all_accounts),  # Use real bank names\n",
        "            'Bank ID': [f'B{hash(name) % 10000}' for name in all_accounts],  # Generate IDs from bank names\n",
        "            'Entity ID': [f'E{hash(name) % 10000}' for name in all_accounts],\n",
        "            'Entity Name': [f'Entity_{name}' for name in all_accounts]\n",
        "        }\n",
        "        accounts = pd.DataFrame(accounts_data)\n",
        "        print(f\"Extracted {len(accounts)} real accounts from transaction data\")\n",
        "    \n",
        "    return transactions, accounts\n",
        "\n",
        "# Removed synthetic data creation functions - using only real data\n",
        "\n",
        "print(\"Data loading functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_graph_from_data(transactions, accounts):\n",
        "    \"\"\"Create graph from transaction and account data - REAL DATA ONLY\"\"\"\n",
        "    print(\"Creating graph from real data...\")\n",
        "    \n",
        "    # Handle different column names in real data\n",
        "    print(\"Transaction columns:\", transactions.columns.tolist())\n",
        "    print(\"Account columns:\", accounts.columns.tolist())\n",
        "    \n",
        "    # Map common column names\n",
        "    from_col = None\n",
        "    to_col = None\n",
        "    amount_col = None\n",
        "    timestamp_col = None\n",
        "    sar_col = None\n",
        "    \n",
        "    # Find transaction columns\n",
        "    for col in transactions.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'from' in col_lower or 'sender' in col_lower:\n",
        "            from_col = col\n",
        "        elif 'to' in col_lower or 'receiver' in col_lower:\n",
        "            to_col = col\n",
        "        elif 'amount' in col_lower or 'value' in col_lower:\n",
        "            amount_col = col\n",
        "        elif 'time' in col_lower or 'date' in col_lower:\n",
        "            timestamp_col = col\n",
        "        elif 'laundering' in col_lower or 'sar' in col_lower or 'suspicious' in col_lower or 'label' in col_lower:\n",
        "            sar_col = col\n",
        "    \n",
        "    print(f\"Using columns: from={from_col}, to={to_col}, amount={amount_col}, time={timestamp_col}, sar={sar_col}\")\n",
        "    \n",
        "    # Create node features - handle real IBM AML data structure\n",
        "    account_features = {}\n",
        "    \n",
        "    # Map account columns to our expected format\n",
        "    account_id_col = None\n",
        "    balance_col = None\n",
        "    risk_col = None\n",
        "    type_col = None\n",
        "    \n",
        "    # Find account ID column\n",
        "    for col in accounts.columns:\n",
        "        col_lower = col.lower()\n",
        "        if 'account' in col_lower and 'number' in col_lower:\n",
        "            account_id_col = col\n",
        "        elif 'balance' in col_lower or 'amount' in col_lower:\n",
        "            balance_col = col\n",
        "        elif 'risk' in col_lower or 'score' in col_lower:\n",
        "            risk_col = col\n",
        "        elif 'type' in col_lower or 'entity' in col_lower:\n",
        "            type_col = col\n",
        "    \n",
        "    print(f\"Account column mapping: id={account_id_col}, balance={balance_col}, risk={risk_col}, type={type_col}\")\n",
        "    \n",
        "    for _, account in accounts.iterrows():\n",
        "        # Use Account Number as account_id\n",
        "        account_id = account[account_id_col] if account_id_col else account.iloc[0]\n",
        "        \n",
        "        # Extract features with defaults\n",
        "        balance = float(account[balance_col]) if balance_col and pd.notna(account[balance_col]) else 5000.0\n",
        "        risk_score = float(account[risk_col]) if risk_col and pd.notna(account[risk_col]) else 0.5\n",
        "        account_type = str(account[type_col]) if type_col and pd.notna(account[type_col]) else 'checking'\n",
        "        \n",
        "        account_features[account_id] = [\n",
        "            balance,\n",
        "            risk_score,\n",
        "            1 if account_type.lower() == 'checking' else 0,\n",
        "            1 if account_type.lower() == 'savings' else 0,\n",
        "            1 if account_type.lower() == 'business' else 0\n",
        "        ]\n",
        "    \n",
        "    # Create edge features and labels\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    labels = []\n",
        "    \n",
        "    print(f\"Processing {len(transactions)} transactions...\")\n",
        "    print(f\"Account features keys: {list(account_features.keys())[:5]}...\")  # Show first 5 account IDs\n",
        "    \n",
        "    matched_edges = 0\n",
        "    for i, (_, transaction) in enumerate(transactions.iterrows()):\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Processing transaction {i}/{len(transactions)}\")\n",
        "            \n",
        "        # Handle the specific IBM AML data structure\n",
        "        from_acc = transaction[from_col] if from_col else transaction.iloc[0]\n",
        "        to_acc = transaction[to_col] if to_col else transaction.iloc[1]\n",
        "        \n",
        "        # Use Amount Paid as the transaction amount\n",
        "        amount = float(transaction[amount_col]) if amount_col and pd.notna(transaction[amount_col]) else 1000.0\n",
        "        \n",
        "        # Handle timestamp\n",
        "        if timestamp_col and timestamp_col in transaction:\n",
        "            try:\n",
        "                if pd.api.types.is_datetime64_any_dtype(transaction[timestamp_col]):\n",
        "                    timestamp = transaction[timestamp_col]\n",
        "                else:\n",
        "                    timestamp = pd.to_datetime(transaction[timestamp_col])\n",
        "                hour = timestamp.hour\n",
        "                day = timestamp.day\n",
        "                month = timestamp.month\n",
        "            except:\n",
        "                hour, day, month = 12, 1, 1\n",
        "        else:\n",
        "            hour, day, month = 12, 1, 1\n",
        "        \n",
        "        # Handle SAR label - use ONLY real \"Is Laundering\" column\n",
        "        if sar_col and sar_col in transaction:\n",
        "            is_sar = int(transaction[sar_col])\n",
        "        else:\n",
        "            # No synthetic labels - skip if no real SAR data\n",
        "            print(f\"WARNING: No SAR label found for transaction {i}, skipping...\")\n",
        "            continue\n",
        "        \n",
        "        # Debug: Check if accounts exist\n",
        "        if i < 5:  # Debug first 5 transactions\n",
        "            print(f\"Transaction {i}: from_acc={from_acc}, to_acc={to_acc}\")\n",
        "            print(f\"  from_acc in account_features: {from_acc in account_features}\")\n",
        "            print(f\"  to_acc in account_features: {to_acc in account_features}\")\n",
        "        \n",
        "        # ONLY use real data - skip transactions with missing accounts\n",
        "        if from_acc in account_features and to_acc in account_features:\n",
        "            edges.append([from_acc, to_acc])\n",
        "            edge_features.append([amount, hour, day, month])\n",
        "            labels.append(is_sar)\n",
        "            matched_edges += 1\n",
        "        else:\n",
        "            # Skip transactions with missing accounts - use only real data\n",
        "            if i < 5:  # Debug first 5 skipped transactions\n",
        "                print(f\"  ✗ Skipped transaction {i}: missing accounts\")\n",
        "                print(f\"    from_acc '{from_acc}' in account_features: {from_acc in account_features}\")\n",
        "                print(f\"    to_acc '{to_acc}' in account_features: {to_acc in account_features}\")\n",
        "            continue\n",
        "    \n",
        "    print(f\"Matched {matched_edges} edges out of {len(transactions)} transactions\")\n",
        "    \n",
        "    # Create node feature matrix\n",
        "    unique_accounts = list(set([edge[0] for edge in edges] + [edge[1] for edge in edges]))\n",
        "    node_features = []\n",
        "    node_labels = []\n",
        "    \n",
        "    for account_id in unique_accounts:\n",
        "        if account_id in account_features:\n",
        "            node_features.append(account_features[account_id])\n",
        "            # Node label: 1 if any transaction from this account is SAR\n",
        "            account_sar = any(labels[i] for i, edge in enumerate(edges) if edge[0] == account_id)\n",
        "            node_labels.append(1 if account_sar else 0)\n",
        "        else:\n",
        "            node_features.append([0, 0, 0, 0, 0])\n",
        "            node_labels.append(0)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float32)\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_features, dtype=torch.float32)\n",
        "    labels = torch.tensor(node_labels, dtype=torch.long)\n",
        "    \n",
        "    print(f\"Created graph with {len(unique_accounts)} nodes and {len(edges)} edges\")\n",
        "    \n",
        "    # Handle division by zero\n",
        "    if len(node_labels) > 0:\n",
        "        sar_rate = sum(node_labels) / len(node_labels)\n",
        "        print(f\"SAR rate: {sar_rate:.3f}\")\n",
        "    else:\n",
        "        print(\"SAR rate: 0.000 (no nodes)\")\n",
        "    \n",
        "    # Use only real data - no synthetic fallback\n",
        "    if len(edges) == 0:\n",
        "        print(\"ERROR: No edges created from real data!\")\n",
        "        print(\"This means there's no overlap between transaction accounts and account data.\")\n",
        "        print(\"Please check your data files and ensure account IDs match.\")\n",
        "        raise ValueError(\"No real data available for training - check data files\")\n",
        "    else:\n",
        "        # Convert to tensors using only real data\n",
        "        node_features = torch.tensor(node_features, dtype=torch.float32)\n",
        "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_features, dtype=torch.float32)\n",
        "        labels = torch.tensor(node_labels, dtype=torch.long)\n",
        "    \n",
        "    return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=labels)\n",
        "\n",
        "print(\"Graph creation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train model with proper device handling\"\"\"\n",
        "    print(f\"Training model for {epochs} epochs...\")\n",
        "    \n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_val_f1 = 0.0\n",
        "    train_losses = []\n",
        "    val_f1_scores = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            # Move batch to device\n",
        "            batch = batch.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                pred = out.argmax(dim=1)\n",
        "                \n",
        "                val_preds.extend(pred.cpu().numpy())\n",
        "                val_labels.extend(batch.y.cpu().numpy())\n",
        "        \n",
        "        # Calculate metrics\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        val_precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "        val_recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "        \n",
        "        train_losses.append(total_loss / len(train_loader))\n",
        "        val_f1_scores.append(val_f1)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, \"\n",
        "              f\"Val F1={val_f1:.4f}, Val Precision={val_precision:.4f}, Val Recall={val_recall:.4f}\")\n",
        "        \n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            print(f\"New best validation F1: {best_val_f1:.4f}\")\n",
        "    \n",
        "    return best_val_f1, train_losses, val_f1_scores\n",
        "\n",
        "print(\"Training function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training execution\n",
        "print(\"=\"*60)\n",
        "print(\"AML Multi-GNN - Clean Training Script\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "transactions, accounts = load_real_data(data_path)\n",
        "\n",
        "# Create graph\n",
        "graph = create_graph_from_data(transactions, accounts)\n",
        "print(f\"Graph created: {graph.num_nodes} nodes, {graph.num_edges} edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loader\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "\n",
        "# CRITICAL FIX: Create individual graphs for each node (graph-level classification)\n",
        "print(\"Creating individual graphs for graph-level classification...\")\n",
        "\n",
        "# Create individual graphs for each node\n",
        "individual_graphs = []\n",
        "for i in range(min(100, graph.num_nodes)):  # Limit to 100 graphs for memory\n",
        "    # Create a small subgraph around each node\n",
        "    center_node = i\n",
        "    neighbor_nodes = [center_node]\n",
        "    \n",
        "    # Add some neighbors\n",
        "    for edge_idx in range(graph.edge_index.size(1)):\n",
        "        edge = graph.edge_index[:, edge_idx]\n",
        "        if edge[0].item() == center_node:\n",
        "            neighbor_nodes.append(edge[1].item())\n",
        "        elif edge[1].item() == center_node:\n",
        "            neighbor_nodes.append(edge[0].item())\n",
        "    \n",
        "    # Limit to reasonable size\n",
        "    neighbor_nodes = neighbor_nodes[:10]\n",
        "    \n",
        "    if len(neighbor_nodes) > 1:  # Only create if has neighbors\n",
        "        # Create subgraph\n",
        "        subgraph = graph.subgraph(torch.tensor(neighbor_nodes))\n",
        "        if subgraph.num_edges > 0:\n",
        "            # Create graph-level label (majority vote of node labels)\n",
        "            node_labels = subgraph.y.tolist()\n",
        "            graph_label = 1 if sum(node_labels) > len(node_labels) / 2 else 0\n",
        "            \n",
        "            # Create new graph with single label\n",
        "            new_graph = Data(\n",
        "                x=subgraph.x,\n",
        "                edge_index=subgraph.edge_index,\n",
        "                edge_attr=subgraph.edge_attr,\n",
        "                y=torch.tensor([graph_label], dtype=torch.long)\n",
        "            )\n",
        "            individual_graphs.append(new_graph)\n",
        "\n",
        "print(f\"Created {len(individual_graphs)} individual graphs\")\n",
        "\n",
        "# Split into train/val\n",
        "train_size = int(0.8 * len(individual_graphs))\n",
        "train_graphs = individual_graphs[:train_size]\n",
        "val_graphs = individual_graphs[train_size:]\n",
        "\n",
        "print(f\"Split: {len(train_graphs)} train graphs, {len(val_graphs)} val graphs\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = PyGDataLoader(train_graphs, batch_size=4, shuffle=True)\n",
        "val_loader = PyGDataLoader(val_graphs, batch_size=4, shuffle=False)\n",
        "\n",
        "print(f\"Data loaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = SimpleGNN(\n",
        "    input_dim=5,  # 5 node features\n",
        "    hidden_dim=32,\n",
        "    output_dim=2,  # Binary classification\n",
        "    num_layers=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "# Train model\n",
        "best_f1, train_losses, val_f1_scores = train_model(\n",
        "    model, train_loader, val_loader, epochs=10, lr=0.001\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation F1: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        pred = out.argmax(dim=1)\n",
        "        \n",
        "        test_preds.extend(pred.cpu().numpy())\n",
        "        test_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "# Final metrics\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "test_precision = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training completed successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
