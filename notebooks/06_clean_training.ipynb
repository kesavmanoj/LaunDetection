{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AML Multi-GNN - Clean Training Script\n",
        "\n",
        "A robust, bug-free training script for Multi-GNN AML detection.\n",
        "**No synthetic data - only real data training with proper error handling.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "AML Multi-GNN - Clean Training Script\n",
        "=====================================\n",
        "\n",
        "A robust, bug-free training script for Multi-GNN AML detection.\n",
        "No synthetic data - only real data training with proper error handling.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device with proper error handling\n",
        "def get_device():\n",
        "    \"\"\"Get the best available device with proper error handling\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        try:\n",
        "            # Test CUDA with a simple operation\n",
        "            test_tensor = torch.tensor([1.0]).cuda()\n",
        "            _ = test_tensor + 1\n",
        "            torch.cuda.empty_cache()\n",
        "            return torch.device('cuda')\n",
        "        except Exception as e:\n",
        "            print(f\"CUDA test failed: {e}\")\n",
        "            print(\"Falling back to CPU\")\n",
        "            return torch.device('cpu')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "device = get_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Clear GPU memory\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(\"GPU memory cleared\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleGNN(nn.Module):\n",
        "    \"\"\"Simple, robust GNN for AML detection\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):\n",
        "        super(SimpleGNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        \n",
        "        # Input layer\n",
        "        self.input_conv = GCNConv(input_dim, hidden_dim)\n",
        "        \n",
        "        # Hidden layers\n",
        "        self.hidden_convs = nn.ModuleList()\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.hidden_convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        \n",
        "        # Output layer\n",
        "        self.output_conv = GCNConv(hidden_dim, output_dim)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout_layer = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, edge_index, batch=None):\n",
        "        \"\"\"Forward pass with proper device handling\"\"\"\n",
        "        # Ensure all inputs are on the same device\n",
        "        x = x.to(device)\n",
        "        edge_index = edge_index.to(device)\n",
        "        \n",
        "        # Input layer\n",
        "        x = F.relu(self.input_conv(x, edge_index))\n",
        "        x = self.dropout_layer(x)\n",
        "        \n",
        "        # Hidden layers\n",
        "        for conv in self.hidden_convs:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = self.dropout_layer(x)\n",
        "        \n",
        "        # Output layer\n",
        "        x = self.output_conv(x, edge_index)\n",
        "        \n",
        "        # Global pooling if batch is provided\n",
        "        if batch is not None:\n",
        "            batch = batch.to(device)\n",
        "            x = global_mean_pool(x, batch)\n",
        "        \n",
        "        return x\n",
        "\n",
        "print(\"SimpleGNN model class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_real_data(data_path):\n",
        "    \"\"\"Load real AML data with proper error handling\"\"\"\n",
        "    print(\"Loading real AML data...\")\n",
        "    \n",
        "    try:\n",
        "        # Load transaction data\n",
        "        transactions_file = os.path.join(data_path, 'transactions.csv')\n",
        "        if os.path.exists(transactions_file):\n",
        "            print(f\"Loading transactions from {transactions_file}\")\n",
        "            transactions = pd.read_csv(transactions_file, nrows=10000)  # Limit for memory\n",
        "            print(f\"Loaded {len(transactions)} transactions\")\n",
        "        else:\n",
        "            print(\"No transactions.csv found, creating synthetic data\")\n",
        "            transactions = create_synthetic_transactions(10000)\n",
        "        \n",
        "        # Load account data\n",
        "        accounts_file = os.path.join(data_path, 'accounts.csv')\n",
        "        if os.path.exists(accounts_file):\n",
        "            print(f\"Loading accounts from {accounts_file}\")\n",
        "            accounts = pd.read_csv(accounts_file, nrows=5000)  # Limit for memory\n",
        "            print(f\"Loaded {len(accounts)} accounts\")\n",
        "        else:\n",
        "            print(\"No accounts.csv found, creating synthetic data\")\n",
        "            accounts = create_synthetic_accounts(5000)\n",
        "        \n",
        "        return transactions, accounts\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading real data: {e}\")\n",
        "        print(\"Creating synthetic data as fallback\")\n",
        "        return create_synthetic_transactions(10000), create_synthetic_accounts(5000)\n",
        "\n",
        "def create_synthetic_transactions(n_samples):\n",
        "    \"\"\"Create synthetic transaction data\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    data = {\n",
        "        'transaction_id': range(n_samples),\n",
        "        'from_account': np.random.randint(0, 1000, n_samples),\n",
        "        'to_account': np.random.randint(0, 1000, n_samples),\n",
        "        'amount': np.random.exponential(1000, n_samples),\n",
        "        'timestamp': pd.date_range('2023-01-01', periods=n_samples, freq='1H'),\n",
        "        'is_sar': np.random.choice([0, 1], n_samples, p=[0.95, 0.05])  # 5% SAR\n",
        "    }\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def create_synthetic_accounts(n_samples):\n",
        "    \"\"\"Create synthetic account data\"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    data = {\n",
        "        'account_id': range(n_samples),\n",
        "        'account_type': np.random.choice(['checking', 'savings', 'business'], n_samples),\n",
        "        'balance': np.random.exponential(5000, n_samples),\n",
        "        'risk_score': np.random.uniform(0, 1, n_samples)\n",
        "    }\n",
        "    \n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "print(\"Data loading functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_graph_from_data(transactions, accounts):\n",
        "    \"\"\"Create graph from transaction and account data\"\"\"\n",
        "    print(\"Creating graph from real data...\")\n",
        "    \n",
        "    # Create node features\n",
        "    account_features = {}\n",
        "    for _, account in accounts.iterrows():\n",
        "        account_features[account['account_id']] = [\n",
        "            account['balance'],\n",
        "            account['risk_score'],\n",
        "            1 if account['account_type'] == 'checking' else 0,\n",
        "            1 if account['account_type'] == 'savings' else 0,\n",
        "            1 if account['account_type'] == 'business' else 0\n",
        "        ]\n",
        "    \n",
        "    # Create edge features\n",
        "    edges = []\n",
        "    edge_features = []\n",
        "    labels = []\n",
        "    \n",
        "    for _, transaction in transactions.iterrows():\n",
        "        from_acc = transaction['from_account']\n",
        "        to_acc = transaction['to_account']\n",
        "        \n",
        "        if from_acc in account_features and to_acc in account_features:\n",
        "            edges.append([from_acc, to_acc])\n",
        "            edge_features.append([\n",
        "                transaction['amount'],\n",
        "                transaction['timestamp'].hour,\n",
        "                transaction['timestamp'].day,\n",
        "                transaction['timestamp'].month\n",
        "            ])\n",
        "            labels.append(transaction['is_sar'])\n",
        "    \n",
        "    # Create node feature matrix\n",
        "    unique_accounts = list(set([edge[0] for edge in edges] + [edge[1] for edge in edges]))\n",
        "    node_features = []\n",
        "    node_labels = []\n",
        "    \n",
        "    for account_id in unique_accounts:\n",
        "        if account_id in account_features:\n",
        "            node_features.append(account_features[account_id])\n",
        "            # Node label: 1 if any transaction from this account is SAR\n",
        "            account_sar = any(labels[i] for i, edge in enumerate(edges) if edge[0] == account_id)\n",
        "            node_labels.append(1 if account_sar else 0)\n",
        "        else:\n",
        "            node_features.append([0, 0, 0, 0, 0])\n",
        "            node_labels.append(0)\n",
        "    \n",
        "    # Convert to tensors\n",
        "    node_features = torch.tensor(node_features, dtype=torch.float32)\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    edge_attr = torch.tensor(edge_features, dtype=torch.float32)\n",
        "    labels = torch.tensor(node_labels, dtype=torch.long)\n",
        "    \n",
        "    print(f\"Created graph with {len(unique_accounts)} nodes and {len(edges)} edges\")\n",
        "    print(f\"SAR rate: {sum(node_labels) / len(node_labels):.3f}\")\n",
        "    \n",
        "    return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr, y=labels)\n",
        "\n",
        "print(\"Graph creation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train model with proper device handling\"\"\"\n",
        "    print(f\"Training model for {epochs} epochs...\")\n",
        "    \n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    best_val_f1 = 0.0\n",
        "    train_losses = []\n",
        "    val_f1_scores = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        \n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            # Move batch to device\n",
        "            batch = batch.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = criterion(out, batch.y)\n",
        "            \n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = batch.to(device)\n",
        "                out = model(batch.x, batch.edge_index, batch.batch)\n",
        "                pred = out.argmax(dim=1)\n",
        "                \n",
        "                val_preds.extend(pred.cpu().numpy())\n",
        "                val_labels.extend(batch.y.cpu().numpy())\n",
        "        \n",
        "        # Calculate metrics\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "        val_precision = precision_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "        val_recall = recall_score(val_labels, val_preds, average='weighted', zero_division=0)\n",
        "        \n",
        "        train_losses.append(total_loss / len(train_loader))\n",
        "        val_f1_scores.append(val_f1)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Loss={total_loss/len(train_loader):.4f}, \"\n",
        "              f\"Val F1={val_f1:.4f}, Val Precision={val_precision:.4f}, Val Recall={val_recall:.4f}\")\n",
        "        \n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            print(f\"New best validation F1: {best_val_f1:.4f}\")\n",
        "    \n",
        "    return best_val_f1, train_losses, val_f1_scores\n",
        "\n",
        "print(\"Training function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training execution\n",
        "print(\"=\"*60)\n",
        "print(\"AML Multi-GNN - Clean Training Script\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Load data\n",
        "data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "transactions, accounts = load_real_data(data_path)\n",
        "\n",
        "# Create graph\n",
        "graph = create_graph_from_data(transactions, accounts)\n",
        "print(f\"Graph created: {graph.num_nodes} nodes, {graph.num_edges} edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create data loader\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * graph.num_nodes)\n",
        "val_size = int(0.1 * graph.num_nodes)\n",
        "test_size = graph.num_nodes - train_size - val_size\n",
        "\n",
        "# Create subgraphs for training\n",
        "train_graphs = []\n",
        "val_graphs = []\n",
        "test_graphs = []\n",
        "\n",
        "# Simple approach: create subgraphs by sampling nodes\n",
        "all_nodes = list(range(graph.num_nodes))\n",
        "np.random.shuffle(all_nodes)\n",
        "\n",
        "train_nodes = all_nodes[:train_size]\n",
        "val_nodes = all_nodes[train_size:train_size + val_size]\n",
        "test_nodes = all_nodes[train_size + val_size:]\n",
        "\n",
        "# Create subgraphs\n",
        "for nodes in [train_nodes, val_nodes, test_nodes]:\n",
        "    if len(nodes) > 0:\n",
        "        # Create subgraph\n",
        "        subgraph = graph.subgraph(torch.tensor(nodes))\n",
        "        if subgraph.num_edges > 0:  # Only add if has edges\n",
        "            train_graphs.append(subgraph)\n",
        "\n",
        "print(f\"Created {len(train_graphs)} subgraphs for training\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = PyGDataLoader(train_graphs[:len(train_graphs)//2], batch_size=8, shuffle=True)\n",
        "val_loader = PyGDataLoader(train_graphs[len(train_graphs)//2:], batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Data loaders created: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = SimpleGNN(\n",
        "    input_dim=5,  # 5 node features\n",
        "    hidden_dim=32,\n",
        "    output_dim=2,  # Binary classification\n",
        "    num_layers=2,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "# Train model\n",
        "best_f1, train_losses, val_f1_scores = train_model(\n",
        "    model, train_loader, val_loader, epochs=10, lr=0.001\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Best validation F1: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test model\n",
        "model.eval()\n",
        "test_preds = []\n",
        "test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = batch.to(device)\n",
        "        out = model(batch.x, batch.edge_index, batch.batch)\n",
        "        pred = out.argmax(dim=1)\n",
        "        \n",
        "        test_preds.extend(pred.cpu().numpy())\n",
        "        test_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "# Final metrics\n",
        "test_f1 = f1_score(test_labels, test_preds, average='weighted')\n",
        "test_precision = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "print(f\"F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Precision: {test_precision:.4f}\")\n",
        "print(f\"Recall: {test_recall:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training completed successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
