{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Graph Construction and Preprocessing\n",
        "\n",
        "This notebook implements the graph construction phase for the AML Multi-GNN project.\n",
        "\n",
        "## Objectives:\n",
        "1. Load processed data from Phase 2\n",
        "2. Construct transaction graphs with multiple views\n",
        "3. Create node and edge features\n",
        "4. Implement graph preprocessing and validation\n",
        "5. Save graph data for model training\n",
        "\n",
        "## Graph Views:\n",
        "- **Transaction View**: Direct transaction relationships\n",
        "- **Account View**: Account-based connections\n",
        "- **Temporal View**: Time-based transaction patterns\n",
        "- **Amount View**: Value-based transaction clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 3: Graph Construction and Preprocessing\n",
        "print(\"=\" * 60)\n",
        "print(\"AML Multi-GNN - Phase 3: Graph Construction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data from Phase 2\n",
        "print(\"Loading processed data from Phase 2...\")\n",
        "\n",
        "try:\n",
        "    # Load unified data\n",
        "    unified_data_path = \"/content/drive/MyDrive/LaunDetection/data/processed/unified_data.csv\"\n",
        "    if os.path.exists(unified_data_path):\n",
        "        df = pd.read_csv(unified_data_path)\n",
        "        print(f\"✓ Loaded unified data: {df.shape}\")\n",
        "    else:\n",
        "        print(\"✗ Unified data not found. Please run Phase 2 first.\")\n",
        "        df = None\n",
        "    \n",
        "    # Load data structure\n",
        "    structure_path = \"/content/drive/MyDrive/LaunDetection/data/processed/data_structure.json\"\n",
        "    if os.path.exists(structure_path):\n",
        "        with open(structure_path, 'r') as f:\n",
        "            data_structure = json.load(f)\n",
        "        print(f\"✓ Loaded data structure: {data_structure}\")\n",
        "    else:\n",
        "        print(\"⚠️  Data structure not found, using default structure\")\n",
        "        data_structure = {\n",
        "            'transactions': 'HI-Small_Trans',\n",
        "            'accounts': 'HI-Small_accounts',\n",
        "            'labels': None\n",
        "        }\n",
        "    \n",
        "    # Load quality metrics\n",
        "    quality_path = \"/content/drive/MyDrive/LaunDetection/data/processed/quality_metrics.json\"\n",
        "    if os.path.exists(quality_path):\n",
        "        with open(quality_path, 'r') as f:\n",
        "            quality_metrics = json.load(f)\n",
        "        print(f\"✓ Loaded quality metrics\")\n",
        "    else:\n",
        "        print(\"⚠️  Quality metrics not found\")\n",
        "        quality_metrics = None\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading processed data: {e}\")\n",
        "    df = None\n",
        "    data_structure = None\n",
        "    quality_metrics = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data validation and preprocessing\n",
        "if df is not None:\n",
        "    print(\"Validating and preprocessing data...\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(f\"Data types:\\n{df.dtypes}\")\n",
        "    \n",
        "    # Check for required columns\n",
        "    required_cols = ['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1', 'Amount Received', 'Is Laundering']\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"⚠️  Missing required columns: {missing_cols}\")\n",
        "    else:\n",
        "        print(\"✓ All required columns present\")\n",
        "    \n",
        "    # Handle missing values\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        print(f\"Missing values found:\\n{df.isnull().sum()}\")\n",
        "        # Fill missing values\n",
        "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "        print(\"✓ Missing values filled\")\n",
        "    else:\n",
        "        print(\"✓ No missing values\")\n",
        "    \n",
        "    # Convert timestamp to datetime\n",
        "    if 'Timestamp' in df.columns:\n",
        "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "        print(\"✓ Timestamp converted to datetime\")\n",
        "    \n",
        "    # Check for laundering cases\n",
        "    if 'Is Laundering' in df.columns:\n",
        "        laundering_count = df['Is Laundering'].sum()\n",
        "        total_count = len(df)\n",
        "        print(f\"Laundering cases: {laundering_count}/{total_count} ({laundering_count/total_count*100:.1f}%)\")\n",
        "        \n",
        "        if laundering_count == 0:\n",
        "            print(\"⚠️  No laundering cases in current sample - this is expected for exploration\")\n",
        "    \n",
        "    print(\"✓ Data validation complete\")\n",
        "else:\n",
        "    print(\"✗ Cannot proceed without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph Construction Functions\n",
        "\n",
        "def create_transaction_graph(df):\n",
        "    \"\"\"\n",
        "    Create a transaction-based graph where:\n",
        "    - Nodes represent accounts\n",
        "    - Edges represent transactions\n",
        "    - Edge features include transaction details\n",
        "    \"\"\"\n",
        "    print(\"Creating transaction graph...\")\n",
        "    \n",
        "    # Create NetworkX graph\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes (accounts)\n",
        "    from_accounts = df['Account'].unique()\n",
        "    to_accounts = df['Account.1'].unique()\n",
        "    all_accounts = list(set(from_accounts) | set(to_accounts))\n",
        "    \n",
        "    for account in all_accounts:\n",
        "        G.add_node(account, node_type='account')\n",
        "    \n",
        "    print(f\"Added {len(all_accounts)} account nodes\")\n",
        "    \n",
        "    # Add edges (transactions)\n",
        "    edge_features = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        from_acc = row['Account']\n",
        "        to_acc = row['Account.1']\n",
        "        \n",
        "        # Create edge features\n",
        "        edge_feat = {\n",
        "            'amount_received': row['Amount Received'],\n",
        "            'amount_paid': row['Amount Paid'],\n",
        "            'receiving_currency': row['Receiving Currency'],\n",
        "            'payment_currency': row['Payment Currency'],\n",
        "            'payment_format': row['Payment Format'],\n",
        "            'timestamp': row['Timestamp'],\n",
        "            'is_laundering': row['Is Laundering'],\n",
        "            'from_bank': row['From Bank'],\n",
        "            'to_bank': row['To Bank']\n",
        "        }\n",
        "        \n",
        "        # Add edge with features\n",
        "        G.add_edge(from_acc, to_acc, **edge_feat)\n",
        "        edge_features.append(edge_feat)\n",
        "    \n",
        "    print(f\"Added {len(edge_features)} transaction edges\")\n",
        "    \n",
        "    return G, edge_features\n",
        "\n",
        "def create_account_graph(df):\n",
        "    \"\"\"\n",
        "    Create an account-based graph where:\n",
        "    - Nodes represent accounts with aggregated features\n",
        "    - Edges represent account relationships\n",
        "    \"\"\"\n",
        "    print(\"Creating account graph...\")\n",
        "    \n",
        "    # Aggregate account features\n",
        "    account_features = {}\n",
        "    \n",
        "    # From account features\n",
        "    from_features = df.groupby('Account').agg({\n",
        "        'Amount Received': ['sum', 'mean', 'count'],\n",
        "        'From Bank': 'first',\n",
        "        'Is Laundering': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    # To account features\n",
        "    to_features = df.groupby('Account.1').agg({\n",
        "        'Amount Paid': ['sum', 'mean', 'count'],\n",
        "        'To Bank': 'first',\n",
        "        'Is Laundering': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    # Combine features\n",
        "    all_accounts = list(set(df['Account'].unique()) | set(df['Account.1'].unique()))\n",
        "    \n",
        "    for account in all_accounts:\n",
        "        features = {\n",
        "            'total_sent': 0,\n",
        "            'total_received': 0,\n",
        "            'avg_sent': 0,\n",
        "            'avg_received': 0,\n",
        "            'transaction_count': 0,\n",
        "            'laundering_count': 0,\n",
        "            'primary_bank': None\n",
        "        }\n",
        "        \n",
        "        # From account features\n",
        "        if account in from_features.index:\n",
        "            features['total_sent'] = from_features.loc[account, ('Amount Received', 'sum')]\n",
        "            features['avg_sent'] = from_features.loc[account, ('Amount Received', 'mean')]\n",
        "            features['transaction_count'] += from_features.loc[account, ('Amount Received', 'count')]\n",
        "            features['laundering_count'] += from_features.loc[account, ('Is Laundering', 'sum')]\n",
        "            features['primary_bank'] = from_features.loc[account, ('From Bank', 'first')]\n",
        "        \n",
        "        # To account features\n",
        "        if account in to_features.index:\n",
        "            features['total_received'] = to_features.loc[account, ('Amount Paid', 'sum')]\n",
        "            features['avg_received'] = to_features.loc[account, ('Amount Paid', 'mean')]\n",
        "            features['transaction_count'] += to_features.loc[account, ('Amount Paid', 'count')]\n",
        "            features['laundering_count'] += to_features.loc[account, ('Is Laundering', 'sum')]\n",
        "            if features['primary_bank'] is None:\n",
        "                features['primary_bank'] = to_features.loc[account, ('To Bank', 'first')]\n",
        "        \n",
        "        account_features[account] = features\n",
        "    \n",
        "    # Create NetworkX graph\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes with features\n",
        "    for account, features in account_features.items():\n",
        "        G.add_node(account, **features)\n",
        "    \n",
        "    # Add edges based on transaction relationships\n",
        "    for idx, row in df.iterrows():\n",
        "        from_acc = row['Account']\n",
        "        to_acc = row['Account.1']\n",
        "        \n",
        "        if G.has_edge(from_acc, to_acc):\n",
        "            # Update edge weight (transaction count)\n",
        "            G[from_acc][to_acc]['weight'] += 1\n",
        "        else:\n",
        "            # Add new edge\n",
        "            G.add_edge(from_acc, to_acc, weight=1)\n",
        "    \n",
        "    print(f\"Added {len(account_features)} account nodes with features\")\n",
        "    print(f\"Added {G.number_of_edges()} account relationships\")\n",
        "    \n",
        "    return G, account_features\n",
        "\n",
        "def create_temporal_graph(df):\n",
        "    \"\"\"\n",
        "    Create a temporal graph based on transaction timing\n",
        "    \"\"\"\n",
        "    print(\"Creating temporal graph...\")\n",
        "    \n",
        "    # Sort by timestamp\n",
        "    df_sorted = df.sort_values('Timestamp')\n",
        "    \n",
        "    # Create time-based connections\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add all accounts as nodes\n",
        "    all_accounts = list(set(df['Account'].unique()) | set(df['Account.1'].unique()))\n",
        "    for account in all_accounts:\n",
        "        G.add_node(account, node_type='account')\n",
        "    \n",
        "    # Create temporal edges based on transaction sequence\n",
        "    for i in range(len(df_sorted) - 1):\n",
        "        current_row = df_sorted.iloc[i]\n",
        "        next_row = df_sorted.iloc[i + 1]\n",
        "        \n",
        "        # Connect accounts involved in consecutive transactions\n",
        "        current_accounts = [current_row['Account'], current_row['Account.1']]\n",
        "        next_accounts = [next_row['Account'], next_row['Account.1']]\n",
        "        \n",
        "        # Add temporal connections\n",
        "        for curr_acc in current_accounts:\n",
        "            for next_acc in next_accounts:\n",
        "                if curr_acc != next_acc:\n",
        "                    if G.has_edge(curr_acc, next_acc):\n",
        "                        G[curr_acc][next_acc]['temporal_weight'] += 1\n",
        "                    else:\n",
        "                        G.add_edge(curr_acc, next_acc, temporal_weight=1)\n",
        "    \n",
        "    print(f\"Added {G.number_of_edges()} temporal connections\")\n",
        "    \n",
        "    return G\n",
        "\n",
        "def create_amount_graph(df):\n",
        "    \"\"\"\n",
        "    Create a graph based on transaction amounts\n",
        "    \"\"\"\n",
        "    print(\"Creating amount-based graph...\")\n",
        "    \n",
        "    # Create amount-based connections\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add all accounts as nodes\n",
        "    all_accounts = list(set(df['Account'].unique()) | set(df['Account.1'].unique()))\n",
        "    for account in all_accounts:\n",
        "        G.add_node(account, node_type='account')\n",
        "    \n",
        "    # Create amount-based edges\n",
        "    for idx, row in df.iterrows():\n",
        "        from_acc = row['Account']\n",
        "        to_acc = row['Account.1']\n",
        "        amount = row['Amount Received']\n",
        "        \n",
        "        # Add edge with amount as weight\n",
        "        if G.has_edge(from_acc, to_acc):\n",
        "            G[from_acc][to_acc]['amount_weight'] += amount\n",
        "        else:\n",
        "            G.add_edge(from_acc, to_acc, amount_weight=amount)\n",
        "    \n",
        "    print(f\"Added {G.number_of_edges()} amount-based connections\")\n",
        "    \n",
        "    return G\n",
        "\n",
        "print(\"✓ Graph construction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build all graph views\n",
        "if df is not None:\n",
        "    print(\"Building multiple graph views...\")\n",
        "    \n",
        "    graphs = {}\n",
        "    \n",
        "    # 1. Transaction Graph\n",
        "    print(\"\\n1. Transaction Graph:\")\n",
        "    trans_graph, edge_features = create_transaction_graph(df)\n",
        "    graphs['transaction'] = trans_graph\n",
        "    \n",
        "    # 2. Account Graph\n",
        "    print(\"\\n2. Account Graph:\")\n",
        "    account_graph, account_features = create_account_graph(df)\n",
        "    graphs['account'] = account_graph\n",
        "    \n",
        "    # 3. Temporal Graph\n",
        "    print(\"\\n3. Temporal Graph:\")\n",
        "    temporal_graph = create_temporal_graph(df)\n",
        "    graphs['temporal'] = temporal_graph\n",
        "    \n",
        "    # 4. Amount Graph\n",
        "    print(\"\\n4. Amount Graph:\")\n",
        "    amount_graph = create_amount_graph(df)\n",
        "    graphs['amount'] = amount_graph\n",
        "    \n",
        "    print(\"\\n✓ All graph views created successfully\")\n",
        "    \n",
        "    # Display graph statistics\n",
        "    print(\"\\nGraph Statistics:\")\n",
        "    for name, graph in graphs.items():\n",
        "        print(f\"{name.capitalize()} Graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        \n",
        "else:\n",
        "    print(\"✗ Cannot build graphs without data\")\n",
        "    graphs = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to PyTorch Geometric format\n",
        "if graphs is not None:\n",
        "    print(\"Converting graphs to PyTorch Geometric format...\")\n",
        "    \n",
        "    pytorch_graphs = {}\n",
        "    \n",
        "    for name, graph in graphs.items():\n",
        "        print(f\"\\nConverting {name} graph...\")\n",
        "        \n",
        "        try:\n",
        "            # Convert to PyTorch Geometric Data object\n",
        "            pyg_data = from_networkx(graph)\n",
        "            \n",
        "            # Add node features if available\n",
        "            if name == 'account' and account_features:\n",
        "                # Create node feature matrix\n",
        "                node_features = []\n",
        "                for node in graph.nodes():\n",
        "                    if node in account_features:\n",
        "                        features = account_features[node]\n",
        "                        feature_vector = [\n",
        "                            features['total_sent'],\n",
        "                            features['total_received'],\n",
        "                            features['avg_sent'],\n",
        "                            features['avg_received'],\n",
        "                            features['transaction_count'],\n",
        "                            features['laundering_count'],\n",
        "                            features['primary_bank'] or 0\n",
        "                        ]\n",
        "                        node_features.append(feature_vector)\n",
        "                    else:\n",
        "                        # Default features for nodes without account features\n",
        "                        node_features.append([0, 0, 0, 0, 0, 0, 0])\n",
        "                \n",
        "                pyg_data.x = torch.tensor(node_features, dtype=torch.float)\n",
        "                print(f\"✓ Added node features: {pyg_data.x.shape}\")\n",
        "            \n",
        "            # Add edge features if available\n",
        "            if name == 'transaction' and edge_features:\n",
        "                # Create edge feature matrix\n",
        "                edge_features_tensor = []\n",
        "                for edge in graph.edges():\n",
        "                    edge_data = graph[edge[0]][edge[1]]\n",
        "                    feature_vector = [\n",
        "                        edge_data.get('amount_received', 0),\n",
        "                        edge_data.get('amount_paid', 0),\n",
        "                        edge_data.get('is_laundering', 0),\n",
        "                        edge_data.get('from_bank', 0),\n",
        "                        edge_data.get('to_bank', 0)\n",
        "                    ]\n",
        "                    edge_features_tensor.append(feature_vector)\n",
        "                \n",
        "                pyg_data.edge_attr = torch.tensor(edge_features_tensor, dtype=torch.float)\n",
        "                print(f\"✓ Added edge features: {pyg_data.edge_attr.shape}\")\n",
        "            \n",
        "            # Add labels (laundering information)\n",
        "            if 'Is Laundering' in df.columns:\n",
        "                # Create node labels based on account involvement in laundering\n",
        "                account_labels = {}\n",
        "                for account in graph.nodes():\n",
        "                    # Check if account is involved in any laundering transactions\n",
        "                    laundering_involvement = df[\n",
        "                        (df['Account'] == account) | (df['Account.1'] == account)\n",
        "                    ]['Is Laundering'].sum()\n",
        "                    \n",
        "                    account_labels[account] = 1 if laundering_involvement > 0 else 0\n",
        "                \n",
        "                # Create label tensor\n",
        "                labels = [account_labels.get(node, 0) for node in graph.nodes()]\n",
        "                pyg_data.y = torch.tensor(labels, dtype=torch.long)\n",
        "                print(f\"✓ Added labels: {pyg_data.y.shape}\")\n",
        "            \n",
        "            pytorch_graphs[name] = pyg_data\n",
        "            print(f\"✓ {name} graph converted successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error converting {name} graph: {e}\")\n",
        "            pytorch_graphs[name] = None\n",
        "    \n",
        "    print(\"\\n✓ All graphs converted to PyTorch Geometric format\")\n",
        "    \n",
        "    # Display PyTorch Geometric graph info\n",
        "    print(\"\\nPyTorch Geometric Graph Information:\")\n",
        "    for name, pyg_data in pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"\\n{name.capitalize()} Graph:\")\n",
        "            print(f\"  Nodes: {pyg_data.num_nodes}\")\n",
        "            print(f\"  Edges: {pyg_data.num_edges}\")\n",
        "            if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                print(f\"  Node features: {pyg_data.x.shape}\")\n",
        "            if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                print(f\"  Edge features: {pyg_data.edge_attr.shape}\")\n",
        "            if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                print(f\"  Labels: {pyg_data.y.shape}\")\n",
        "                print(f\"  Label distribution: {torch.bincount(pyg_data.y)}\")\n",
        "        else:\n",
        "            print(f\"\\n{name.capitalize()} Graph: Failed to convert\")\n",
        "            \n",
        "else:\n",
        "    print(\"✗ Cannot convert graphs without data\")\n",
        "    pytorch_graphs = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph visualization and analysis\n",
        "if pytorch_graphs is not None:\n",
        "    print(\"Analyzing graph properties...\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Graph Views Analysis', fontsize=16)\n",
        "    \n",
        "    graph_names = ['transaction', 'account', 'temporal', 'amount']\n",
        "    \n",
        "    for i, name in enumerate(graph_names):\n",
        "        if name in pytorch_graphs and pytorch_graphs[name] is not None:\n",
        "            ax = axes[i//2, i%2]\n",
        "            \n",
        "            # Get NetworkX graph for visualization\n",
        "            if name in graphs:\n",
        "                G = graphs[name]\n",
        "                \n",
        "                # Create subgraph for visualization (limit nodes for clarity)\n",
        "                if G.number_of_nodes() > 50:\n",
        "                    # Sample nodes for visualization\n",
        "                    nodes_to_keep = list(G.nodes())[:50]\n",
        "                    G_viz = G.subgraph(nodes_to_keep)\n",
        "                else:\n",
        "                    G_viz = G\n",
        "                \n",
        "                # Plot graph\n",
        "                pos = nx.spring_layout(G_viz, k=1, iterations=50)\n",
        "                nx.draw(G_viz, pos, ax=ax, node_size=50, node_color='lightblue', \n",
        "                       edge_color='gray', arrows=True, arrowsize=10)\n",
        "                \n",
        "                ax.set_title(f'{name.capitalize()} Graph\\n({G_viz.number_of_nodes()} nodes, {G_viz.number_of_edges()} edges)')\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, f'{name.capitalize()}\\nGraph not available', \n",
        "                       ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title(f'{name.capitalize()} Graph')\n",
        "        else:\n",
        "            ax = axes[i//2, i%2]\n",
        "            ax.text(0.5, 0.5, f'{name.capitalize()}\\nGraph not available', \n",
        "                   ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{name.capitalize()} Graph')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Graph statistics\n",
        "    print(\"\\nDetailed Graph Statistics:\")\n",
        "    for name, pyg_data in pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"\\n{name.capitalize()} Graph:\")\n",
        "            print(f\"  Nodes: {pyg_data.num_nodes}\")\n",
        "            print(f\"  Edges: {pyg_data.num_edges}\")\n",
        "            print(f\"  Density: {pyg_data.num_edges / (pyg_data.num_nodes * (pyg_data.num_nodes - 1)):.4f}\")\n",
        "            \n",
        "            if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                print(f\"  Node features: {pyg_data.x.shape}\")\n",
        "                print(f\"  Node feature range: [{pyg_data.x.min():.2f}, {pyg_data.x.max():.2f}]\")\n",
        "            \n",
        "            if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                print(f\"  Edge features: {pyg_data.edge_attr.shape}\")\n",
        "                print(f\"  Edge feature range: [{pyg_data.edge_attr.min():.2f}, {pyg_data.edge_attr.max():.2f}]\")\n",
        "            \n",
        "            if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                print(f\"  Labels: {pyg_data.y.shape}\")\n",
        "                label_counts = torch.bincount(pyg_data.y)\n",
        "                print(f\"  Label distribution: {dict(zip(range(len(label_counts)), label_counts.tolist()))}\")\n",
        "                \n",
        "                if len(label_counts) > 1:\n",
        "                    print(f\"  Class imbalance ratio: {label_counts[1].item() / label_counts[0].item():.4f}\")\n",
        "                else:\n",
        "                    print(f\"  Only one class present (all {label_counts[0].item()} samples)\")\n",
        "    \n",
        "    print(\"\\n✓ Graph analysis complete\")\n",
        "    \n",
        "else:\n",
        "    print(\"✗ Cannot analyze graphs without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save graph data for model training\n",
        "if pytorch_graphs is not None:\n",
        "    print(\"Saving graph data for model training...\")\n",
        "    \n",
        "    try:\n",
        "        # Create output directory\n",
        "        output_dir = \"/content/drive/MyDrive/LaunDetection/data/processed/graphs\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Save each graph view\n",
        "        for name, pyg_data in pytorch_graphs.items():\n",
        "            if pyg_data is not None:\n",
        "                graph_path = os.path.join(output_dir, f\"{name}_graph.pt\")\n",
        "                torch.save(pyg_data, graph_path)\n",
        "                print(f\"✓ Saved {name} graph to {graph_path}\")\n",
        "        \n",
        "        # Save graph metadata\n",
        "        metadata = {\n",
        "            'num_graphs': len([g for g in pytorch_graphs.values() if g is not None]),\n",
        "            'graph_names': list(pytorch_graphs.keys()),\n",
        "            'creation_time': datetime.now().isoformat(),\n",
        "            'data_shape': df.shape if df is not None else None,\n",
        "            'data_structure': data_structure\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(output_dir, \"graph_metadata.json\")\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"✓ Saved graph metadata to {metadata_path}\")\n",
        "        \n",
        "        # Save graph statistics\n",
        "        stats = {}\n",
        "        for name, pyg_data in pytorch_graphs.items():\n",
        "            if pyg_data is not None:\n",
        "                stats[name] = {\n",
        "                    'num_nodes': pyg_data.num_nodes,\n",
        "                    'num_edges': pyg_data.num_edges,\n",
        "                    'has_node_features': hasattr(pyg_data, 'x') and pyg_data.x is not None,\n",
        "                    'has_edge_features': hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None,\n",
        "                    'has_labels': hasattr(pyg_data, 'y') and pyg_data.y is not None\n",
        "                }\n",
        "                \n",
        "                if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                    stats[name]['node_feature_shape'] = list(pyg_data.x.shape)\n",
        "                \n",
        "                if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                    stats[name]['edge_feature_shape'] = list(pyg_data.edge_attr.shape)\n",
        "                \n",
        "                if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                    stats[name]['label_shape'] = list(pyg_data.y.shape)\n",
        "                    stats[name]['label_distribution'] = torch.bincount(pyg_data.y).tolist()\n",
        "        \n",
        "        stats_path = os.path.join(output_dir, \"graph_statistics.json\")\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(f\"✓ Saved graph statistics to {stats_path}\")\n",
        "        \n",
        "        print(\"\\n✓ All graph data saved successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error saving graph data: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"✗ Cannot save graphs without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory cleanup and final summary\n",
        "print(\"Cleaning up memory...\")\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Phase 3 - Graph Construction Completed!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if pytorch_graphs is not None:\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"✓ Created {len([g for g in pytorch_graphs.values() if g is not None])} graph views\")\n",
        "    print(f\"✓ All graphs converted to PyTorch Geometric format\")\n",
        "    print(f\"✓ Graph data saved to /content/drive/MyDrive/LaunDetection/data/processed/graphs/\")\n",
        "    \n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Review the graph construction results above\")\n",
        "    print(\"2. Proceed to Phase 4: Multi-GNN Architecture\")\n",
        "    print(\"3. Run: %run notebooks/03_multi_gnn_architecture.ipynb\")\n",
        "    \n",
        "    print(\"\\nGraph Views Created:\")\n",
        "    for name, pyg_data in pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"  - {name.capitalize()}: {pyg_data.num_nodes} nodes, {pyg_data.num_edges} edges\")\n",
        "        else:\n",
        "            print(f\"  - {name.capitalize()}: Failed to create\")\n",
        "            \n",
        "else:\n",
        "    print(\"\\n✗ Graph construction failed - please check data loading\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Ensure Phase 2 completed successfully\")\n",
        "    print(\"2. Check that unified_data.csv exists in processed folder\")\n",
        "    print(\"3. Verify data structure and quality metrics\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
