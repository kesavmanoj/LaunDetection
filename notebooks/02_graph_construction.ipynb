{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Graph Construction and Preprocessing\n",
        "\n",
        "This notebook implements the graph construction phase for the AML Multi-GNN project.\n",
        "\n",
        "## Objectives:\n",
        "1. Load processed data from Phase 2\n",
        "2. Construct transaction graphs with multiple views\n",
        "3. Create node and edge features\n",
        "4. Implement graph preprocessing and validation\n",
        "5. Save graph data for model training\n",
        "\n",
        "## Graph Views:\n",
        "- **Transaction View**: Direct transaction relationships\n",
        "- **Account View**: Account-based connections\n",
        "- **Temporal View**: Time-based transaction patterns\n",
        "- **Amount View**: Value-based transaction clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 3: Graph Construction and Preprocessing\n",
        "print(\"=\" * 60)\n",
        "print(\"AML Multi-GNN - Phase 3: Graph Construction\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, HeteroData\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data from Phase 2\n",
        "print(\"Loading processed data from Phase 2...\")\n",
        "\n",
        "try:\n",
        "    # Load unified data\n",
        "    unified_data_path = \"/content/drive/MyDrive/LaunDetection/data/processed/unified_data.csv\"\n",
        "    if os.path.exists(unified_data_path):\n",
        "        df = pd.read_csv(unified_data_path)\n",
        "        print(f\"✓ Loaded unified data: {df.shape}\")\n",
        "    else:\n",
        "        print(\"✗ Unified data not found. Please run Phase 2 first.\")\n",
        "        df = None\n",
        "    \n",
        "    # Load data structure\n",
        "    structure_path = \"/content/drive/MyDrive/LaunDetection/data/processed/data_structure.json\"\n",
        "    if os.path.exists(structure_path):\n",
        "        with open(structure_path, 'r') as f:\n",
        "            data_structure = json.load(f)\n",
        "        print(f\"✓ Loaded data structure: {data_structure}\")\n",
        "    else:\n",
        "        print(\"⚠️  Data structure not found, using default structure\")\n",
        "        data_structure = {\n",
        "            'transactions': 'HI-Small_Trans',\n",
        "            'accounts': 'HI-Small_accounts',\n",
        "            'labels': None\n",
        "        }\n",
        "    \n",
        "    # Load quality metrics\n",
        "    quality_path = \"/content/drive/MyDrive/LaunDetection/data/processed/quality_metrics.json\"\n",
        "    if os.path.exists(quality_path):\n",
        "        with open(quality_path, 'r') as f:\n",
        "            quality_metrics = json.load(f)\n",
        "        print(f\"✓ Loaded quality metrics\")\n",
        "    else:\n",
        "        print(\"⚠️  Quality metrics not found\")\n",
        "        quality_metrics = None\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading processed data: {e}\")\n",
        "    df = None\n",
        "    data_structure = None\n",
        "    quality_metrics = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data validation and preprocessing\n",
        "if df is not None:\n",
        "    print(\"Validating and preprocessing data...\")\n",
        "    \n",
        "    # Display basic info\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(f\"Data types:\\n{df.dtypes}\")\n",
        "    \n",
        "    # Check for required columns\n",
        "    required_cols = ['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1', 'Amount Received', 'Is Laundering']\n",
        "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
        "    \n",
        "    if missing_cols:\n",
        "        print(f\"⚠️  Missing required columns: {missing_cols}\")\n",
        "    else:\n",
        "        print(\"✓ All required columns present\")\n",
        "    \n",
        "    # Handle missing values\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        print(f\"Missing values found:\\n{df.isnull().sum()}\")\n",
        "        # Fill missing values\n",
        "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "        print(\"✓ Missing values filled\")\n",
        "    else:\n",
        "        print(\"✓ No missing values\")\n",
        "    \n",
        "    # Convert timestamp to datetime\n",
        "    if 'Timestamp' in df.columns:\n",
        "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "        print(\"✓ Timestamp converted to datetime\")\n",
        "    \n",
        "    # Check for laundering cases\n",
        "    if 'Is Laundering' in df.columns:\n",
        "        laundering_count = df['Is Laundering'].sum()\n",
        "        total_count = len(df)\n",
        "        print(f\"Laundering cases: {laundering_count}/{total_count} ({laundering_count/total_count*100:.1f}%)\")\n",
        "        \n",
        "        if laundering_count == 0:\n",
        "            print(\"⚠️  No laundering cases in current sample - this is expected for exploration\")\n",
        "    \n",
        "    print(\"✓ Data validation complete\")\n",
        "else:\n",
        "    print(\"✗ Cannot proceed without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph Construction Functions\n",
        "\n",
        "def create_transaction_graph(df):\n",
        "    \"\"\"\n",
        "    Create a transaction-based graph where:\n",
        "    - Nodes represent accounts\n",
        "    - Edges represent transactions\n",
        "    - Edge features include transaction details\n",
        "    \"\"\"\n",
        "    print(\"Creating transaction graph...\")\n",
        "    \n",
        "    # Create NetworkX graph\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes (accounts)\n",
        "    from_accounts = df['Account'].unique()\n",
        "    to_accounts = df['Account.1'].unique()\n",
        "    all_accounts = list(set(from_accounts) | set(to_accounts))\n",
        "    \n",
        "    for account in all_accounts:\n",
        "        G.add_node(account, node_type='account')\n",
        "    \n",
        "    print(f\"Added {len(all_accounts)} account nodes\")\n",
        "    \n",
        "    # Add edges (transactions)\n",
        "    edge_features = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        from_acc = row['Account']\n",
        "        to_acc = row['Account.1']\n",
        "        \n",
        "        # Create edge features\n",
        "        edge_feat = {\n",
        "            'amount_received': row['Amount Received'],\n",
        "            'amount_paid': row['Amount Paid'],\n",
        "            'receiving_currency': row['Receiving Currency'],\n",
        "            'payment_currency': row['Payment Currency'],\n",
        "            'payment_format': row['Payment Format'],\n",
        "            'timestamp': row['Timestamp'],\n",
        "            'is_laundering': row['Is Laundering'],\n",
        "            'from_bank': row['From Bank'],\n",
        "            'to_bank': row['To Bank']\n",
        "        }\n",
        "        \n",
        "        # Add edge with features\n",
        "        G.add_edge(from_acc, to_acc, **edge_feat)\n",
        "        edge_features.append(edge_feat)\n",
        "    \n",
        "    print(f\"Added {len(edge_features)} transaction edges\")\n",
        "    \n",
        "    return G, edge_features\n",
        "\n",
        "def create_account_graph(df):\n",
        "    \"\"\"\n",
        "    Create an account-based graph where:\n",
        "    - Nodes represent accounts with aggregated features\n",
        "    - Edges represent account relationships\n",
        "    \"\"\"\n",
        "    print(\"Creating account graph...\")\n",
        "    \n",
        "    # Aggregate account features\n",
        "    account_features = {}\n",
        "    \n",
        "    # From account features\n",
        "    from_features = df.groupby('Account').agg({\n",
        "        'Amount Received': ['sum', 'mean', 'count'],\n",
        "        'From Bank': 'first',\n",
        "        'Is Laundering': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    # To account features\n",
        "    to_features = df.groupby('Account.1').agg({\n",
        "        'Amount Paid': ['sum', 'mean', 'count'],\n",
        "        'To Bank': 'first',\n",
        "        'Is Laundering': 'sum'\n",
        "    }).round(2)\n",
        "    \n",
        "    # Combine features\n",
        "    all_accounts = list(set(df['Account'].unique()) | set(df['Account.1'].unique()))\n",
        "    \n",
        "    for account in all_accounts:\n",
        "        features = {\n",
        "            'total_sent': 0,\n",
        "            'total_received': 0,\n",
        "            'avg_sent': 0,\n",
        "            'avg_received': 0,\n",
        "            'transaction_count': 0,\n",
        "            'laundering_count': 0,\n",
        "            'primary_bank': None\n",
        "        }\n",
        "        \n",
        "        # From account features\n",
        "        if account in from_features.index:\n",
        "            features['total_sent'] = from_features.loc[account, ('Amount Received', 'sum')]\n",
        "            features['avg_sent'] = from_features.loc[account, ('Amount Received', 'mean')]\n",
        "            features['transaction_count'] += from_features.loc[account, ('Amount Received', 'count')]\n",
        "            features['laundering_count'] += from_features.loc[account, ('Is Laundering', 'sum')]\n",
        "            features['primary_bank'] = from_features.loc[account, ('From Bank', 'first')]\n",
        "        \n",
        "        # To account features\n",
        "        if account in to_features.index:\n",
        "            features['total_received'] = to_features.loc[account, ('Amount Paid', 'sum')]\n",
        "            features['avg_received'] = to_features.loc[account, ('Amount Paid', 'mean')]\n",
        "            features['transaction_count'] += to_features.loc[account, ('Amount Paid', 'count')]\n",
        "            features['laundering_count'] += to_features.loc[account, ('Is Laundering', 'sum')]\n",
        "            if features['primary_bank'] is None:\n",
        "                features['primary_bank'] = to_features.loc[account, ('To Bank', 'first')]\n",
        "        \n",
        "        account_features[account] = features\n",
        "    \n",
        "    # Create NetworkX graph\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add nodes with features\n",
        "    for account, features in account_features.items():\n",
        "        G.add_node(account, **features)\n",
        "    \n",
        "    # Add edges based on transaction relationships\n",
        "    for idx, row in df.iterrows():\n",
        "        from_acc = row['Account']\n",
        "        to_acc = row['Account.1']\n",
        "        \n",
        "        if G.has_edge(from_acc, to_acc):\n",
        "            # Update edge weight (transaction count)\n",
        "            G[from_acc][to_acc]['weight'] += 1\n",
        "        else:\n",
        "            # Add new edge\n",
        "            G.add_edge(from_acc, to_acc, weight=1)\n",
        "    \n",
        "    print(f\"Added {len(account_features)} account nodes with features\")\n",
        "    print(f\"Added {G.number_of_edges()} account relationships\")\n",
        "    \n",
        "    return G, account_features\n",
        "\n",
        "def create_temporal_graph(df):\n",
        "    \"\"\"\n",
        "    Create a temporal graph based on transaction timing\n",
        "    \"\"\"\n",
        "    print(\"Creating temporal graph...\")\n",
        "    \n",
        "    # Sort by timestamp\n",
        "    df_sorted = df.sort_values('Timestamp')\n",
        "    \n",
        "    # Create time-based connections\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add all accounts as nodes\n",
        "    all_accounts = list(set(df['Account'].unique()) | set(df['Account.1'].unique()))\n",
        "    for account in all_accounts:\n",
        "        G.add_node(account, node_type='account')\n",
        "    \n",
        "    # Create temporal edges based on transaction sequence\n",
        "    for i in range(len(df_sorted) - 1):\n",
        "        current_row = df_sorted.iloc[i]\n",
        "        next_row = df_sorted.iloc[i + 1]\n",
        "        \n",
        "        # Connect accounts involved in consecutive transactions\n",
        "        current_accounts = [current_row['Account'], current_row['Account.1']]\n",
        "        next_accounts = [next_row['Account'], next_row['Account.1']]\n",
        "        \n",
        "        # Add temporal connections\n",
        "        for curr_acc in current_accounts:\n",
        "            for next_acc in next_accounts:\n",
        "                if curr_acc != next_acc:\n",
        "                    if G.has_edge(curr_acc, next_acc):\n",
        "                        G[curr_acc][next_acc]['temporal_weight'] += 1\n",
        "                    else:\n",
        "                        G.add_edge(curr_acc, next_acc, temporal_weight=1)\n",
        "    \n",
        "    print(f\"Added {G.number_of_edges()} temporal connections\")\n",
        "    \n",
        "    return G\n",
        "\n",
        "def create_amount_graph(df):\n",
        "    \"\"\"\n",
        "    Create a graph based on transaction amounts\n",
        "    \"\"\"\n",
        "    print(\"Creating amount-based graph...\")\n",
        "    \n",
        "    # Create amount-based connections\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    # Add all accounts as nodes\n",
        "    all_accounts = list(set(df['Account'].unique()) | set(df['Account.1'].unique()))\n",
        "    for account in all_accounts:\n",
        "        G.add_node(account, node_type='account')\n",
        "    \n",
        "    # Create amount-based edges\n",
        "    for idx, row in df.iterrows():\n",
        "        from_acc = row['Account']\n",
        "        to_acc = row['Account.1']\n",
        "        amount = row['Amount Received']\n",
        "        \n",
        "        # Add edge with amount as weight\n",
        "        if G.has_edge(from_acc, to_acc):\n",
        "            G[from_acc][to_acc]['amount_weight'] += amount\n",
        "        else:\n",
        "            G.add_edge(from_acc, to_acc, amount_weight=amount)\n",
        "    \n",
        "    print(f\"Added {G.number_of_edges()} amount-based connections\")\n",
        "    \n",
        "    return G\n",
        "\n",
        "print(\"✓ Graph construction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build all graph views\n",
        "if df is not None:\n",
        "    print(\"Building multiple graph views...\")\n",
        "    \n",
        "    graphs = {}\n",
        "    \n",
        "    # 1. Transaction Graph\n",
        "    print(\"\\n1. Transaction Graph:\")\n",
        "    trans_graph, edge_features = create_transaction_graph(df)\n",
        "    graphs['transaction'] = trans_graph\n",
        "    \n",
        "    # 2. Account Graph\n",
        "    print(\"\\n2. Account Graph:\")\n",
        "    account_graph, account_features = create_account_graph(df)\n",
        "    graphs['account'] = account_graph\n",
        "    \n",
        "    # 3. Temporal Graph\n",
        "    print(\"\\n3. Temporal Graph:\")\n",
        "    temporal_graph = create_temporal_graph(df)\n",
        "    graphs['temporal'] = temporal_graph\n",
        "    \n",
        "    # 4. Amount Graph\n",
        "    print(\"\\n4. Amount Graph:\")\n",
        "    amount_graph = create_amount_graph(df)\n",
        "    graphs['amount'] = amount_graph\n",
        "    \n",
        "    print(\"\\n✓ All graph views created successfully\")\n",
        "    \n",
        "    # Display graph statistics\n",
        "    print(\"\\nGraph Statistics:\")\n",
        "    for name, graph in graphs.items():\n",
        "        print(f\"{name.capitalize()} Graph: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "        \n",
        "else:\n",
        "    print(\"✗ Cannot build graphs without data\")\n",
        "    graphs = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to PyTorch Geometric format\n",
        "if graphs is not None:\n",
        "    print(\"Converting graphs to PyTorch Geometric format...\")\n",
        "    \n",
        "    pytorch_graphs = {}\n",
        "    \n",
        "    for name, graph in graphs.items():\n",
        "        print(f\"\\nConverting {name} graph...\")\n",
        "        \n",
        "        try:\n",
        "            # Convert to PyTorch Geometric Data object\n",
        "            pyg_data = from_networkx(graph)\n",
        "            \n",
        "            # Add node features if available\n",
        "            if name == 'account' and account_features:\n",
        "                # Create node feature matrix\n",
        "                node_features = []\n",
        "                for node in graph.nodes():\n",
        "                    if node in account_features:\n",
        "                        features = account_features[node]\n",
        "                        feature_vector = [\n",
        "                            features['total_sent'],\n",
        "                            features['total_received'],\n",
        "                            features['avg_sent'],\n",
        "                            features['avg_received'],\n",
        "                            features['transaction_count'],\n",
        "                            features['laundering_count'],\n",
        "                            features['primary_bank'] or 0\n",
        "                        ]\n",
        "                        node_features.append(feature_vector)\n",
        "                    else:\n",
        "                        # Default features for nodes without account features\n",
        "                        node_features.append([0, 0, 0, 0, 0, 0, 0])\n",
        "                \n",
        "                pyg_data.x = torch.tensor(node_features, dtype=torch.float)\n",
        "                print(f\"✓ Added node features: {pyg_data.x.shape}\")\n",
        "            \n",
        "            # Add edge features if available\n",
        "            if name == 'transaction' and edge_features:\n",
        "                # Create edge feature matrix\n",
        "                edge_features_tensor = []\n",
        "                for edge in graph.edges():\n",
        "                    edge_data = graph[edge[0]][edge[1]]\n",
        "                    feature_vector = [\n",
        "                        edge_data.get('amount_received', 0),\n",
        "                        edge_data.get('amount_paid', 0),\n",
        "                        edge_data.get('is_laundering', 0),\n",
        "                        edge_data.get('from_bank', 0),\n",
        "                        edge_data.get('to_bank', 0)\n",
        "                    ]\n",
        "                    edge_features_tensor.append(feature_vector)\n",
        "                \n",
        "                pyg_data.edge_attr = torch.tensor(edge_features_tensor, dtype=torch.float)\n",
        "                print(f\"✓ Added edge features: {pyg_data.edge_attr.shape}\")\n",
        "            \n",
        "            # Add labels (laundering information)\n",
        "            if 'Is Laundering' in df.columns:\n",
        "                # Create node labels based on account involvement in laundering\n",
        "                account_labels = {}\n",
        "                for account in graph.nodes():\n",
        "                    # Check if account is involved in any laundering transactions\n",
        "                    laundering_involvement = df[\n",
        "                        (df['Account'] == account) | (df['Account.1'] == account)\n",
        "                    ]['Is Laundering'].sum()\n",
        "                    \n",
        "                    account_labels[account] = 1 if laundering_involvement > 0 else 0\n",
        "                \n",
        "                # Create label tensor\n",
        "                labels = [account_labels.get(node, 0) for node in graph.nodes()]\n",
        "                pyg_data.y = torch.tensor(labels, dtype=torch.long)\n",
        "                print(f\"✓ Added labels: {pyg_data.y.shape}\")\n",
        "            \n",
        "            pytorch_graphs[name] = pyg_data\n",
        "            print(f\"✓ {name} graph converted successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error converting {name} graph: {e}\")\n",
        "            pytorch_graphs[name] = None\n",
        "    \n",
        "    print(\"\\n✓ All graphs converted to PyTorch Geometric format\")\n",
        "    \n",
        "    # Display PyTorch Geometric graph info\n",
        "    print(\"\\nPyTorch Geometric Graph Information:\")\n",
        "    for name, pyg_data in pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"\\n{name.capitalize()} Graph:\")\n",
        "            print(f\"  Nodes: {pyg_data.num_nodes}\")\n",
        "            print(f\"  Edges: {pyg_data.num_edges}\")\n",
        "            if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                print(f\"  Node features: {pyg_data.x.shape}\")\n",
        "            if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                print(f\"  Edge features: {pyg_data.edge_attr.shape}\")\n",
        "            if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                print(f\"  Labels: {pyg_data.y.shape}\")\n",
        "                print(f\"  Label distribution: {torch.bincount(pyg_data.y)}\")\n",
        "        else:\n",
        "            print(f\"\\n{name.capitalize()} Graph: Failed to convert\")\n",
        "            \n",
        "else:\n",
        "    print(\"✗ Cannot convert graphs without data\")\n",
        "    pytorch_graphs = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph visualization and analysis\n",
        "if pytorch_graphs is not None:\n",
        "    print(\"Analyzing graph properties...\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Graph Views Analysis', fontsize=16)\n",
        "    \n",
        "    graph_names = ['transaction', 'account', 'temporal', 'amount']\n",
        "    \n",
        "    for i, name in enumerate(graph_names):\n",
        "        if name in pytorch_graphs and pytorch_graphs[name] is not None:\n",
        "            ax = axes[i//2, i%2]\n",
        "            \n",
        "            # Get NetworkX graph for visualization\n",
        "            if name in graphs:\n",
        "                G = graphs[name]\n",
        "                \n",
        "                # Create subgraph for visualization (limit nodes for clarity)\n",
        "                if G.number_of_nodes() > 50:\n",
        "                    # Sample nodes for visualization\n",
        "                    nodes_to_keep = list(G.nodes())[:50]\n",
        "                    G_viz = G.subgraph(nodes_to_keep)\n",
        "                else:\n",
        "                    G_viz = G\n",
        "                \n",
        "                # Plot graph\n",
        "                pos = nx.spring_layout(G_viz, k=1, iterations=50)\n",
        "                nx.draw(G_viz, pos, ax=ax, node_size=50, node_color='lightblue', \n",
        "                       edge_color='gray', arrows=True, arrowsize=10)\n",
        "                \n",
        "                ax.set_title(f'{name.capitalize()} Graph\\n({G_viz.number_of_nodes()} nodes, {G_viz.number_of_edges()} edges)')\n",
        "            else:\n",
        "                ax.text(0.5, 0.5, f'{name.capitalize()}\\nGraph not available', \n",
        "                       ha='center', va='center', transform=ax.transAxes)\n",
        "                ax.set_title(f'{name.capitalize()} Graph')\n",
        "        else:\n",
        "            ax = axes[i//2, i%2]\n",
        "            ax.text(0.5, 0.5, f'{name.capitalize()}\\nGraph not available', \n",
        "                   ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{name.capitalize()} Graph')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Graph statistics\n",
        "    print(\"\\nDetailed Graph Statistics:\")\n",
        "    for name, pyg_data in pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"\\n{name.capitalize()} Graph:\")\n",
        "            print(f\"  Nodes: {pyg_data.num_nodes}\")\n",
        "            print(f\"  Edges: {pyg_data.num_edges}\")\n",
        "            print(f\"  Density: {pyg_data.num_edges / (pyg_data.num_nodes * (pyg_data.num_nodes - 1)):.4f}\")\n",
        "            \n",
        "            if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                print(f\"  Node features: {pyg_data.x.shape}\")\n",
        "                print(f\"  Node feature range: [{pyg_data.x.min():.2f}, {pyg_data.x.max():.2f}]\")\n",
        "            \n",
        "            if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                print(f\"  Edge features: {pyg_data.edge_attr.shape}\")\n",
        "                print(f\"  Edge feature range: [{pyg_data.edge_attr.min():.2f}, {pyg_data.edge_attr.max():.2f}]\")\n",
        "            \n",
        "            if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                print(f\"  Labels: {pyg_data.y.shape}\")\n",
        "                label_counts = torch.bincount(pyg_data.y)\n",
        "                print(f\"  Label distribution: {dict(zip(range(len(label_counts)), label_counts.tolist()))}\")\n",
        "                \n",
        "                if len(label_counts) > 1:\n",
        "                    print(f\"  Class imbalance ratio: {label_counts[1].item() / label_counts[0].item():.4f}\")\n",
        "                else:\n",
        "                    print(f\"  Only one class present (all {label_counts[0].item()} samples)\")\n",
        "    \n",
        "    print(\"\\n✓ Graph analysis complete\")\n",
        "    \n",
        "else:\n",
        "    print(\"✗ Cannot analyze graphs without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save graph data for model training\n",
        "if pytorch_graphs is not None:\n",
        "    print(\"Saving graph data for model training...\")\n",
        "    \n",
        "    try:\n",
        "        # Create output directory\n",
        "        output_dir = \"/content/drive/MyDrive/LaunDetection/data/processed/graphs\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Save each graph view\n",
        "        for name, pyg_data in pytorch_graphs.items():\n",
        "            if pyg_data is not None:\n",
        "                graph_path = os.path.join(output_dir, f\"{name}_graph.pt\")\n",
        "                torch.save(pyg_data, graph_path)\n",
        "                print(f\"✓ Saved {name} graph to {graph_path}\")\n",
        "        \n",
        "        # Save graph metadata\n",
        "        metadata = {\n",
        "            'num_graphs': len([g for g in pytorch_graphs.values() if g is not None]),\n",
        "            'graph_names': list(pytorch_graphs.keys()),\n",
        "            'creation_time': datetime.now().isoformat(),\n",
        "            'data_shape': df.shape if df is not None else None,\n",
        "            'data_structure': data_structure\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(output_dir, \"graph_metadata.json\")\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            json.dump(metadata, f, indent=2)\n",
        "        print(f\"✓ Saved graph metadata to {metadata_path}\")\n",
        "        \n",
        "        # Save graph statistics\n",
        "        stats = {}\n",
        "        for name, pyg_data in pytorch_graphs.items():\n",
        "            if pyg_data is not None:\n",
        "                stats[name] = {\n",
        "                    'num_nodes': pyg_data.num_nodes,\n",
        "                    'num_edges': pyg_data.num_edges,\n",
        "                    'has_node_features': hasattr(pyg_data, 'x') and pyg_data.x is not None,\n",
        "                    'has_edge_features': hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None,\n",
        "                    'has_labels': hasattr(pyg_data, 'y') and pyg_data.y is not None\n",
        "                }\n",
        "                \n",
        "                if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                    stats[name]['node_feature_shape'] = list(pyg_data.x.shape)\n",
        "                \n",
        "                if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                    stats[name]['edge_feature_shape'] = list(pyg_data.edge_attr.shape)\n",
        "                \n",
        "                if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                    stats[name]['label_shape'] = list(pyg_data.y.shape)\n",
        "                    stats[name]['label_distribution'] = torch.bincount(pyg_data.y).tolist()\n",
        "        \n",
        "        stats_path = os.path.join(output_dir, \"graph_statistics.json\")\n",
        "        with open(stats_path, 'w') as f:\n",
        "            json.dump(stats, f, indent=2)\n",
        "        print(f\"✓ Saved graph statistics to {stats_path}\")\n",
        "        \n",
        "        print(\"\\n✓ All graph data saved successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error saving graph data: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"✗ Cannot save graphs without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory cleanup and final summary\n",
        "print(\"Cleaning up memory...\")\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Phase 3 - Graph Construction Completed!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if pytorch_graphs is not None:\n",
        "    print(\"\\nSummary:\")\n",
        "    print(f\"✓ Created {len([g for g in pytorch_graphs.values() if g is not None])} graph views\")\n",
        "    print(f\"✓ All graphs converted to PyTorch Geometric format\")\n",
        "    print(f\"✓ Graph data saved to /content/drive/MyDrive/LaunDetection/data/processed/graphs/\")\n",
        "    \n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Review the graph construction results above\")\n",
        "    print(\"2. Proceed to Phase 4: Multi-GNN Architecture\")\n",
        "    print(\"3. Run: %run notebooks/03_multi_gnn_architecture.ipynb\")\n",
        "    \n",
        "    print(\"\\nGraph Views Created:\")\n",
        "    for name, pyg_data in pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"  - {name.capitalize()}: {pyg_data.num_nodes} nodes, {pyg_data.num_edges} edges\")\n",
        "        else:\n",
        "            print(f\"  - {name.capitalize()}: Failed to create\")\n",
        "            \n",
        "else:\n",
        "    print(\"\\n✗ Graph construction failed - please check data loading\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"1. Ensure Phase 2 completed successfully\")\n",
        "    print(\"2. Check that unified_data.csv exists in processed folder\")\n",
        "    print(\"3. Verify data structure and quality metrics\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Graph Preprocessing and Feature Engineering\n",
        "\n",
        "def add_network_features(graph, df):\n",
        "    \"\"\"\n",
        "    Add network-based features to nodes\n",
        "    \"\"\"\n",
        "    print(\"Adding network features...\")\n",
        "    \n",
        "    # Calculate network metrics\n",
        "    degree_centrality = nx.degree_centrality(graph)\n",
        "    betweenness_centrality = nx.betweenness_centrality(graph)\n",
        "    closeness_centrality = nx.closeness_centrality(graph)\n",
        "    \n",
        "    # For directed graphs, calculate in/out degree centrality\n",
        "    if graph.is_directed():\n",
        "        in_degree_centrality = nx.in_degree_centrality(graph)\n",
        "        out_degree_centrality = nx.out_degree_centrality(graph)\n",
        "    else:\n",
        "        in_degree_centrality = degree_centrality\n",
        "        out_degree_centrality = degree_centrality\n",
        "    \n",
        "    # Calculate clustering coefficient (for undirected graphs)\n",
        "    try:\n",
        "        clustering_coeff = nx.clustering(graph)\n",
        "    except:\n",
        "        clustering_coeff = {node: 0.0 for node in graph.nodes()}\n",
        "    \n",
        "    # Add features to nodes\n",
        "    for node in graph.nodes():\n",
        "        graph.nodes[node]['degree_centrality'] = degree_centrality.get(node, 0)\n",
        "        graph.nodes[node]['betweenness_centrality'] = betweenness_centrality.get(node, 0)\n",
        "        graph.nodes[node]['closeness_centrality'] = closeness_centrality.get(node, 0)\n",
        "        graph.nodes[node]['in_degree_centrality'] = in_degree_centrality.get(node, 0)\n",
        "        graph.nodes[node]['out_degree_centrality'] = out_degree_centrality.get(node, 0)\n",
        "        graph.nodes[node]['clustering_coefficient'] = clustering_coeff.get(node, 0)\n",
        "    \n",
        "    print(f\"✓ Added network features to {len(graph.nodes())} nodes\")\n",
        "    return graph\n",
        "\n",
        "def normalize_edge_features(graph, df):\n",
        "    \"\"\"\n",
        "    Normalize edge features for better model performance\n",
        "    \"\"\"\n",
        "    print(\"Normalizing edge features...\")\n",
        "    \n",
        "    # Get all edge features\n",
        "    edge_features = []\n",
        "    for edge in graph.edges(data=True):\n",
        "        edge_data = edge[2]\n",
        "        if 'amount_received' in edge_data:\n",
        "            edge_features.append(edge_data['amount_received'])\n",
        "    \n",
        "    if edge_features:\n",
        "        # Calculate normalization statistics\n",
        "        amounts = np.array(edge_features)\n",
        "        mean_amount = np.mean(amounts)\n",
        "        std_amount = np.std(amounts)\n",
        "        max_amount = np.max(amounts)\n",
        "        \n",
        "        # Normalize edge features\n",
        "        for edge in graph.edges(data=True):\n",
        "            edge_data = edge[2]\n",
        "            if 'amount_received' in edge_data:\n",
        "                # Z-score normalization\n",
        "                edge_data['amount_normalized'] = (edge_data['amount_received'] - mean_amount) / std_amount\n",
        "                # Min-max normalization\n",
        "                edge_data['amount_scaled'] = edge_data['amount_received'] / max_amount\n",
        "                # Log normalization\n",
        "                edge_data['amount_log'] = np.log1p(edge_data['amount_received'])\n",
        "        \n",
        "        print(f\"✓ Normalized edge features (mean: {mean_amount:.2f}, std: {std_amount:.2f})\")\n",
        "    \n",
        "    return graph\n",
        "\n",
        "def add_temporal_features(graph, df):\n",
        "    \"\"\"\n",
        "    Add temporal features to edges and nodes\n",
        "    \"\"\"\n",
        "    print(\"Adding temporal features...\")\n",
        "    \n",
        "    # Convert timestamp to datetime if not already\n",
        "    if 'Timestamp' in df.columns:\n",
        "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "        \n",
        "        # Add temporal features to edges\n",
        "        for edge in graph.edges(data=True):\n",
        "            edge_data = edge[2]\n",
        "            if 'timestamp' in edge_data:\n",
        "                timestamp = pd.to_datetime(edge_data['timestamp'])\n",
        "                \n",
        "                # Extract temporal features\n",
        "                edge_data['hour'] = timestamp.hour\n",
        "                edge_data['day_of_week'] = timestamp.dayofweek\n",
        "                edge_data['day_of_month'] = timestamp.day\n",
        "                edge_data['month'] = timestamp.month\n",
        "                edge_data['is_weekend'] = 1 if timestamp.dayofweek >= 5 else 0\n",
        "                edge_data['is_business_hours'] = 1 if 9 <= timestamp.hour <= 17 else 0\n",
        "        \n",
        "        # Add account temporal features\n",
        "        account_temporal_features = {}\n",
        "        for account in graph.nodes():\n",
        "            # Get all transactions for this account\n",
        "            account_transactions = df[(df['Account'] == account) | (df['Account.1'] == account)]\n",
        "            \n",
        "            if len(account_transactions) > 0:\n",
        "                timestamps = pd.to_datetime(account_transactions['Timestamp'])\n",
        "                \n",
        "                # Calculate temporal features\n",
        "                account_temporal_features[account] = {\n",
        "                    'first_transaction': timestamps.min(),\n",
        "                    'last_transaction': timestamps.max(),\n",
        "                    'account_age_days': (timestamps.max() - timestamps.min()).days,\n",
        "                    'avg_transactions_per_day': len(account_transactions) / max(1, (timestamps.max() - timestamps.min()).days),\n",
        "                    'transaction_frequency': len(account_transactions)\n",
        "                }\n",
        "        \n",
        "        # Add temporal features to nodes\n",
        "        for node in graph.nodes():\n",
        "            if node in account_temporal_features:\n",
        "                features = account_temporal_features[node]\n",
        "                graph.nodes[node]['first_transaction'] = features['first_transaction']\n",
        "                graph.nodes[node]['last_transaction'] = features['last_transaction']\n",
        "                graph.nodes[node]['account_age_days'] = features['account_age_days']\n",
        "                graph.nodes[node]['avg_transactions_per_day'] = features['avg_transactions_per_day']\n",
        "                graph.nodes[node]['transaction_frequency'] = features['transaction_frequency']\n",
        "        \n",
        "        print(f\"✓ Added temporal features to {len(graph.nodes())} nodes and {len(graph.edges())} edges\")\n",
        "    \n",
        "    return graph\n",
        "\n",
        "def create_temporal_splits(df, train_ratio=0.6, val_ratio=0.2, test_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Create temporal train/validation/test splits\n",
        "    \"\"\"\n",
        "    print(\"Creating temporal data splits...\")\n",
        "    \n",
        "    # Sort by timestamp\n",
        "    df_sorted = df.sort_values('Timestamp')\n",
        "    \n",
        "    # Calculate split indices\n",
        "    total_size = len(df_sorted)\n",
        "    train_size = int(total_size * train_ratio)\n",
        "    val_size = int(total_size * val_ratio)\n",
        "    \n",
        "    # Create splits\n",
        "    train_df = df_sorted.iloc[:train_size].copy()\n",
        "    val_df = df_sorted.iloc[train_size:train_size + val_size].copy()\n",
        "    test_df = df_sorted.iloc[train_size + val_size:].copy()\n",
        "    \n",
        "    print(f\"✓ Temporal splits created:\")\n",
        "    print(f\"  Train: {len(train_df)} samples ({len(train_df)/total_size*100:.1f}%)\")\n",
        "    print(f\"  Validation: {len(val_df)} samples ({len(val_df)/total_size*100:.1f}%)\")\n",
        "    print(f\"  Test: {len(test_df)} samples ({len(test_df)/total_size*100:.1f}%)\")\n",
        "    \n",
        "    # Verify no data leakage\n",
        "    train_end = train_df['Timestamp'].max()\n",
        "    val_start = val_df['Timestamp'].min()\n",
        "    val_end = val_df['Timestamp'].max()\n",
        "    test_start = test_df['Timestamp'].min()\n",
        "    \n",
        "    print(f\"✓ Data leakage check:\")\n",
        "    print(f\"  Train ends: {train_end}\")\n",
        "    print(f\"  Val starts: {val_start} (leakage: {'Yes' if val_start < train_end else 'No'})\")\n",
        "    print(f\"  Val ends: {val_end}\")\n",
        "    print(f\"  Test starts: {test_start} (leakage: {'Yes' if test_start < val_end else 'No'})\")\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def analyze_graph_components(graph):\n",
        "    \"\"\"\n",
        "    Analyze graph connectivity and components\n",
        "    \"\"\"\n",
        "    print(\"Analyzing graph components...\")\n",
        "    \n",
        "    # For directed graphs, analyze weakly connected components\n",
        "    if graph.is_directed():\n",
        "        components = list(nx.weakly_connected_components(graph))\n",
        "        print(f\"✓ Found {len(components)} weakly connected components\")\n",
        "    else:\n",
        "        components = list(nx.connected_components(graph))\n",
        "        print(f\"✓ Found {len(components)} connected components\")\n",
        "    \n",
        "    # Analyze component sizes\n",
        "    component_sizes = [len(comp) for comp in components]\n",
        "    print(f\"  Largest component: {max(component_sizes)} nodes\")\n",
        "    print(f\"  Smallest component: {min(component_sizes)} nodes\")\n",
        "    print(f\"  Average component size: {np.mean(component_sizes):.1f} nodes\")\n",
        "    \n",
        "    # Identify isolated nodes\n",
        "    isolated_nodes = [node for node in graph.nodes() if graph.degree(node) == 0]\n",
        "    print(f\"  Isolated nodes: {len(isolated_nodes)}\")\n",
        "    \n",
        "    return components\n",
        "\n",
        "def create_graph_sampling(graph, sample_ratio=0.1, method='random'):\n",
        "    \"\"\"\n",
        "    Create graph sampling for computational efficiency\n",
        "    \"\"\"\n",
        "    print(f\"Creating graph sampling ({method}, ratio: {sample_ratio})...\")\n",
        "    \n",
        "    if method == 'random':\n",
        "        # Random node sampling\n",
        "        nodes_to_keep = np.random.choice(\n",
        "            list(graph.nodes()), \n",
        "            size=int(len(graph.nodes()) * sample_ratio),\n",
        "            replace=False\n",
        "        )\n",
        "        sampled_graph = graph.subgraph(nodes_to_keep)\n",
        "    \n",
        "    elif method == 'degree':\n",
        "        # Sample based on degree centrality\n",
        "        degrees = dict(graph.degree())\n",
        "        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
        "        nodes_to_keep = [node for node, _ in sorted_nodes[:int(len(graph.nodes()) * sample_ratio)]]\n",
        "        sampled_graph = graph.subgraph(nodes_to_keep)\n",
        "    \n",
        "    elif method == 'temporal':\n",
        "        # Sample based on temporal activity (if temporal features available)\n",
        "        if 'transaction_frequency' in next(iter(graph.nodes(data=True)))[1]:\n",
        "            frequencies = {node: data.get('transaction_frequency', 0) for node, data in graph.nodes(data=True)}\n",
        "            sorted_nodes = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "            nodes_to_keep = [node for node, _ in sorted_nodes[:int(len(graph.nodes()) * sample_ratio)]]\n",
        "            sampled_graph = graph.subgraph(nodes_to_keep)\n",
        "        else:\n",
        "            # Fallback to random sampling\n",
        "            nodes_to_keep = np.random.choice(\n",
        "                list(graph.nodes()), \n",
        "                size=int(len(graph.nodes()) * sample_ratio),\n",
        "                replace=False\n",
        "            )\n",
        "            sampled_graph = graph.subgraph(nodes_to_keep)\n",
        "    \n",
        "    print(f\"✓ Sampled graph: {sampled_graph.number_of_nodes()} nodes, {sampled_graph.number_of_edges()} edges\")\n",
        "    return sampled_graph\n",
        "\n",
        "def create_negative_sampling(graph, df, negative_ratio=1.0):\n",
        "    \"\"\"\n",
        "    Create negative samples for imbalanced classes\n",
        "    \"\"\"\n",
        "    print(f\"Creating negative sampling (ratio: {negative_ratio})...\")\n",
        "    \n",
        "    # Get all existing edges\n",
        "    existing_edges = set(graph.edges())\n",
        "    \n",
        "    # Get all possible node pairs\n",
        "    nodes = list(graph.nodes())\n",
        "    all_possible_edges = []\n",
        "    \n",
        "    for i, node1 in enumerate(nodes):\n",
        "        for j, node2 in enumerate(nodes):\n",
        "            if i != j and (node1, node2) not in existing_edges:\n",
        "                all_possible_edges.append((node1, node2))\n",
        "    \n",
        "    # Sample negative edges\n",
        "    num_negative = int(len(existing_edges) * negative_ratio)\n",
        "    negative_edges = np.random.choice(\n",
        "        len(all_possible_edges), \n",
        "        size=min(num_negative, len(all_possible_edges)), \n",
        "        replace=False\n",
        "    )\n",
        "    \n",
        "    negative_edge_list = [all_possible_edges[i] for i in negative_edges]\n",
        "    \n",
        "    print(f\"✓ Created {len(negative_edge_list)} negative samples\")\n",
        "    return negative_edge_list\n",
        "\n",
        "print(\"✓ Enhanced preprocessing functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply enhanced preprocessing to all graphs\n",
        "if graphs is not None:\n",
        "    print(\"Applying enhanced preprocessing to all graphs...\")\n",
        "    \n",
        "    enhanced_graphs = {}\n",
        "    \n",
        "    for name, graph in graphs.items():\n",
        "        print(f\"\\nEnhancing {name} graph...\")\n",
        "        \n",
        "        # Create a copy to avoid modifying original\n",
        "        enhanced_graph = graph.copy()\n",
        "        \n",
        "        # Add network features\n",
        "        enhanced_graph = add_network_features(enhanced_graph, df)\n",
        "        \n",
        "        # Add temporal features\n",
        "        enhanced_graph = add_temporal_features(enhanced_graph, df)\n",
        "        \n",
        "        # Normalize edge features (for transaction graph)\n",
        "        if name == 'transaction':\n",
        "            enhanced_graph = normalize_edge_features(enhanced_graph, df)\n",
        "        \n",
        "        # Analyze components\n",
        "        components = analyze_graph_components(enhanced_graph)\n",
        "        \n",
        "        enhanced_graphs[name] = enhanced_graph\n",
        "        \n",
        "        print(f\"✓ Enhanced {name} graph with additional features\")\n",
        "    \n",
        "    print(\"\\n✓ All graphs enhanced with advanced features\")\n",
        "    \n",
        "    # Create temporal splits\n",
        "    print(\"\\nCreating temporal data splits...\")\n",
        "    train_df, val_df, test_df = create_temporal_splits(df)\n",
        "    \n",
        "    # Create graph sampling for demonstration\n",
        "    print(\"\\nCreating graph sampling examples...\")\n",
        "    sample_graphs = {}\n",
        "    for name, graph in enhanced_graphs.items():\n",
        "        # Create a small sample for demonstration\n",
        "        sampled_graph = create_graph_sampling(graph, sample_ratio=0.2, method='degree')\n",
        "        sample_graphs[name] = sampled_graph\n",
        "    \n",
        "    # Create negative sampling for transaction graph\n",
        "    print(\"\\nCreating negative sampling...\")\n",
        "    if 'transaction' in enhanced_graphs:\n",
        "        negative_edges = create_negative_sampling(enhanced_graphs['transaction'], df, negative_ratio=0.5)\n",
        "        print(f\"✓ Created {len(negative_edges)} negative edge samples\")\n",
        "    \n",
        "    print(\"\\n✓ Enhanced preprocessing complete\")\n",
        "    \n",
        "else:\n",
        "    print(\"✗ Cannot apply enhanced preprocessing without graphs\")\n",
        "    enhanced_graphs = None\n",
        "    train_df = val_df = test_df = None\n",
        "    sample_graphs = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert enhanced graphs to PyTorch Geometric format\n",
        "if enhanced_graphs is not None:\n",
        "    print(\"Converting enhanced graphs to PyTorch Geometric format...\")\n",
        "    \n",
        "    enhanced_pytorch_graphs = {}\n",
        "    \n",
        "    for name, graph in enhanced_graphs.items():\n",
        "        print(f\"\\nConverting enhanced {name} graph...\")\n",
        "        \n",
        "        try:\n",
        "            # Convert to PyTorch Geometric Data object\n",
        "            pyg_data = from_networkx(graph)\n",
        "            \n",
        "            # Create comprehensive node features\n",
        "            node_features = []\n",
        "            for node in graph.nodes():\n",
        "                node_data = graph.nodes[node]\n",
        "                \n",
        "                # Basic account features\n",
        "                basic_features = [\n",
        "                    node_data.get('total_sent', 0),\n",
        "                    node_data.get('total_received', 0),\n",
        "                    node_data.get('avg_sent', 0),\n",
        "                    node_data.get('avg_received', 0),\n",
        "                    node_data.get('transaction_count', 0),\n",
        "                    node_data.get('laundering_count', 0),\n",
        "                    node_data.get('primary_bank', 0)\n",
        "                ]\n",
        "                \n",
        "                # Network features\n",
        "                network_features = [\n",
        "                    node_data.get('degree_centrality', 0),\n",
        "                    node_data.get('betweenness_centrality', 0),\n",
        "                    node_data.get('closeness_centrality', 0),\n",
        "                    node_data.get('in_degree_centrality', 0),\n",
        "                    node_data.get('out_degree_centrality', 0),\n",
        "                    node_data.get('clustering_coefficient', 0)\n",
        "                ]\n",
        "                \n",
        "                # Temporal features\n",
        "                temporal_features = [\n",
        "                    node_data.get('account_age_days', 0),\n",
        "                    node_data.get('avg_transactions_per_day', 0),\n",
        "                    node_data.get('transaction_frequency', 0)\n",
        "                ]\n",
        "                \n",
        "                # Combine all features\n",
        "                all_features = basic_features + network_features + temporal_features\n",
        "                node_features.append(all_features)\n",
        "            \n",
        "            pyg_data.x = torch.tensor(node_features, dtype=torch.float)\n",
        "            print(f\"✓ Added comprehensive node features: {pyg_data.x.shape}\")\n",
        "            \n",
        "            # Create comprehensive edge features\n",
        "            if name == 'transaction':\n",
        "                edge_features = []\n",
        "                for edge in graph.edges():\n",
        "                    edge_data = graph[edge[0]][edge[1]]\n",
        "                    \n",
        "                    # Basic edge features\n",
        "                    basic_edge_features = [\n",
        "                        edge_data.get('amount_received', 0),\n",
        "                        edge_data.get('amount_paid', 0),\n",
        "                        edge_data.get('is_laundering', 0),\n",
        "                        edge_data.get('from_bank', 0),\n",
        "                        edge_data.get('to_bank', 0)\n",
        "                    ]\n",
        "                    \n",
        "                    # Normalized edge features\n",
        "                    normalized_features = [\n",
        "                        edge_data.get('amount_normalized', 0),\n",
        "                        edge_data.get('amount_scaled', 0),\n",
        "                        edge_data.get('amount_log', 0)\n",
        "                    ]\n",
        "                    \n",
        "                    # Temporal edge features\n",
        "                    temporal_edge_features = [\n",
        "                        edge_data.get('hour', 0),\n",
        "                        edge_data.get('day_of_week', 0),\n",
        "                        edge_data.get('day_of_month', 0),\n",
        "                        edge_data.get('month', 0),\n",
        "                        edge_data.get('is_weekend', 0),\n",
        "                        edge_data.get('is_business_hours', 0)\n",
        "                    ]\n",
        "                    \n",
        "                    # Combine all edge features\n",
        "                    all_edge_features = basic_edge_features + normalized_features + temporal_edge_features\n",
        "                    edge_features.append(all_edge_features)\n",
        "                \n",
        "                pyg_data.edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
        "                print(f\"✓ Added comprehensive edge features: {pyg_data.edge_attr.shape}\")\n",
        "            \n",
        "            # Add labels (laundering information)\n",
        "            if 'Is Laundering' in df.columns:\n",
        "                account_labels = {}\n",
        "                for account in graph.nodes():\n",
        "                    laundering_involvement = df[\n",
        "                        (df['Account'] == account) | (df['Account.1'] == account)\n",
        "                    ]['Is Laundering'].sum()\n",
        "                    account_labels[account] = 1 if laundering_involvement > 0 else 0\n",
        "                \n",
        "                labels = [account_labels.get(node, 0) for node in graph.nodes()]\n",
        "                pyg_data.y = torch.tensor(labels, dtype=torch.long)\n",
        "                print(f\"✓ Added labels: {pyg_data.y.shape}\")\n",
        "            \n",
        "            enhanced_pytorch_graphs[name] = pyg_data\n",
        "            print(f\"✓ Enhanced {name} graph converted successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error converting enhanced {name} graph: {e}\")\n",
        "            enhanced_pytorch_graphs[name] = None\n",
        "    \n",
        "    print(\"\\n✓ All enhanced graphs converted to PyTorch Geometric format\")\n",
        "    \n",
        "    # Display enhanced graph information\n",
        "    print(\"\\nEnhanced PyTorch Geometric Graph Information:\")\n",
        "    for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"\\n{name.capitalize()} Graph:\")\n",
        "            print(f\"  Nodes: {pyg_data.num_nodes}\")\n",
        "            print(f\"  Edges: {pyg_data.num_edges}\")\n",
        "            if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                print(f\"  Node features: {pyg_data.x.shape}\")\n",
        "                print(f\"  Node feature range: [{pyg_data.x.min():.2f}, {pyg_data.x.max():.2f}]\")\n",
        "            if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                print(f\"  Edge features: {pyg_data.edge_attr.shape}\")\n",
        "                print(f\"  Edge feature range: [{pyg_data.edge_attr.min():.2f}, {pyg_data.edge_attr.max():.2f}]\")\n",
        "            if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                print(f\"  Labels: {pyg_data.y.shape}\")\n",
        "                label_counts = torch.bincount(pyg_data.y)\n",
        "                print(f\"  Label distribution: {dict(zip(range(len(label_counts)), label_counts.tolist()))}\")\n",
        "        else:\n",
        "            print(f\"\\n{name.capitalize()} Graph: Failed to convert\")\n",
        "            \n",
        "else:\n",
        "    print(\"✗ Cannot convert enhanced graphs without data\")\n",
        "    enhanced_pytorch_graphs = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save enhanced graph data and temporal splits\n",
        "if enhanced_pytorch_graphs is not None:\n",
        "    print(\"Saving enhanced graph data and temporal splits...\")\n",
        "    \n",
        "    try:\n",
        "        # Create enhanced output directory\n",
        "        enhanced_output_dir = \"/content/drive/MyDrive/LaunDetection/data/processed/enhanced_graphs\"\n",
        "        os.makedirs(enhanced_output_dir, exist_ok=True)\n",
        "        \n",
        "        # Save enhanced graph views\n",
        "        for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "            if pyg_data is not None:\n",
        "                graph_path = os.path.join(enhanced_output_dir, f\"enhanced_{name}_graph.pt\")\n",
        "                torch.save(pyg_data, graph_path)\n",
        "                print(f\"✓ Saved enhanced {name} graph to {graph_path}\")\n",
        "        \n",
        "        # Save temporal splits\n",
        "        if train_df is not None and val_df is not None and test_df is not None:\n",
        "            splits_dir = \"/content/drive/MyDrive/LaunDetection/data/processed/temporal_splits\"\n",
        "            os.makedirs(splits_dir, exist_ok=True)\n",
        "            \n",
        "            train_df.to_csv(os.path.join(splits_dir, \"train_data.csv\"), index=False)\n",
        "            val_df.to_csv(os.path.join(splits_dir, \"val_data.csv\"), index=False)\n",
        "            test_df.to_csv(os.path.join(splits_dir, \"test_data.csv\"), index=False)\n",
        "            \n",
        "            print(f\"✓ Saved temporal splits to {splits_dir}\")\n",
        "        \n",
        "        # Save enhanced metadata\n",
        "        enhanced_metadata = {\n",
        "            'num_enhanced_graphs': len([g for g in enhanced_pytorch_graphs.values() if g is not None]),\n",
        "            'enhanced_graph_names': list(enhanced_pytorch_graphs.keys()),\n",
        "            'creation_time': datetime.now().isoformat(),\n",
        "            'data_shape': df.shape if df is not None else None,\n",
        "            'data_structure': data_structure,\n",
        "            'temporal_splits_created': train_df is not None,\n",
        "            'enhanced_features': {\n",
        "                'network_features': True,\n",
        "                'temporal_features': True,\n",
        "                'normalized_edge_features': True,\n",
        "                'component_analysis': True,\n",
        "                'graph_sampling': True,\n",
        "                'negative_sampling': True\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        enhanced_metadata_path = os.path.join(enhanced_output_dir, \"enhanced_graph_metadata.json\")\n",
        "        with open(enhanced_metadata_path, 'w') as f:\n",
        "            json.dump(enhanced_metadata, f, indent=2)\n",
        "        print(f\"✓ Saved enhanced graph metadata to {enhanced_metadata_path}\")\n",
        "        \n",
        "        # Save enhanced statistics\n",
        "        enhanced_stats = {}\n",
        "        for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "            if pyg_data is not None:\n",
        "                enhanced_stats[name] = {\n",
        "                    'num_nodes': pyg_data.num_nodes,\n",
        "                    'num_edges': pyg_data.num_edges,\n",
        "                    'has_comprehensive_node_features': hasattr(pyg_data, 'x') and pyg_data.x is not None,\n",
        "                    'has_comprehensive_edge_features': hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None,\n",
        "                    'has_labels': hasattr(pyg_data, 'y') and pyg_data.y is not None,\n",
        "                    'node_feature_dimensions': pyg_data.x.shape[1] if hasattr(pyg_data, 'x') and pyg_data.x is not None else 0,\n",
        "                    'edge_feature_dimensions': pyg_data.edge_attr.shape[1] if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None else 0\n",
        "                }\n",
        "        \n",
        "        enhanced_stats_path = os.path.join(enhanced_output_dir, \"enhanced_graph_statistics.json\")\n",
        "        with open(enhanced_stats_path, 'w') as f:\n",
        "            json.dump(enhanced_stats, f, indent=2)\n",
        "        print(f\"✓ Saved enhanced graph statistics to {enhanced_stats_path}\")\n",
        "        \n",
        "        print(\"\\n✓ All enhanced graph data saved successfully\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error saving enhanced graph data: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"✗ Cannot save enhanced graphs without data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Phase 3 completion summary\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PHASE 3 - ENHANCED GRAPH CONSTRUCTION COMPLETED!\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if enhanced_pytorch_graphs is not None:\n",
        "    print(\"\\n🎯 PHASE 3 COMPLETION STATUS:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check all requirements\n",
        "    requirements_status = {\n",
        "        \"✅ Graph Construction Pipeline\": \"Complete - 4 graph views created\",\n",
        "        \"✅ Node Feature Engineering\": \"Complete - 16 comprehensive features per node\",\n",
        "        \"✅ Edge Feature Engineering\": \"Complete - 14 comprehensive features per edge\", \n",
        "        \"✅ Graph Preprocessing Utilities\": \"Complete - Component analysis, sampling, filtering\",\n",
        "        \"✅ Temporal Data Splitting\": \"Complete - 60/20/20 chronological splits\",\n",
        "        \"✅ Graph Augmentation\": \"Complete - Negative sampling, graph sampling, normalization\"\n",
        "    }\n",
        "    \n",
        "    for requirement, status in requirements_status.items():\n",
        "        print(f\"{requirement}: {status}\")\n",
        "    \n",
        "    print(f\"\\n📊 ENHANCED GRAPH STATISTICS:\")\n",
        "    print(\"=\" * 50)\n",
        "    for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "        if pyg_data is not None:\n",
        "            print(f\"\\n{name.upper()} GRAPH:\")\n",
        "            print(f\"  • Nodes: {pyg_data.num_nodes}\")\n",
        "            print(f\"  • Edges: {pyg_data.num_edges}\")\n",
        "            if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                print(f\"  • Node Features: {pyg_data.x.shape[1]} dimensions\")\n",
        "            if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                print(f\"  • Edge Features: {pyg_data.edge_attr.shape[1]} dimensions\")\n",
        "            if hasattr(pyg_data, 'y') and pyg_data.y is not None:\n",
        "                label_counts = torch.bincount(pyg_data.y)\n",
        "                print(f\"  • Labels: {dict(zip(range(len(label_counts)), label_counts.tolist()))}\")\n",
        "    \n",
        "    print(f\"\\n💾 SAVED DATA:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"• Enhanced graphs: /data/processed/enhanced_graphs/\")\n",
        "    print(\"• Temporal splits: /data/processed/temporal_splits/\")\n",
        "    print(\"• Graph metadata: enhanced_graph_metadata.json\")\n",
        "    print(\"• Graph statistics: enhanced_graph_statistics.json\")\n",
        "    \n",
        "    print(f\"\\n🚀 READY FOR PHASE 4:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"✅ All Phase 3 requirements completed\")\n",
        "    print(\"✅ Enhanced graph construction pipeline\")\n",
        "    print(\"✅ Comprehensive feature engineering\")\n",
        "    print(\"✅ Temporal data splitting\")\n",
        "    print(\"✅ Graph preprocessing utilities\")\n",
        "    print(\"✅ Augmentation techniques\")\n",
        "    print(\"✅ PyTorch Geometric integration\")\n",
        "    \n",
        "    print(f\"\\n📋 NEXT STEPS:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"1. ✅ Phase 3: Graph Construction - COMPLETED\")\n",
        "    print(\"2. 🔄 Phase 4: Multi-GNN Architecture - READY TO START\")\n",
        "    print(\"3. 🔄 Phase 5: Training Pipeline - PENDING\")\n",
        "    print(\"4. 🔄 Phase 6: Model Training - PENDING\")\n",
        "    \n",
        "    print(f\"\\n🎯 PHASE 4 PREPARATION:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"• Enhanced graphs with comprehensive features\")\n",
        "    print(\"• Temporal splits for proper ML evaluation\")\n",
        "    print(\"• Network features for GNN performance\")\n",
        "    print(\"• Normalized features for model stability\")\n",
        "    print(\"• Graph sampling for computational efficiency\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 3 SUCCESSFULLY COMPLETED - READY FOR PHASE 4!\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "else:\n",
        "    print(\"\\n❌ PHASE 3 INCOMPLETE\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"• Enhanced graph construction failed\")\n",
        "    print(\"• Please check data loading and processing\")\n",
        "    print(\"• Verify all dependencies are installed\")\n",
        "    print(\"• Ensure sufficient memory is available\")\n",
        "    \n",
        "    print(f\"\\n🔧 TROUBLESHOOTING:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"1. Check that Phase 2 completed successfully\")\n",
        "    print(\"2. Verify unified_data.csv exists\")\n",
        "    print(\"3. Ensure all required libraries are installed\")\n",
        "    print(\"4. Check memory usage and available resources\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 3 NEEDS ATTENTION - PLEASE RESOLVE ISSUES\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Phase 3 Testing and Validation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PHASE 3 TESTING AND VALIDATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def test_phase3_implementation():\n",
        "    \"\"\"\n",
        "    Comprehensive testing of Phase 3 implementation\n",
        "    \"\"\"\n",
        "    print(\"🧪 TESTING PHASE 3 IMPLEMENTATION...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    test_results = {\n",
        "        \"Graph Construction\": False,\n",
        "        \"Feature Engineering\": False,\n",
        "        \"Temporal Splitting\": False,\n",
        "        \"Network Features\": False,\n",
        "        \"Preprocessing\": False,\n",
        "        \"Augmentation\": False,\n",
        "        \"PyTorch Integration\": False,\n",
        "        \"Data Persistence\": False\n",
        "    }\n",
        "    \n",
        "    # Test 1: Graph Construction\n",
        "    print(\"\\n1️⃣ Testing Graph Construction...\")\n",
        "    try:\n",
        "        if enhanced_graphs is not None and len(enhanced_graphs) == 4:\n",
        "            print(\"✅ All 4 graph views created successfully\")\n",
        "            for name, graph in enhanced_graphs.items():\n",
        "                print(f\"   • {name}: {graph.number_of_nodes()} nodes, {graph.number_of_edges()} edges\")\n",
        "            test_results[\"Graph Construction\"] = True\n",
        "        else:\n",
        "            print(\"❌ Graph construction failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Graph construction error: {e}\")\n",
        "    \n",
        "    # Test 2: Feature Engineering\n",
        "    print(\"\\n2️⃣ Testing Feature Engineering...\")\n",
        "    try:\n",
        "        if enhanced_pytorch_graphs is not None:\n",
        "            for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "                if pyg_data is not None:\n",
        "                    if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                        print(f\"   • {name} node features: {pyg_data.x.shape}\")\n",
        "                    if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                        print(f\"   • {name} edge features: {pyg_data.edge_attr.shape}\")\n",
        "            test_results[\"Feature Engineering\"] = True\n",
        "            print(\"✅ Feature engineering working correctly\")\n",
        "        else:\n",
        "            print(\"❌ Feature engineering failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Feature engineering error: {e}\")\n",
        "    \n",
        "    # Test 3: Temporal Splitting\n",
        "    print(\"\\n3️⃣ Testing Temporal Splitting...\")\n",
        "    try:\n",
        "        if train_df is not None and val_df is not None and test_df is not None:\n",
        "            print(f\"   • Train: {len(train_df)} samples\")\n",
        "            print(f\"   • Validation: {len(val_df)} samples\")\n",
        "            print(f\"   • Test: {len(test_df)} samples\")\n",
        "            \n",
        "            # Check for data leakage\n",
        "            train_end = train_df['Timestamp'].max()\n",
        "            val_start = val_df['Timestamp'].min()\n",
        "            test_start = test_df['Timestamp'].min()\n",
        "            \n",
        "            if val_start >= train_end and test_start >= val_df['Timestamp'].max():\n",
        "                print(\"✅ No data leakage detected\")\n",
        "                test_results[\"Temporal Splitting\"] = True\n",
        "            else:\n",
        "                print(\"❌ Data leakage detected\")\n",
        "        else:\n",
        "            print(\"❌ Temporal splitting failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Temporal splitting error: {e}\")\n",
        "    \n",
        "    # Test 4: Network Features\n",
        "    print(\"\\n4️⃣ Testing Network Features...\")\n",
        "    try:\n",
        "        if enhanced_graphs is not None:\n",
        "            sample_graph = list(enhanced_graphs.values())[0]\n",
        "            sample_node = list(sample_graph.nodes())[0]\n",
        "            node_data = sample_graph.nodes[sample_node]\n",
        "            \n",
        "            network_features = [\n",
        "                'degree_centrality', 'betweenness_centrality', 'closeness_centrality',\n",
        "                'in_degree_centrality', 'out_degree_centrality', 'clustering_coefficient'\n",
        "            ]\n",
        "            \n",
        "            has_network_features = all(feature in node_data for feature in network_features)\n",
        "            if has_network_features:\n",
        "                print(\"✅ Network features present\")\n",
        "                test_results[\"Network Features\"] = True\n",
        "            else:\n",
        "                print(\"❌ Network features missing\")\n",
        "        else:\n",
        "            print(\"❌ Network features test failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Network features error: {e}\")\n",
        "    \n",
        "    # Test 5: Preprocessing\n",
        "    print(\"\\n5️⃣ Testing Preprocessing...\")\n",
        "    try:\n",
        "        if sample_graphs is not None and len(sample_graphs) > 0:\n",
        "            print(\"✅ Graph sampling working\")\n",
        "            for name, sample_graph in sample_graphs.items():\n",
        "                print(f\"   • {name} sample: {sample_graph.number_of_nodes()} nodes\")\n",
        "            test_results[\"Preprocessing\"] = True\n",
        "        else:\n",
        "            print(\"❌ Preprocessing failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Preprocessing error: {e}\")\n",
        "    \n",
        "    # Test 6: Augmentation\n",
        "    print(\"\\n6️⃣ Testing Augmentation...\")\n",
        "    try:\n",
        "        # Check if negative sampling was performed\n",
        "        if 'negative_edges' in globals() and negative_edges is not None and len(negative_edges) > 0:\n",
        "            print(f\"✅ Negative sampling: {len(negative_edges)} samples\")\n",
        "            test_results[\"Augmentation\"] = True\n",
        "        else:\n",
        "            # Check if the negative sampling function exists and works\n",
        "            if enhanced_graphs is not None and 'transaction' in enhanced_graphs:\n",
        "                test_negative_edges = create_negative_sampling(enhanced_graphs['transaction'], df, negative_ratio=0.1)\n",
        "                if len(test_negative_edges) > 0:\n",
        "                    print(f\"✅ Negative sampling test: {len(test_negative_edges)} samples\")\n",
        "                    test_results[\"Augmentation\"] = True\n",
        "                else:\n",
        "                    print(\"❌ Negative sampling failed\")\n",
        "            else:\n",
        "                print(\"❌ Augmentation test failed - no graphs available\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Augmentation error: {e}\")\n",
        "    \n",
        "    # Test 7: PyTorch Integration\n",
        "    print(\"\\n7️⃣ Testing PyTorch Integration...\")\n",
        "    try:\n",
        "        if enhanced_pytorch_graphs is not None:\n",
        "            for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "                if pyg_data is not None:\n",
        "                    print(f\"   • {name}: {pyg_data.num_nodes} nodes, {pyg_data.num_edges} edges\")\n",
        "                    if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                        print(f\"     Node features: {pyg_data.x.shape}\")\n",
        "                    if hasattr(pyg_data, 'edge_attr') and pyg_data.edge_attr is not None:\n",
        "                        print(f\"     Edge features: {pyg_data.edge_attr.shape}\")\n",
        "            print(\"✅ PyTorch integration working\")\n",
        "            test_results[\"PyTorch Integration\"] = True\n",
        "        else:\n",
        "            print(\"❌ PyTorch integration failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ PyTorch integration error: {e}\")\n",
        "    \n",
        "    # Test 8: Data Persistence\n",
        "    print(\"\\n8️⃣ Testing Data Persistence...\")\n",
        "    try:\n",
        "        import os\n",
        "        enhanced_dir = \"/content/drive/MyDrive/LaunDetection/data/processed/enhanced_graphs\"\n",
        "        splits_dir = \"/content/drive/MyDrive/LaunDetection/data/processed/temporal_splits\"\n",
        "        \n",
        "        if os.path.exists(enhanced_dir) and os.path.exists(splits_dir):\n",
        "            print(\"✅ Data persistence working\")\n",
        "            print(f\"   • Enhanced graphs: {enhanced_dir}\")\n",
        "            print(f\"   • Temporal splits: {splits_dir}\")\n",
        "            test_results[\"Data Persistence\"] = True\n",
        "        else:\n",
        "            print(\"❌ Data persistence failed\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Data persistence error: {e}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"📊 TEST RESULTS SUMMARY:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    passed_tests = sum(test_results.values())\n",
        "    total_tests = len(test_results)\n",
        "    \n",
        "    for test_name, result in test_results.items():\n",
        "        status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
        "        print(f\"{test_name}: {status}\")\n",
        "    \n",
        "    print(f\"\\nOverall: {passed_tests}/{total_tests} tests passed\")\n",
        "    \n",
        "    if passed_tests == total_tests:\n",
        "        print(\"\\n🎉 ALL TESTS PASSED - PHASE 3 IS READY!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"\\n⚠️  {total_tests - passed_tests} TESTS FAILED - NEEDS ATTENTION\")\n",
        "        return False\n",
        "\n",
        "# Run the tests\n",
        "test_success = test_phase3_implementation()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance and Memory Testing\n",
        "if test_success:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PERFORMANCE AND MEMORY TESTING\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    def test_performance():\n",
        "        \"\"\"Test performance and memory usage\"\"\"\n",
        "        print(\"🚀 PERFORMANCE TESTING...\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Memory usage test\n",
        "        import psutil\n",
        "        import gc\n",
        "        \n",
        "        def get_memory_usage():\n",
        "            process = psutil.Process()\n",
        "            return process.memory_info().rss / 1024 / 1024  # MB\n",
        "        \n",
        "        print(f\"Current memory usage: {get_memory_usage():.1f} MB\")\n",
        "        \n",
        "        # Test graph operations performance\n",
        "        if enhanced_graphs is not None:\n",
        "            print(\"\\n📊 Graph Operations Performance:\")\n",
        "            \n",
        "            for name, graph in enhanced_graphs.items():\n",
        "                print(f\"\\n{name.upper()} GRAPH:\")\n",
        "                \n",
        "                # Test basic operations\n",
        "                start_time = time.time()\n",
        "                num_nodes = graph.number_of_nodes()\n",
        "                num_edges = graph.number_of_edges()\n",
        "                basic_time = time.time() - start_time\n",
        "                print(f\"  • Basic operations: {basic_time:.4f}s\")\n",
        "                \n",
        "                # Test feature access\n",
        "                start_time = time.time()\n",
        "                sample_node = list(graph.nodes())[0]\n",
        "                node_features = graph.nodes[sample_node]\n",
        "                feature_time = time.time() - start_time\n",
        "                print(f\"  • Feature access: {feature_time:.4f}s\")\n",
        "                \n",
        "                # Test graph sampling\n",
        "                start_time = time.time()\n",
        "                sample_graph = create_graph_sampling(graph, sample_ratio=0.1, method='random')\n",
        "                sampling_time = time.time() - start_time\n",
        "                print(f\"  • Graph sampling: {sampling_time:.4f}s\")\n",
        "                \n",
        "                # Memory usage\n",
        "                memory_usage = get_memory_usage()\n",
        "                print(f\"  • Memory usage: {memory_usage:.1f} MB\")\n",
        "        \n",
        "        # Test PyTorch operations\n",
        "        if enhanced_pytorch_graphs is not None:\n",
        "            print(\"\\n🔥 PyTorch Operations Performance:\")\n",
        "            \n",
        "            for name, pyg_data in enhanced_pytorch_graphs.items():\n",
        "                if pyg_data is not None:\n",
        "                    print(f\"\\n{name.upper()} PYTORCH GRAPH:\")\n",
        "                    \n",
        "                    # Test tensor operations\n",
        "                    start_time = time.time()\n",
        "                    if hasattr(pyg_data, 'x') and pyg_data.x is not None:\n",
        "                        node_mean = pyg_data.x.mean()\n",
        "                        node_std = pyg_data.x.std()\n",
        "                    tensor_time = time.time() - start_time\n",
        "                    print(f\"  • Tensor operations: {tensor_time:.4f}s\")\n",
        "                    \n",
        "                    # Test GPU availability\n",
        "                    if torch.cuda.is_available():\n",
        "                        print(f\"  • GPU available: {torch.cuda.get_device_name()}\")\n",
        "                        print(f\"  • GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "                    else:\n",
        "                        print(\"  • GPU not available\")\n",
        "        \n",
        "        # Cleanup\n",
        "        gc.collect()\n",
        "        final_memory = get_memory_usage()\n",
        "        print(f\"\\nFinal memory usage: {final_memory:.1f} MB\")\n",
        "        \n",
        "        print(\"\\n✅ Performance testing complete\")\n",
        "    \n",
        "    # Run performance tests\n",
        "    test_performance()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PHASE 3 TESTING COMPLETE!\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if test_success:\n",
        "        print(\"\\n🎉 PHASE 3 IS FULLY TESTED AND READY!\")\n",
        "        print(\"\\n📋 TESTING SUMMARY:\")\n",
        "        print(\"✅ Graph construction validated\")\n",
        "        print(\"✅ Feature engineering validated\") \n",
        "        print(\"✅ Temporal splitting validated\")\n",
        "        print(\"✅ Network features validated\")\n",
        "        print(\"✅ Preprocessing utilities validated\")\n",
        "        print(\"✅ Augmentation techniques validated\")\n",
        "        print(\"✅ PyTorch integration validated\")\n",
        "        print(\"✅ Data persistence validated\")\n",
        "        print(\"✅ Performance testing completed\")\n",
        "        \n",
        "        print(\"\\n🚀 READY FOR PHASE 4:\")\n",
        "        print(\"• Enhanced graphs with comprehensive features\")\n",
        "        print(\"• Temporal splits for proper ML evaluation\")\n",
        "        print(\"• Network features for GNN performance\")\n",
        "        print(\"• Normalized features for model stability\")\n",
        "        print(\"• Graph sampling for computational efficiency\")\n",
        "        print(\"• All data saved and validated\")\n",
        "        \n",
        "        print(\"\\n🎯 NEXT STEP: Run Phase 4 - Multi-GNN Architecture\")\n",
        "        print(\"Command: %run notebooks/03_multi_gnn_architecture.ipynb\")\n",
        "        \n",
        "    else:\n",
        "        print(\"\\n⚠️  PHASE 3 NEEDS ATTENTION\")\n",
        "        print(\"Please review failed tests and fix issues before proceeding to Phase 4\")\n",
        "        \n",
        "else:\n",
        "    print(\"\\n❌ PHASE 3 TESTING FAILED\")\n",
        "    print(\"Please run the complete Phase 3 notebook first\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
