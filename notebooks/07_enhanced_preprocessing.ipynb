{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "\n",
        "This notebook implements comprehensive preprocessing for the IBM AML Synthetic Dataset based on analysis of HI-Small_report.json and sample_subgraph.gpickle.\n",
        "\n",
        "## Key Features:\n",
        "- Enhanced node and edge feature engineering\n",
        "- Advanced class imbalance handling\n",
        "- Temporal feature encoding\n",
        "- Network topology features\n",
        "- Memory-efficient processing\n",
        "- Graph sampling and augmentation\n",
        "\n",
        "## Implementation Phases:\n",
        "1. **Phase 1**: Critical features (node/edge features, class imbalance)\n",
        "2. **Phase 2**: Advanced features (network topology, temporal patterns)\n",
        "3. **Phase 3**: Optimization (feature selection, hyperparameter tuning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "print(\"=\" * 60)\n",
        "print(\"Enhanced AML Preprocessing Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, TargetEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Preprocessing Class\n",
        "class EnhancedAMLPreprocessor:\n",
        "    \"\"\"\n",
        "    Enhanced preprocessing for IBM AML Multi-GNN model\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_path, output_path, chunk_size=10000):\n",
        "        self.data_path = data_path\n",
        "        self.output_path = output_path\n",
        "        self.chunk_size = chunk_size\n",
        "        self.scalers = {}\n",
        "        self.encoders = {}\n",
        "        self.feature_stats = {}\n",
        "        \n",
        "    def load_data(self):\n",
        "        \"\"\"Load and validate data\"\"\"\n",
        "        print(\"Loading IBM AML dataset...\")\n",
        "        \n",
        "        # Load transactions\n",
        "        trans_file = os.path.join(self.data_path, 'HI-Small_Trans.csv')\n",
        "        if os.path.exists(trans_file):\n",
        "            self.transactions = pd.read_csv(trans_file)\n",
        "            print(f\"✓ Loaded {len(self.transactions)} transactions\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Transaction file not found: {trans_file}\")\n",
        "        \n",
        "        # Load accounts\n",
        "        accounts_file = os.path.join(self.data_path, 'HI-Small_accounts.csv')\n",
        "        if os.path.exists(accounts_file):\n",
        "            self.accounts = pd.read_csv(accounts_file)\n",
        "            print(f\"✓ Loaded {len(self.accounts)} accounts\")\n",
        "        else:\n",
        "            # Create accounts from transactions\n",
        "            print(\"Creating accounts from transaction data...\")\n",
        "            self.accounts = self._create_accounts_from_transactions()\n",
        "        \n",
        "        # Validate data\n",
        "        self._validate_data()\n",
        "        \n",
        "    def _create_accounts_from_transactions(self):\n",
        "        \"\"\"Create accounts from transaction data\"\"\"\n",
        "        all_accounts = set(self.transactions['Account'].tolist() + \n",
        "                          self.transactions['Account.1'].tolist())\n",
        "        \n",
        "        accounts_data = {\n",
        "            'Account Number': list(all_accounts),\n",
        "            'Bank Name': [f\"Bank_{i}\" for i in range(len(all_accounts))],\n",
        "            'Bank ID': [f\"B{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity ID': [f\"E{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity Name': [f\"Entity_{i}\" for i in range(len(all_accounts))]\n",
        "        }\n",
        "        \n",
        "        return pd.DataFrame(accounts_data)\n",
        "    \n",
        "    def _validate_data(self):\n",
        "        \"\"\"Validate data quality\"\"\"\n",
        "        print(\"Validating data quality...\")\n",
        "        \n",
        "        # Check for missing values\n",
        "        missing_trans = self.transactions.isnull().sum().sum()\n",
        "        missing_accounts = self.accounts.isnull().sum().sum()\n",
        "        \n",
        "        print(f\"Missing values - Transactions: {missing_trans}, Accounts: {missing_accounts}\")\n",
        "        \n",
        "        # Check class distribution\n",
        "        if 'Is Laundering' in self.transactions.columns:\n",
        "            class_dist = self.transactions['Is Laundering'].value_counts()\n",
        "            print(f\"Class distribution: {class_dist}\")\n",
        "            print(f\"SAR rate: {class_dist[1] / len(self.transactions):.4f}\")\n",
        "        \n",
        "        # Check data types\n",
        "        print(f\"Transaction columns: {list(self.transactions.columns)}\")\n",
        "        print(f\"Account columns: {list(self.accounts.columns)}\")\n",
        "\n",
        "print(\"✓ Enhanced preprocessing class defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Node Features\n",
        "def create_enhanced_node_features(self):\n",
        "    \"\"\"Create comprehensive node features\"\"\"\n",
        "    print(\"Creating enhanced node features...\")\n",
        "    \n",
        "    node_features = {}\n",
        "    \n",
        "    for _, account in self.accounts.iterrows():\n",
        "        account_id = account['Account Number']\n",
        "        \n",
        "        # Get account transactions\n",
        "        account_trans = self.transactions[\n",
        "            (self.transactions['Account'] == account_id) | \n",
        "            (self.transactions['Account.1'] == account_id)\n",
        "        ]\n",
        "        \n",
        "        if len(account_trans) == 0:\n",
        "            # Default features for accounts with no transactions\n",
        "            node_features[account_id] = self._get_default_node_features()\n",
        "            continue\n",
        "        \n",
        "        # Basic transaction features\n",
        "        sent_trans = account_trans[account_trans['Account'] == account_id]\n",
        "        received_trans = account_trans[account_trans['Account.1'] == account_id]\n",
        "        \n",
        "        # Amount features\n",
        "        total_sent = sent_trans['Amount Paid'].sum() if len(sent_trans) > 0 else 0\n",
        "        total_received = received_trans['Amount Received'].sum() if len(received_trans) > 0 else 0\n",
        "        avg_amount = account_trans['Amount Paid'].mean()\n",
        "        max_amount = account_trans['Amount Paid'].max()\n",
        "        min_amount = account_trans['Amount Paid'].min()\n",
        "        \n",
        "        # Temporal features\n",
        "        timestamps = pd.to_datetime(account_trans['Timestamp'])\n",
        "        temporal_span = (timestamps.max() - timestamps.min()).days\n",
        "        transaction_frequency = len(account_trans) / max(1, temporal_span)\n",
        "        \n",
        "        # Currency and bank diversity\n",
        "        currency_diversity = account_trans['Payment Currency'].nunique()\n",
        "        bank_diversity = account_trans['To Bank'].nunique()\n",
        "        \n",
        "        # Time-based features\n",
        "        night_transactions = timestamps.dt.hour.isin([22, 23, 0, 1, 2, 3, 4, 5, 6]).sum()\n",
        "        weekend_transactions = timestamps.dt.weekday.isin([5, 6]).sum()\n",
        "        night_ratio = night_transactions / len(account_trans)\n",
        "        weekend_ratio = weekend_transactions / len(account_trans)\n",
        "        \n",
        "        # Risk indicators\n",
        "        is_crypto_bank = 'Crytpo' in str(account_id)\n",
        "        is_international = currency_diversity > 1\n",
        "        is_high_frequency = transaction_frequency > 1.0\n",
        "        \n",
        "        # Network features (will be updated later)\n",
        "        in_degree = 0\n",
        "        out_degree = 0\n",
        "        betweenness_centrality = 0\n",
        "        pagerank = 0\n",
        "        \n",
        "        node_features[account_id] = {\n",
        "            'transaction_count': len(account_trans),\n",
        "            'total_sent': total_sent,\n",
        "            'total_received': total_received,\n",
        "            'avg_amount': avg_amount,\n",
        "            'max_amount': max_amount,\n",
        "            'min_amount': min_amount,\n",
        "            'temporal_span': temporal_span,\n",
        "            'transaction_frequency': transaction_frequency,\n",
        "            'currency_diversity': currency_diversity,\n",
        "            'bank_diversity': bank_diversity,\n",
        "            'night_ratio': night_ratio,\n",
        "            'weekend_ratio': weekend_ratio,\n",
        "            'is_crypto_bank': int(is_crypto_bank),\n",
        "            'is_international': int(is_international),\n",
        "            'is_high_frequency': int(is_high_frequency),\n",
        "            'in_degree': in_degree,\n",
        "            'out_degree': out_degree,\n",
        "            'betweenness_centrality': betweenness_centrality,\n",
        "            'pagerank': pagerank\n",
        "        }\n",
        "    \n",
        "    return node_features\n",
        "\n",
        "def _get_default_node_features(self):\n",
        "    \"\"\"Get default features for accounts with no transactions\"\"\"\n",
        "    return {\n",
        "        'transaction_count': 0,\n",
        "        'total_sent': 0,\n",
        "        'total_received': 0,\n",
        "        'avg_amount': 0,\n",
        "        'max_amount': 0,\n",
        "        'min_amount': 0,\n",
        "        'temporal_span': 0,\n",
        "        'transaction_frequency': 0,\n",
        "        'currency_diversity': 0,\n",
        "        'bank_diversity': 0,\n",
        "        'night_ratio': 0,\n",
        "        'weekend_ratio': 0,\n",
        "        'is_crypto_bank': 0,\n",
        "        'is_international': 0,\n",
        "        'is_high_frequency': 0,\n",
        "        'in_degree': 0,\n",
        "        'out_degree': 0,\n",
        "        'betweenness_centrality': 0,\n",
        "        'pagerank': 0\n",
        "    }\n",
        "\n",
        "print(\"✓ Enhanced node feature functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Edge Features\n",
        "def create_enhanced_edge_features(self):\n",
        "    \"\"\"Create comprehensive edge features\"\"\"\n",
        "    print(\"Creating enhanced edge features...\")\n",
        "    \n",
        "    edge_features = []\n",
        "    edge_labels = []\n",
        "    \n",
        "    # Prepare encoders\n",
        "    self._prepare_encoders()\n",
        "    \n",
        "    for _, transaction in self.transactions.iterrows():\n",
        "        # Temporal features\n",
        "        timestamp = pd.to_datetime(transaction['Timestamp'])\n",
        "        temporal_features = self._create_temporal_features(timestamp)\n",
        "        \n",
        "        # Amount features\n",
        "        amount_features = self._create_amount_features(transaction)\n",
        "        \n",
        "        # Categorical features\n",
        "        categorical_features = self._create_categorical_features(transaction)\n",
        "        \n",
        "        # Combine all features\n",
        "        edge_feature = temporal_features + amount_features + categorical_features\n",
        "        edge_features.append(edge_feature)\n",
        "        \n",
        "        # Label\n",
        "        edge_labels.append(transaction['Is Laundering'])\n",
        "    \n",
        "    return np.array(edge_features), np.array(edge_labels)\n",
        "\n",
        "def _prepare_encoders(self):\n",
        "    \"\"\"Prepare encoders for categorical features\"\"\"\n",
        "    # Currency encoder\n",
        "    currencies = self.transactions['Payment Currency'].unique()\n",
        "    self.encoders['currency'] = LabelEncoder()\n",
        "    self.encoders['currency'].fit(currencies)\n",
        "    \n",
        "    # Format encoder\n",
        "    formats = self.transactions['Payment Format'].unique()\n",
        "    self.encoders['format'] = LabelEncoder()\n",
        "    self.encoders['format'].fit(formats)\n",
        "    \n",
        "    # Bank encoder\n",
        "    banks = self.transactions['From Bank'].unique()\n",
        "    self.encoders['bank'] = LabelEncoder()\n",
        "    self.encoders['bank'].fit(banks)\n",
        "\n",
        "def _create_temporal_features(self, timestamp):\n",
        "    \"\"\"Create cyclic temporal features\"\"\"\n",
        "    # Hour features\n",
        "    hour_sin = np.sin(2 * np.pi * timestamp.hour / 24)\n",
        "    hour_cos = np.cos(2 * np.pi * timestamp.hour / 24)\n",
        "    \n",
        "    # Day of week features\n",
        "    day_sin = np.sin(2 * np.pi * timestamp.dayofweek / 7)\n",
        "    day_cos = np.cos(2 * np.pi * timestamp.dayofweek / 7)\n",
        "    \n",
        "    # Day of month features\n",
        "    day_month_sin = np.sin(2 * np.pi * timestamp.day / 31)\n",
        "    day_month_cos = np.cos(2 * np.pi * timestamp.day / 31)\n",
        "    \n",
        "    # Month features\n",
        "    month_sin = np.sin(2 * np.pi * timestamp.month / 12)\n",
        "    month_cos = np.cos(2 * np.pi * timestamp.month / 12)\n",
        "    \n",
        "    # Year features (normalized)\n",
        "    year_normalized = (timestamp.year - 2020) / 5\n",
        "    \n",
        "    return [hour_sin, hour_cos, day_sin, day_cos, \n",
        "            day_month_sin, day_month_cos, month_sin, month_cos, year_normalized]\n",
        "\n",
        "def _create_amount_features(self, transaction):\n",
        "    \"\"\"Create amount-based features\"\"\"\n",
        "    amount_paid = transaction['Amount Paid']\n",
        "    amount_received = transaction['Amount Received']\n",
        "    \n",
        "    # Log transformation\n",
        "    amount_paid_log = np.log1p(amount_paid)\n",
        "    amount_received_log = np.log1p(amount_received)\n",
        "    \n",
        "    # Normalized amounts (will be updated with scaler)\n",
        "    amount_paid_norm = amount_paid  # Will be normalized later\n",
        "    amount_received_norm = amount_received  # Will be normalized later\n",
        "    \n",
        "    # Amount ratios\n",
        "    amount_ratio = amount_paid / max(amount_received, 1)\n",
        "    \n",
        "    return [amount_paid_log, amount_received_log, \n",
        "            amount_paid_norm, amount_received_norm, amount_ratio]\n",
        "\n",
        "def _create_categorical_features(self, transaction):\n",
        "    \"\"\"Create categorical features\"\"\"\n",
        "    # Currency encoding\n",
        "    currency_encoded = self.encoders['currency'].transform([transaction['Payment Currency']])[0]\n",
        "    \n",
        "    # Format encoding\n",
        "    format_encoded = self.encoders['format'].transform([transaction['Payment Format']])[0]\n",
        "    \n",
        "    # Bank encoding\n",
        "    bank_encoded = self.encoders['bank'].transform([transaction['From Bank']])[0]\n",
        "    \n",
        "    return [currency_encoded, format_encoded, bank_encoded]\n",
        "\n",
        "print(\"✓ Enhanced edge feature functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Imbalance Handling\n",
        "def handle_class_imbalance(self, X, y, strategy='smote'):\n",
        "    \"\"\"Handle class imbalance using multiple strategies\"\"\"\n",
        "    print(f\"Handling class imbalance using {strategy}...\")\n",
        "    \n",
        "    if strategy == 'smote':\n",
        "        smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "        \n",
        "    elif strategy == 'undersample':\n",
        "        undersampler = RandomUnderSampler(random_state=42)\n",
        "        X_resampled, y_resampled = undersampler.fit_resample(X, y)\n",
        "        \n",
        "    elif strategy == 'combined':\n",
        "        # First oversample minority class\n",
        "        smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "        X_oversampled, y_oversampled = smote.fit_resample(X, y)\n",
        "        \n",
        "        # Then undersample majority class\n",
        "        undersampler = RandomUnderSampler(random_state=42)\n",
        "        X_resampled, y_resampled = undersampler.fit_resample(X_oversampled, y_oversampled)\n",
        "    \n",
        "    print(f\"Original distribution: {np.bincount(y)}\")\n",
        "    print(f\"Resampled distribution: {np.bincount(y_resampled)}\")\n",
        "    \n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def create_cost_sensitive_weights(self, y):\n",
        "    \"\"\"Create cost-sensitive class weights\"\"\"\n",
        "    print(\"Creating cost-sensitive class weights...\")\n",
        "    \n",
        "    # Compute balanced class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    \n",
        "    # Additional cost for false negatives (missed illicit transactions)\n",
        "    cost_multiplier = 10.0  # Emphasize false negatives\n",
        "    adjusted_weights = class_weights * cost_multiplier\n",
        "    \n",
        "    weight_dict = dict(zip(classes, adjusted_weights))\n",
        "    print(f\"Class weights: {weight_dict}\")\n",
        "    \n",
        "    return weight_dict\n",
        "\n",
        "print(\"✓ Class imbalance handling functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and Run Preprocessing\n",
        "print(\"Initializing enhanced preprocessing...\")\n",
        "\n",
        "# Configuration\n",
        "data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "output_path = \"/content/drive/MyDrive/LaunDetection/data/processed/enhanced\"\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = EnhancedAMLPreprocessor(data_path, output_path)\n",
        "\n",
        "print(\"✓ Preprocessor initialized\")\n",
        "print(f\"Data path: {data_path}\")\n",
        "print(f\"Output path: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Enhanced Preprocessing\n",
        "print(\"Starting enhanced preprocessing...\")\n",
        "\n",
        "try:\n",
        "    # Load data\n",
        "    preprocessor.load_data()\n",
        "    \n",
        "    # Create enhanced features\n",
        "    node_features = preprocessor.create_enhanced_node_features()\n",
        "    edge_features, edge_labels = preprocessor.create_enhanced_edge_features()\n",
        "    \n",
        "    print(f\"✓ Created {len(node_features)} node features\")\n",
        "    print(f\"✓ Created {len(edge_features)} edge features\")\n",
        "    print(f\"✓ Edge feature dimensions: {edge_features.shape[1]}\")\n",
        "    \n",
        "    # Show sample features\n",
        "    print(f\"\\nSample node features for first account:\")\n",
        "    first_account = list(node_features.keys())[0]\n",
        "    print(f\"Account: {first_account}\")\n",
        "    for key, value in list(node_features[first_account].items())[:5]:\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    print(f\"\\nSample edge features:\")\n",
        "    print(f\"  Temporal features: {edge_features[0][:9]}\")\n",
        "    print(f\"  Amount features: {edge_features[0][9:14]}\")\n",
        "    print(f\"  Categorical features: {edge_features[0][14:17]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during preprocessing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
