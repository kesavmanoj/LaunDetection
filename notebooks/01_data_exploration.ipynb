{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AML Multi-GNN - Phase 2: Data Exploration\n",
        "\n",
        "This notebook implements Phase 2 of the AML Multi-GNN project, focusing on data exploration and analysis of the existing IBM AML dataset in Google Drive.\n",
        "\n",
        "## Objectives:\n",
        "1. Load existing data from Google Drive\n",
        "2. Explore data structure and quality\n",
        "3. Analyze transaction patterns\n",
        "4. Create visualizations\n",
        "5. Generate data quality report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import project utilities\n",
        "from utils.gpu_utils import get_device, print_system_info\n",
        "from utils.logging_utils import setup_logging\n",
        "from utils.random_utils import set_random_seed\n",
        "from utils.data_loader_colab import load_existing_data, identify_data_structure, create_unified_dataframe, analyze_data_quality, print_data_summary\n",
        "\n",
        "# Setup logging\n",
        "logger = setup_logging(experiment_name=\"phase_2_data_exploration\")\n",
        "\n",
        "# Print system info\n",
        "print_system_info()\n",
        "\n",
        "# Memory monitoring function\n",
        "def check_memory():\n",
        "    import psutil\n",
        "    memory = psutil.virtual_memory()\n",
        "    print(f\"Memory: {memory.used / 1e9:.1f}GB used / {memory.total / 1e9:.1f}GB total ({memory.percent:.1f}%)\")\n",
        "    return memory.percent\n",
        "\n",
        "# Check initial memory\n",
        "print(\"Initial memory usage:\")\n",
        "check_memory()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing data from Google Drive (ultra-conservative approach)\n",
        "print(\"Loading existing IBM AML dataset from Google Drive...\")\n",
        "print(\"Using ultra-conservative loading to prevent crashes...\")\n",
        "\n",
        "try:\n",
        "    # Ultra-conservative approach: load only tiny samples\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    \n",
        "    data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "    data = {}\n",
        "    \n",
        "    # Load only HI-Small files with extreme memory management\n",
        "    hi_small_files = [f for f in os.listdir(data_path) if 'HI-Small' in f and f.endswith('.csv')]\n",
        "    print(f\"Found {len(hi_small_files)} HI-Small CSV files\")\n",
        "    \n",
        "    for file in hi_small_files:\n",
        "        file_path = os.path.join(data_path, file)\n",
        "        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
        "        print(f\"Loading {file} ({file_size:.1f} MB)...\")\n",
        "        \n",
        "        # Always use tiny chunks for memory safety\n",
        "        print(f\"  Reading in tiny chunks...\")\n",
        "        chunk_size = 500  # Ultra-small chunks\n",
        "        chunks = []\n",
        "        \n",
        "        # Read only first chunk to avoid memory issues\n",
        "        for chunk_num, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):\n",
        "            chunks.append(chunk)\n",
        "            print(f\"    Chunk {chunk_num + 1}: {chunk.shape}\")\n",
        "            \n",
        "            # Limit to first chunk only to save memory\n",
        "            if chunk_num >= 0:\n",
        "                print(f\"    Limiting to first chunk only to save memory...\")\n",
        "                break\n",
        "        \n",
        "        # Use the single chunk\n",
        "        df = chunks[0] if chunks else pd.DataFrame()\n",
        "        print(f\"  ✓ Loaded sample of {file}: {df.shape} (first chunk only)\")\n",
        "        \n",
        "        data[file.replace('.csv', '')] = df\n",
        "        \n",
        "        # Clear memory after each file\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        \n",
        "        # Check memory usage\n",
        "        memory_usage = check_memory()\n",
        "        if memory_usage > 80:  # If memory usage > 80%\n",
        "            print(f\"⚠️  High memory usage ({memory_usage:.1f}%), stopping to prevent crash\")\n",
        "            break\n",
        "    \n",
        "    print(f\"✓ Successfully loaded {len(data)} data files (ultra-conservative)\")\n",
        "    \n",
        "    # Identify data structure\n",
        "    print(\"Identifying data structure...\")\n",
        "    structure = identify_data_structure(data)\n",
        "    print(f\"✓ Identified data structure: {structure}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading data: {e}\")\n",
        "    print(\"Trying minimal sample loading...\")\n",
        "    \n",
        "    try:\n",
        "        # Minimal approach: load just a few rows to get structure\n",
        "        import os\n",
        "        import pandas as pd\n",
        "        \n",
        "        data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "        data = {}\n",
        "        \n",
        "        # Load only the smallest file first\n",
        "        hi_small_files = [f for f in os.listdir(data_path) if 'HI-Small' in f and f.endswith('.csv')]\n",
        "        \n",
        "        # Find the smallest file\n",
        "        file_sizes = []\n",
        "        for file in hi_small_files:\n",
        "            file_path = os.path.join(data_path, file)\n",
        "            file_size = os.path.getsize(file_path)\n",
        "            file_sizes.append((file, file_size))\n",
        "        \n",
        "        # Sort by size and take the smallest\n",
        "        file_sizes.sort(key=lambda x: x[1])\n",
        "        smallest_file = file_sizes[0][0]\n",
        "        \n",
        "        print(f\"Loading smallest file: {smallest_file}\")\n",
        "        \n",
        "        # Load just the first 100 rows\n",
        "        file_path = os.path.join(data_path, smallest_file)\n",
        "        df = pd.read_csv(file_path, nrows=100)\n",
        "        data[smallest_file.replace('.csv', '')] = df\n",
        "        print(f\"✓ Loaded sample of {smallest_file}: {df.shape} (first 100 rows)\")\n",
        "        \n",
        "        # Create minimal structure\n",
        "        structure = {\n",
        "            'transactions': smallest_file.replace('.csv', ''),\n",
        "            'accounts': None,\n",
        "            'labels': None,\n",
        "            'other_files': []\n",
        "        }\n",
        "        \n",
        "        print(f\"✓ Created minimal data structure: {structure}\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"✗ Minimal loading also failed: {e2}\")\n",
        "        data = None\n",
        "        structure = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fix data structure identification\n",
        "print(\"Fixing data structure identification...\")\n",
        "\n",
        "if data and len(data) > 0:\n",
        "    # Manual structure identification\n",
        "    manual_structure = {\n",
        "        'transactions': None,\n",
        "        'accounts': None,\n",
        "        'labels': None,\n",
        "        'other_files': []\n",
        "    }\n",
        "    \n",
        "    for key, df in data.items():\n",
        "        if df is not None and not df.empty:\n",
        "            columns = list(df.columns)\n",
        "            print(f\"Analyzing {key} with columns: {columns}\")\n",
        "            \n",
        "            # Check for transaction indicators\n",
        "            if any(col in ['Timestamp', 'From Bank', 'To Bank', 'Amount Received', 'Amount Paid'] for col in columns):\n",
        "                manual_structure['transactions'] = key\n",
        "                print(f\"  → Identified as transactions: {key}\")\n",
        "            \n",
        "            # Check for account indicators  \n",
        "            elif any(col in ['Bank Name', 'Account Number', 'Entity ID', 'Entity Name'] for col in columns):\n",
        "                manual_structure['accounts'] = key\n",
        "                print(f\"  → Identified as accounts: {key}\")\n",
        "            \n",
        "            # Check for label indicators\n",
        "            elif any(col in ['Is Laundering', 'Label', 'Class', 'Target'] for col in columns):\n",
        "                manual_structure['labels'] = key\n",
        "                print(f\"  → Identified as labels: {key}\")\n",
        "            \n",
        "            else:\n",
        "                manual_structure['other_files'].append(key)\n",
        "                print(f\"  → Added to other files: {key}\")\n",
        "    \n",
        "    structure = manual_structure\n",
        "    print(f\"✓ Fixed data structure: {structure}\")\n",
        "else:\n",
        "    print(\"No data available to analyze structure\")\n",
        "    structure = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified dataframe (with progress tracking)\n",
        "if data is not None and len(data) > 0:\n",
        "    print(\"Creating unified dataframe...\")\n",
        "    print(\"This may take time for large datasets...\")\n",
        "    \n",
        "    try:\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        \n",
        "        if structure and (structure.get('transactions') or structure.get('accounts')):\n",
        "            unified_df = create_unified_dataframe(data, structure)\n",
        "        else:\n",
        "            print(\"⚠️  No valid data structure found, creating minimal unified dataframe...\")\n",
        "            \n",
        "            # Create a simple unified dataframe from available data\n",
        "            unified_dfs = []\n",
        "            for key, df in data.items():\n",
        "                if df is not None and not df.empty:\n",
        "                    # Add a source column to identify the original file\n",
        "                    df_copy = df.copy()\n",
        "                    df_copy['source_file'] = key\n",
        "                    unified_dfs.append(df_copy)\n",
        "            \n",
        "            if unified_dfs:\n",
        "                unified_df = pd.concat(unified_dfs, ignore_index=True)\n",
        "                print(f\"✓ Created minimal unified dataframe: {unified_df.shape}\")\n",
        "            else:\n",
        "                print(\"✗ No data available for unified dataframe\")\n",
        "                unified_df = None\n",
        "        \n",
        "        create_time = time.time() - start_time\n",
        "        \n",
        "        if unified_df is not None:\n",
        "            print(f\"✓ Created unified dataframe: {unified_df.shape} in {create_time:.1f} seconds\")\n",
        "            \n",
        "            # Analyze data quality (this can be slow)\n",
        "            print(\"Analyzing data quality...\")\n",
        "            quality_metrics = analyze_data_quality(unified_df)\n",
        "            \n",
        "            # Print comprehensive summary\n",
        "            print_data_summary(unified_df, quality_metrics)\n",
        "        else:\n",
        "            print(\"⚠️  Cannot create unified dataframe - no valid data\")\n",
        "            quality_metrics = None\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error creating unified dataframe: {e}\")\n",
        "        print(\"This might be due to memory constraints or data size\")\n",
        "        unified_df = None\n",
        "        quality_metrics = None\n",
        "else:\n",
        "    print(\"⚠️  Cannot create unified dataframe - data loading failed\")\n",
        "    unified_df = None\n",
        "    quality_metrics = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data exploration and visualization\n",
        "if unified_df is not None:\n",
        "    print(\"Performing data exploration...\")\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    \n",
        "    # Set up plotting\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(unified_df.describe())\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(\"\\nMissing Values:\")\n",
        "    missing_data = unified_df.isnull().sum()\n",
        "    missing_percent = (missing_data / len(unified_df)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing_data,\n",
        "        'Missing Percentage': missing_percent\n",
        "    })\n",
        "    print(missing_df[missing_df['Missing Count'] > 0])\n",
        "    \n",
        "    # Data types\n",
        "    print(\"\\nData Types:\")\n",
        "    print(unified_df.dtypes)\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️  Cannot perform exploration - unified dataframe not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data for next phase\n",
        "if unified_df is not None:\n",
        "    print(\"Saving processed data for next phase...\")\n",
        "    \n",
        "    try:\n",
        "        # Save to Google Drive\n",
        "        output_path = \"/content/drive/MyDrive/LaunDetection/data/processed/unified_data.csv\"\n",
        "        unified_df.to_csv(output_path, index=False)\n",
        "        print(f\"✓ Saved unified data to: {output_path}\")\n",
        "        \n",
        "        # Save data structure info\n",
        "        import json\n",
        "        structure_path = \"/content/drive/MyDrive/LaunDetection/data/processed/data_structure.json\"\n",
        "        with open(structure_path, 'w') as f:\n",
        "            json.dump(structure, f, indent=2)\n",
        "        print(f\"✓ Saved data structure to: {structure_path}\")\n",
        "        \n",
        "        # Save quality metrics\n",
        "        quality_path = \"/content/drive/MyDrive/LaunDetection/data/processed/quality_metrics.json\"\n",
        "        with open(quality_path, 'w') as f:\n",
        "            json.dump(quality_metrics, f, indent=2, default=str)\n",
        "        print(f\"✓ Saved quality metrics to: {quality_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error saving processed data: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Phase 2 - Data Exploration Completed!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Review the data exploration results above\")\n",
        "    print(\"2. Proceed to Phase 3: Graph Construction and Preprocessing\")\n",
        "    print(\"3. Run: %run notebooks/02_graph_construction.ipynb\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️  Cannot save processed data - unified dataframe not available\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
