{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AML Multi-GNN - Phase 2: Data Exploration\n",
        "\n",
        "This notebook implements Phase 2 of the AML Multi-GNN project, focusing on data exploration and analysis of the existing IBM AML dataset in Google Drive.\n",
        "\n",
        "## Objectives:\n",
        "1. Load existing data from Google Drive\n",
        "2. Explore data structure and quality\n",
        "3. Analyze transaction patterns\n",
        "4. Create visualizations\n",
        "5. Generate data quality report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup environment\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path.cwd()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "# Import project utilities\n",
        "from utils.gpu_utils import get_device, print_system_info\n",
        "from utils.logging_utils import setup_logging\n",
        "from utils.random_utils import set_random_seed\n",
        "from utils.data_loader_colab import load_existing_data, identify_data_structure, create_unified_dataframe, analyze_data_quality, print_data_summary\n",
        "\n",
        "# Setup logging\n",
        "logger = setup_logging(experiment_name=\"phase_2_data_exploration\")\n",
        "\n",
        "# Print system info\n",
        "print_system_info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing data from Google Drive (with progress tracking)\n",
        "print(\"Loading existing IBM AML dataset from Google Drive...\")\n",
        "print(\"This may take a few minutes for large datasets...\")\n",
        "\n",
        "try:\n",
        "    # Load data with progress indication\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    data = load_existing_data()\n",
        "    load_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"✓ Successfully loaded {len(data)} data files in {load_time:.1f} seconds\")\n",
        "    \n",
        "    # Identify data structure\n",
        "    print(\"Identifying data structure...\")\n",
        "    structure = identify_data_structure(data)\n",
        "    print(f\"✓ Identified data structure: {structure}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error loading data: {e}\")\n",
        "    print(\"This might be due to large dataset size or memory issues\")\n",
        "    data = None\n",
        "    structure = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create unified dataframe (with progress tracking)\n",
        "if data is not None and structure is not None:\n",
        "    print(\"Creating unified dataframe...\")\n",
        "    print(\"This may take time for large datasets...\")\n",
        "    \n",
        "    try:\n",
        "        import time\n",
        "        start_time = time.time()\n",
        "        \n",
        "        unified_df = create_unified_dataframe(data, structure)\n",
        "        create_time = time.time() - start_time\n",
        "        \n",
        "        print(f\"✓ Created unified dataframe: {unified_df.shape} in {create_time:.1f} seconds\")\n",
        "        \n",
        "        # Analyze data quality (this can be slow)\n",
        "        print(\"Analyzing data quality...\")\n",
        "        quality_metrics = analyze_data_quality(unified_df)\n",
        "        \n",
        "        # Print comprehensive summary\n",
        "        print_data_summary(unified_df, quality_metrics)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error creating unified dataframe: {e}\")\n",
        "        print(\"This might be due to memory constraints or data size\")\n",
        "        unified_df = None\n",
        "        quality_metrics = None\n",
        "else:\n",
        "    print(\"⚠️  Cannot create unified dataframe - data loading failed\")\n",
        "    unified_df = None\n",
        "    quality_metrics = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data exploration and visualization\n",
        "if unified_df is not None:\n",
        "    print(\"Performing data exploration...\")\n",
        "    \n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    \n",
        "    # Set up plotting\n",
        "    plt.style.use('default')\n",
        "    sns.set_palette(\"husl\")\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(unified_df.describe())\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(\"\\nMissing Values:\")\n",
        "    missing_data = unified_df.isnull().sum()\n",
        "    missing_percent = (missing_data / len(unified_df)) * 100\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Missing Count': missing_data,\n",
        "        'Missing Percentage': missing_percent\n",
        "    })\n",
        "    print(missing_df[missing_df['Missing Count'] > 0])\n",
        "    \n",
        "    # Data types\n",
        "    print(\"\\nData Types:\")\n",
        "    print(unified_df.dtypes)\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️  Cannot perform exploration - unified dataframe not available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data for next phase\n",
        "if unified_df is not None:\n",
        "    print(\"Saving processed data for next phase...\")\n",
        "    \n",
        "    try:\n",
        "        # Save to Google Drive\n",
        "        output_path = \"/content/drive/MyDrive/LaunDetection/data/processed/unified_data.csv\"\n",
        "        unified_df.to_csv(output_path, index=False)\n",
        "        print(f\"✓ Saved unified data to: {output_path}\")\n",
        "        \n",
        "        # Save data structure info\n",
        "        import json\n",
        "        structure_path = \"/content/drive/MyDrive/LaunDetection/data/processed/data_structure.json\"\n",
        "        with open(structure_path, 'w') as f:\n",
        "            json.dump(structure, f, indent=2)\n",
        "        print(f\"✓ Saved data structure to: {structure_path}\")\n",
        "        \n",
        "        # Save quality metrics\n",
        "        quality_path = \"/content/drive/MyDrive/LaunDetection/data/processed/quality_metrics.json\"\n",
        "        with open(quality_path, 'w') as f:\n",
        "            json.dump(quality_metrics, f, indent=2, default=str)\n",
        "        print(f\"✓ Saved quality metrics to: {quality_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error saving processed data: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Phase 2 - Data Exploration Completed!\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"1. Review the data exploration results above\")\n",
        "    print(\"2. Proceed to Phase 3: Graph Construction and Preprocessing\")\n",
        "    print(\"3. Run: %run notebooks/02_graph_construction.ipynb\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠️  Cannot save processed data - unified dataframe not available\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
