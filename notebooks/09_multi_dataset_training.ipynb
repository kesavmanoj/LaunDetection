{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Dataset AML Training Pipeline\n",
        "\n",
        "This notebook implements the complete multi-dataset training pipeline with memory management for large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install torch torch-geometric networkx pandas numpy scikit-learn imbalanced-learn tqdm matplotlib seaborn plotly --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import pickle\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ Multi-Dataset AML Training Pipeline\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add current directory to path for imports\n",
        "sys.path.append('/content/drive/MyDrive/LaunDetection')\n",
        "\n",
        "# Import our custom modules\n",
        "from multi_dataset_preprocessing import MultiDatasetPreprocessor\n",
        "from multi_dataset_training import MultiDatasetTrainer\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Multi-Dataset Preprocessing with Memory Management\n",
        "print(\"üîÑ STEP 1: MULTI-DATASET PREPROCESSING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "preprocessor = MultiDatasetPreprocessor()\n",
        "processed_data = preprocessor.run_full_preprocessing()\n",
        "\n",
        "if processed_data:\n",
        "    print(\"‚úÖ Multi-dataset preprocessing completed successfully!\")\n",
        "else:\n",
        "    print(\"‚ùå Multi-dataset preprocessing failed!\")\n",
        "    raise Exception(\"Preprocessing failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Multi-Dataset Training\n",
        "print(\"üöÄ STEP 2: MULTI-DATASET TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer = MultiDatasetTrainer()\n",
        "\n",
        "# Load processed datasets\n",
        "datasets = trainer.load_processed_datasets()\n",
        "\n",
        "if not datasets:\n",
        "    print(\"‚ùå No processed datasets found!\")\n",
        "    raise Exception(\"No processed datasets found\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(datasets)} processed datasets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create combined dataset\n",
        "print(\"üîÑ Creating combined multi-dataset...\")\n",
        "combined_data = trainer.create_combined_dataset(datasets)\n",
        "\n",
        "# Convert to PyTorch format\n",
        "print(\"üîÑ Converting to PyTorch Geometric format...\")\n",
        "data = trainer.create_pytorch_data(combined_data)\n",
        "\n",
        "print(f\"‚úÖ Combined dataset ready: {data.num_nodes:,} nodes, {data.num_edges:,} edges\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the multi-dataset model\n",
        "print(\"üöÄ Starting Multi-Dataset Training...\")\n",
        "model, best_f1 = trainer.train_multi_dataset_model(data, epochs=100, learning_rate=0.001)\n",
        "\n",
        "print(f\"‚úÖ Training completed! Best F1: {best_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the trained model\n",
        "print(\"üìä Evaluating Multi-Dataset Model...\")\n",
        "metrics, aml_metrics, cm = trainer.evaluate_multi_dataset_model(data)\n",
        "\n",
        "print(\"\\nüéâ MULTI-DATASET TRAINING COMPLETE!\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚úÖ Overall F1: {metrics['f1_weighted']:.4f}\")\n",
        "print(f\"‚úÖ AML F1: {aml_metrics['aml_f1']:.4f}\")\n",
        "print(f\"‚úÖ ROC-AUC: {metrics['roc_auc']:.4f}\")\n",
        "\n",
        "if aml_metrics['aml_f1'] > 0.5:\n",
        "    print(\"üéâ EXCELLENT! AML detection significantly improved!\")\n",
        "elif aml_metrics['aml_f1'] > 0.3:\n",
        "    print(\"‚úÖ GOOD! AML detection improved!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è AML detection needs further improvement\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
