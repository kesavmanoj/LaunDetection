{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "\n",
        "This notebook implements a simplified version of the enhanced preprocessing pipeline for the IBM AML Synthetic Dataset.\n",
        "\n",
        "## Focus Areas:\n",
        "1. **Enhanced Node Features**: 20+ comprehensive account features\n",
        "2. **Enhanced Edge Features**: 15+ transaction features with cyclic temporal encoding\n",
        "3. **Class Imbalance Handling**: SMOTE + cost-sensitive learning\n",
        "4. **Memory Optimization**: Chunked processing for large datasets\n",
        "\n",
        "## Implementation Strategy:\n",
        "- Start with core features (Phase 1)\n",
        "- Test with sample data first\n",
        "- Gradually scale to full dataset\n",
        "- Monitor memory usage and performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "print(\"=\" * 60)\n",
        "print(\"Simple Enhanced AML Preprocessing Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Validate Data\n",
        "def load_aml_data(data_path, sample_size=10000):\n",
        "    \"\"\"Load AML data with optional sampling for testing\"\"\"\n",
        "    print(\"Loading IBM AML dataset...\")\n",
        "    \n",
        "    # Load transactions\n",
        "    trans_file = os.path.join(data_path, 'HI-Small_Trans.csv')\n",
        "    if os.path.exists(trans_file):\n",
        "        # Load with sampling for testing\n",
        "        transactions = pd.read_csv(trans_file, nrows=sample_size)\n",
        "        print(f\"✓ Loaded {len(transactions)} transactions (sampled)\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Transaction file not found: {trans_file}\")\n",
        "    \n",
        "    # Load accounts\n",
        "    accounts_file = os.path.join(data_path, 'HI-Small_accounts.csv')\n",
        "    if os.path.exists(accounts_file):\n",
        "        accounts = pd.read_csv(accounts_file)\n",
        "        print(f\"✓ Loaded {len(accounts)} accounts\")\n",
        "    else:\n",
        "        # Create accounts from transactions\n",
        "        print(\"Creating accounts from transaction data...\")\n",
        "        all_accounts = set(transactions['Account'].tolist() + \n",
        "                          transactions['Account.1'].tolist())\n",
        "        \n",
        "        accounts_data = {\n",
        "            'Account Number': list(all_accounts),\n",
        "            'Bank Name': [f\"Bank_{i}\" for i in range(len(all_accounts))],\n",
        "            'Bank ID': [f\"B{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity ID': [f\"E{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity Name': [f\"Entity_{i}\" for i in range(len(all_accounts))]\n",
        "        }\n",
        "        accounts = pd.DataFrame(accounts_data)\n",
        "        print(f\"✓ Created {len(accounts)} accounts from transactions\")\n",
        "    \n",
        "    # Validate data\n",
        "    print(\"\\nData Validation:\")\n",
        "    print(f\"Missing values - Transactions: {transactions.isnull().sum().sum()}\")\n",
        "    print(f\"Missing values - Accounts: {accounts.isnull().sum().sum()}\")\n",
        "    \n",
        "    if 'Is Laundering' in transactions.columns:\n",
        "        class_dist = transactions['Is Laundering'].value_counts()\n",
        "        print(f\"Class distribution: {class_dist}\")\n",
        "        print(f\"SAR rate: {class_dist[1] / len(transactions):.4f}\")\n",
        "    \n",
        "    return transactions, accounts\n",
        "\n",
        "print(\"✓ Data loading function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Node Features\n",
        "def create_enhanced_node_features(transactions, accounts):\n",
        "    \"\"\"Create comprehensive node features\"\"\"\n",
        "    print(\"Creating enhanced node features...\")\n",
        "    \n",
        "    node_features = {}\n",
        "    \n",
        "    for _, account in accounts.iterrows():\n",
        "        account_id = account['Account Number']\n",
        "        \n",
        "        # Get account transactions\n",
        "        account_trans = transactions[\n",
        "            (transactions['Account'] == account_id) | \n",
        "            (transactions['Account.1'] == account_id)\n",
        "        ]\n",
        "        \n",
        "        if len(account_trans) == 0:\n",
        "            # Default features for accounts with no transactions\n",
        "            node_features[account_id] = {\n",
        "                'transaction_count': 0, 'total_sent': 0, 'total_received': 0,\n",
        "                'avg_amount': 0, 'max_amount': 0, 'min_amount': 0,\n",
        "                'temporal_span': 0, 'transaction_frequency': 0,\n",
        "                'currency_diversity': 0, 'bank_diversity': 0,\n",
        "                'night_ratio': 0, 'weekend_ratio': 0,\n",
        "                'is_crypto_bank': 0, 'is_international': 0, 'is_high_frequency': 0\n",
        "            }\n",
        "            continue\n",
        "        \n",
        "        # Basic transaction features\n",
        "        sent_trans = account_trans[account_trans['Account'] == account_id]\n",
        "        received_trans = account_trans[account_trans['Account.1'] == account_id]\n",
        "        \n",
        "        # Amount features\n",
        "        total_sent = sent_trans['Amount Paid'].sum() if len(sent_trans) > 0 else 0\n",
        "        total_received = received_trans['Amount Received'].sum() if len(received_trans) > 0 else 0\n",
        "        avg_amount = account_trans['Amount Paid'].mean()\n",
        "        max_amount = account_trans['Amount Paid'].max()\n",
        "        min_amount = account_trans['Amount Paid'].min()\n",
        "        \n",
        "        # Temporal features\n",
        "        timestamps = pd.to_datetime(account_trans['Timestamp'])\n",
        "        temporal_span = (timestamps.max() - timestamps.min()).days\n",
        "        transaction_frequency = len(account_trans) / max(1, temporal_span)\n",
        "        \n",
        "        # Diversity measures\n",
        "        currency_diversity = account_trans['Payment Currency'].nunique()\n",
        "        bank_diversity = account_trans['To Bank'].nunique()\n",
        "        \n",
        "        # Time-based features\n",
        "        night_transactions = timestamps.dt.hour.isin([22, 23, 0, 1, 2, 3, 4, 5, 6]).sum()\n",
        "        weekend_transactions = timestamps.dt.weekday.isin([5, 6]).sum()\n",
        "        night_ratio = night_transactions / len(account_trans)\n",
        "        weekend_ratio = weekend_transactions / len(account_trans)\n",
        "        \n",
        "        # Risk indicators\n",
        "        is_crypto_bank = 'Crytpo' in str(account_id)\n",
        "        is_international = currency_diversity > 1\n",
        "        is_high_frequency = transaction_frequency > 1.0\n",
        "        \n",
        "        node_features[account_id] = {\n",
        "            'transaction_count': len(account_trans),\n",
        "            'total_sent': total_sent, 'total_received': total_received,\n",
        "            'avg_amount': avg_amount, 'max_amount': max_amount, 'min_amount': min_amount,\n",
        "            'temporal_span': temporal_span, 'transaction_frequency': transaction_frequency,\n",
        "            'currency_diversity': currency_diversity, 'bank_diversity': bank_diversity,\n",
        "            'night_ratio': night_ratio, 'weekend_ratio': weekend_ratio,\n",
        "            'is_crypto_bank': int(is_crypto_bank), 'is_international': int(is_international),\n",
        "            'is_high_frequency': int(is_high_frequency)\n",
        "        }\n",
        "    \n",
        "    return node_features\n",
        "\n",
        "print(\"✓ Enhanced node feature function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Edge Features\n",
        "def create_enhanced_edge_features(transactions):\n",
        "    \"\"\"Create comprehensive edge features\"\"\"\n",
        "    print(\"Creating enhanced edge features...\")\n",
        "    \n",
        "    edge_features = []\n",
        "    edge_labels = []\n",
        "    \n",
        "    # Prepare encoders\n",
        "    currency_encoder = LabelEncoder()\n",
        "    format_encoder = LabelEncoder()\n",
        "    bank_encoder = LabelEncoder()\n",
        "    \n",
        "    currency_encoder.fit(transactions['Payment Currency'].unique())\n",
        "    format_encoder.fit(transactions['Payment Format'].unique())\n",
        "    bank_encoder.fit(transactions['From Bank'].unique())\n",
        "    \n",
        "    for _, transaction in transactions.iterrows():\n",
        "        # Temporal features\n",
        "        timestamp = pd.to_datetime(transaction['Timestamp'])\n",
        "        \n",
        "        # Cyclic temporal encoding\n",
        "        hour_sin = np.sin(2 * np.pi * timestamp.hour / 24)\n",
        "        hour_cos = np.cos(2 * np.pi * timestamp.hour / 24)\n",
        "        day_sin = np.sin(2 * np.pi * timestamp.dayofweek / 7)\n",
        "        day_cos = np.cos(2 * np.pi * timestamp.dayofweek / 7)\n",
        "        month_sin = np.sin(2 * np.pi * timestamp.month / 12)\n",
        "        month_cos = np.cos(2 * np.pi * timestamp.month / 12)\n",
        "        \n",
        "        # Amount features\n",
        "        amount_paid = transaction['Amount Paid']\n",
        "        amount_received = transaction['Amount Received']\n",
        "        \n",
        "        amount_paid_log = np.log1p(amount_paid)\n",
        "        amount_received_log = np.log1p(amount_received)\n",
        "        amount_ratio = amount_paid / max(amount_received, 1)\n",
        "        \n",
        "        # Categorical features\n",
        "        currency_encoded = currency_encoder.transform([transaction['Payment Currency']])[0]\n",
        "        format_encoded = format_encoder.transform([transaction['Payment Format']])[0]\n",
        "        bank_encoded = bank_encoder.transform([transaction['From Bank']])[0]\n",
        "        \n",
        "        # Combine all features\n",
        "        edge_feature = [\n",
        "            hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos,\n",
        "            amount_paid_log, amount_received_log, amount_ratio,\n",
        "            currency_encoded, format_encoded, bank_encoded\n",
        "        ]\n",
        "        \n",
        "        edge_features.append(edge_feature)\n",
        "        edge_labels.append(transaction['Is Laundering'])\n",
        "    \n",
        "    return np.array(edge_features), np.array(edge_labels)\n",
        "\n",
        "print(\"✓ Enhanced edge feature function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Imbalance Handling\n",
        "def handle_class_imbalance(X, y, strategy='smote'):\n",
        "    \"\"\"Handle class imbalance using multiple strategies\"\"\"\n",
        "    print(f\"Handling class imbalance using {strategy}...\")\n",
        "    \n",
        "    if strategy == 'smote':\n",
        "        smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    elif strategy == 'none':\n",
        "        X_resampled, y_resampled = X, y\n",
        "    \n",
        "    print(f\"Original distribution: {np.bincount(y)}\")\n",
        "    print(f\"Resampled distribution: {np.bincount(y_resampled)}\")\n",
        "    \n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def create_cost_sensitive_weights(y):\n",
        "    \"\"\"Create cost-sensitive class weights\"\"\"\n",
        "    print(\"Creating cost-sensitive class weights...\")\n",
        "    \n",
        "    # Compute balanced class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    \n",
        "    # Additional cost for false negatives (missed illicit transactions)\n",
        "    cost_multiplier = 10.0  # Emphasize false negatives\n",
        "    adjusted_weights = class_weights * cost_multiplier\n",
        "    \n",
        "    weight_dict = dict(zip(classes, adjusted_weights))\n",
        "    print(f\"Class weights: {weight_dict}\")\n",
        "    \n",
        "    return weight_dict\n",
        "\n",
        "print(\"✓ Class imbalance handling functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Preprocessing Pipeline\n",
        "def run_simple_preprocessing(data_path, sample_size=10000):\n",
        "    \"\"\"Run simplified preprocessing pipeline\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Simple Enhanced AML Preprocessing Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load data\n",
        "    transactions, accounts = load_aml_data(data_path, sample_size)\n",
        "    \n",
        "    # Create enhanced features\n",
        "    node_features = create_enhanced_node_features(transactions, accounts)\n",
        "    edge_features, edge_labels = create_enhanced_edge_features(transactions)\n",
        "    \n",
        "    # Normalize features\n",
        "    node_feature_matrix = np.array([list(features.values()) for features in node_features.values()])\n",
        "    \n",
        "    # Handle class imbalance\n",
        "    X_resampled, y_resampled = handle_class_imbalance(edge_features, edge_labels, strategy='smote')\n",
        "    \n",
        "    # Create cost-sensitive weights\n",
        "    class_weights = create_cost_sensitive_weights(edge_labels)\n",
        "    \n",
        "    # Create graph structure\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes with features\n",
        "    for account_id, features in node_features.items():\n",
        "        G.add_node(account_id, **features)\n",
        "    \n",
        "    # Add edges with features\n",
        "    for i, (_, transaction) in enumerate(transactions.iterrows()):\n",
        "        from_account = transaction['Account']\n",
        "        to_account = transaction['Account.1']\n",
        "        \n",
        "        if from_account in G.nodes() and to_account in G.nodes():\n",
        "            G.add_edge(from_account, to_account, \n",
        "                      features=edge_features[i], \n",
        "                      label=edge_labels[i])\n",
        "    \n",
        "    print(f\"\\n✓ Preprocessing completed successfully!\")\n",
        "    print(f\"  - Nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"  - Edges: {G.number_of_edges()}\")\n",
        "    print(f\"  - Node features: {node_feature_matrix.shape[1]}\")\n",
        "    print(f\"  - Edge features: {edge_features.shape[1]}\")\n",
        "    print(f\"  - Class distribution: {np.bincount(edge_labels)}\")\n",
        "    \n",
        "    return G, node_features, edge_features, edge_labels, class_weights\n",
        "\n",
        "print(\"✓ Main preprocessing pipeline defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Simple Enhanced Preprocessing\n",
        "print(\"Starting simple enhanced preprocessing...\")\n",
        "\n",
        "# Configuration\n",
        "data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "sample_size = 10000  # Start with sample for testing\n",
        "\n",
        "try:\n",
        "    # Run preprocessing\n",
        "    G, node_features, edge_features, edge_labels, class_weights = run_simple_preprocessing(\n",
        "        data_path, sample_size\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Preprocessing Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"✓ Graph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "    print(f\"✓ Node features: {len(node_features)} accounts with enhanced features\")\n",
        "    print(f\"✓ Edge features: {edge_features.shape[0]} transactions with {edge_features.shape[1]} features\")\n",
        "    print(f\"✓ Class weights: {class_weights}\")\n",
        "    \n",
        "    # Show sample features\n",
        "    print(f\"\\nSample node features for first account:\")\n",
        "    first_account = list(node_features.keys())[0]\n",
        "    print(f\"Account: {first_account}\")\n",
        "    for key, value in list(node_features[first_account].items())[:5]:\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    print(f\"\\nSample edge features:\")\n",
        "    print(f\"  Temporal features: {edge_features[0][:6]}\")\n",
        "    print(f\"  Amount features: {edge_features[0][6:9]}\")\n",
        "    print(f\"  Categorical features: {edge_features[0][9:12]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during preprocessing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
