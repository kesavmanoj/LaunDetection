{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "\n",
        "This notebook implements a simplified version of the enhanced preprocessing pipeline for the IBM AML Synthetic Dataset.\n",
        "\n",
        "## Focus Areas:\n",
        "1. **Enhanced Node Features**: 20+ comprehensive account features\n",
        "2. **Enhanced Edge Features**: 15+ transaction features with cyclic temporal encoding\n",
        "3. **Class Imbalance Handling**: SMOTE + cost-sensitive learning\n",
        "4. **Memory Optimization**: Chunked processing for large datasets\n",
        "\n",
        "## Implementation Strategy:\n",
        "- Start with core features (Phase 1)\n",
        "- Test with sample data first\n",
        "- Gradually scale to full dataset\n",
        "- Monitor memory usage and performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "print(\"=\" * 60)\n",
        "print(\"Simple Enhanced AML Preprocessing Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import time\n",
        "import pickle\n",
        "import shutil\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ Libraries imported successfully\")\n",
        "\n",
        "# Progress estimation function\n",
        "def estimate_processing_time(sample_size):\n",
        "    \"\"\"Estimate processing time based on sample size\"\"\"\n",
        "    if sample_size <= 1000:\n",
        "        return \"1-2 minutes\"\n",
        "    elif sample_size <= 10000:\n",
        "        return \"5-10 minutes\"\n",
        "    elif sample_size <= 100000:\n",
        "        return \"30-60 minutes\"\n",
        "    else:\n",
        "        return \"2-4 hours\"\n",
        "\n",
        "# Checkpoint management functions\n",
        "def create_checkpoint_dir(base_path, sample_size):\n",
        "    \"\"\"Create checkpoint directory for this preprocessing run\"\"\"\n",
        "    checkpoint_dir = os.path.join(base_path, f\"checkpoints_{sample_size}\")\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    return checkpoint_dir\n",
        "\n",
        "def save_checkpoint(checkpoint_dir, step_name, data, metadata=None):\n",
        "    \"\"\"Save checkpoint data with metadata\"\"\"\n",
        "    checkpoint_file = os.path.join(checkpoint_dir, f\"{step_name}.pkl\")\n",
        "    checkpoint_data = {\n",
        "        'data': data,\n",
        "        'metadata': metadata or {},\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'step': step_name\n",
        "    }\n",
        "    \n",
        "    # Save to temporary file first, then move (atomic operation)\n",
        "    temp_file = checkpoint_file + '.tmp'\n",
        "    with open(temp_file, 'wb') as f:\n",
        "        pickle.dump(checkpoint_data, f)\n",
        "    shutil.move(temp_file, checkpoint_file)\n",
        "    \n",
        "    print(f\"✓ Checkpoint saved: {step_name}\")\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, step_name):\n",
        "    \"\"\"Load checkpoint data if it exists\"\"\"\n",
        "    checkpoint_file = os.path.join(checkpoint_dir, f\"{step_name}.pkl\")\n",
        "    if os.path.exists(checkpoint_file):\n",
        "        with open(checkpoint_file, 'rb') as f:\n",
        "            checkpoint_data = pickle.load(f)\n",
        "        print(f\"✓ Checkpoint loaded: {step_name}\")\n",
        "        return checkpoint_data['data'], checkpoint_data['metadata']\n",
        "    return None, None\n",
        "\n",
        "def list_available_checkpoints(checkpoint_dir):\n",
        "    \"\"\"List all available checkpoints\"\"\"\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        return []\n",
        "    \n",
        "    checkpoints = []\n",
        "    for file in os.listdir(checkpoint_dir):\n",
        "        if file.endswith('.pkl'):\n",
        "            step_name = file.replace('.pkl', '')\n",
        "            checkpoints.append(step_name)\n",
        "    return sorted(checkpoints)\n",
        "\n",
        "def resume_from_checkpoint(checkpoint_dir, target_step):\n",
        "    \"\"\"Resume preprocessing from a specific checkpoint\"\"\"\n",
        "    available = list_available_checkpoints(checkpoint_dir)\n",
        "    if target_step in available:\n",
        "        print(f\"✓ Resuming from checkpoint: {target_step}\")\n",
        "        return load_checkpoint(checkpoint_dir, target_step)\n",
        "    else:\n",
        "        print(f\"⚠️  Checkpoint {target_step} not found. Available: {available}\")\n",
        "        return None, None\n",
        "\n",
        "# Chunked processing functions\n",
        "def process_data_in_chunks(data, chunk_size, process_func, checkpoint_dir, step_name):\n",
        "    \"\"\"Process data in chunks with checkpointing\"\"\"\n",
        "    total_chunks = (len(data) + chunk_size - 1) // chunk_size\n",
        "    results = []\n",
        "    \n",
        "    print(f\"📦 Processing {len(data)} items in {total_chunks} chunks of {chunk_size}\")\n",
        "    \n",
        "    for chunk_idx in range(total_chunks):\n",
        "        start_idx = chunk_idx * chunk_size\n",
        "        end_idx = min((chunk_idx + 1) * chunk_size, len(data))\n",
        "        chunk_data = data.iloc[start_idx:end_idx]\n",
        "        \n",
        "        print(f\"  Processing chunk {chunk_idx + 1}/{total_chunks} (items {start_idx}-{end_idx-1})\")\n",
        "        \n",
        "        # Process chunk\n",
        "        chunk_result = process_func(chunk_data)\n",
        "        results.append(chunk_result)\n",
        "        \n",
        "        # Save chunk checkpoint\n",
        "        chunk_checkpoint_name = f\"{step_name}_chunk_{chunk_idx}\"\n",
        "        save_checkpoint(checkpoint_dir, chunk_checkpoint_name, chunk_result, {\n",
        "            'chunk_idx': chunk_idx,\n",
        "            'start_idx': start_idx,\n",
        "            'end_idx': end_idx,\n",
        "            'chunk_size': len(chunk_data)\n",
        "        })\n",
        "        \n",
        "        # Memory cleanup\n",
        "        gc.collect()\n",
        "    \n",
        "    return results\n",
        "\n",
        "def combine_chunk_results(chunk_results, combine_func):\n",
        "    \"\"\"Combine results from multiple chunks\"\"\"\n",
        "    print(f\"🔗 Combining {len(chunk_results)} chunk results...\")\n",
        "    return combine_func(chunk_results)\n",
        "\n",
        "def load_chunk_checkpoints(checkpoint_dir, step_name):\n",
        "    \"\"\"Load all chunk checkpoints for a step\"\"\"\n",
        "    chunk_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(f\"{step_name}_chunk_\")]\n",
        "    chunk_results = []\n",
        "    \n",
        "    for chunk_file in sorted(chunk_files):\n",
        "        chunk_data, metadata = load_checkpoint(checkpoint_dir, chunk_file.replace('.pkl', ''))\n",
        "        if chunk_data is not None:\n",
        "            chunk_results.append(chunk_data)\n",
        "    \n",
        "    return chunk_results\n",
        "\n",
        "print(\"✓ Chunked processing functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Validate Data\n",
        "def load_aml_data(data_path, sample_size=10000):\n",
        "    \"\"\"Load AML data with optional sampling for testing\"\"\"\n",
        "    print(\"Loading IBM AML dataset...\")\n",
        "    \n",
        "    # Load transactions\n",
        "    trans_file = os.path.join(data_path, 'HI-Small_Trans.csv')\n",
        "    if os.path.exists(trans_file):\n",
        "        # Load with sampling for testing\n",
        "        transactions = pd.read_csv(trans_file, nrows=sample_size)\n",
        "        print(f\"✓ Loaded {len(transactions)} transactions (sampled)\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Transaction file not found: {trans_file}\")\n",
        "    \n",
        "    # Load accounts\n",
        "    accounts_file = os.path.join(data_path, 'HI-Small_accounts.csv')\n",
        "    if os.path.exists(accounts_file):\n",
        "        accounts = pd.read_csv(accounts_file)\n",
        "        print(f\"✓ Loaded {len(accounts)} accounts\")\n",
        "    else:\n",
        "        # Create accounts from transactions\n",
        "        print(\"Creating accounts from transaction data...\")\n",
        "        all_accounts = set(transactions['Account'].tolist() + \n",
        "                          transactions['Account.1'].tolist())\n",
        "        \n",
        "        accounts_data = {\n",
        "            'Account Number': list(all_accounts),\n",
        "            'Bank Name': [f\"Bank_{i}\" for i in range(len(all_accounts))],\n",
        "            'Bank ID': [f\"B{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity ID': [f\"E{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity Name': [f\"Entity_{i}\" for i in range(len(all_accounts))]\n",
        "        }\n",
        "        accounts = pd.DataFrame(accounts_data)\n",
        "        print(f\"✓ Created {len(accounts)} accounts from transactions\")\n",
        "    \n",
        "    # Validate data\n",
        "    print(\"\\nData Validation:\")\n",
        "    print(f\"Missing values - Transactions: {transactions.isnull().sum().sum()}\")\n",
        "    print(f\"Missing values - Accounts: {accounts.isnull().sum().sum()}\")\n",
        "    \n",
        "    if 'Is Laundering' in transactions.columns:\n",
        "        class_dist = transactions['Is Laundering'].value_counts()\n",
        "        print(f\"Class distribution: {class_dist}\")\n",
        "        print(f\"SAR rate: {class_dist[1] / len(transactions):.4f}\")\n",
        "    \n",
        "    return transactions, accounts\n",
        "\n",
        "print(\"✓ Data loading function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Node Features\n",
        "def create_enhanced_node_features(transactions, accounts):\n",
        "    \"\"\"Create comprehensive node features\"\"\"\n",
        "    print(\"Creating enhanced node features...\")\n",
        "    \n",
        "    node_features = {}\n",
        "    total_accounts = len(accounts)\n",
        "    \n",
        "    # Progress bar for node features\n",
        "    from tqdm import tqdm\n",
        "    print(f\"Processing {total_accounts} accounts...\")\n",
        "    \n",
        "    for idx, (_, account) in enumerate(tqdm(accounts.iterrows(), total=total_accounts, desc=\"Node Features\")):\n",
        "        account_id = account['Account Number']\n",
        "        \n",
        "        # Get account transactions\n",
        "        account_trans = transactions[\n",
        "            (transactions['Account'] == account_id) | \n",
        "            (transactions['Account.1'] == account_id)\n",
        "        ]\n",
        "        \n",
        "        if len(account_trans) == 0:\n",
        "            # Default features for accounts with no transactions\n",
        "            node_features[account_id] = {\n",
        "                'transaction_count': 0, 'total_sent': 0, 'total_received': 0,\n",
        "                'avg_amount': 0, 'max_amount': 0, 'min_amount': 0,\n",
        "                'temporal_span': 0, 'transaction_frequency': 0,\n",
        "                'currency_diversity': 0, 'bank_diversity': 0,\n",
        "                'night_ratio': 0, 'weekend_ratio': 0,\n",
        "                'is_crypto_bank': 0, 'is_international': 0, 'is_high_frequency': 0\n",
        "            }\n",
        "            continue\n",
        "        \n",
        "        # Basic transaction features\n",
        "        sent_trans = account_trans[account_trans['Account'] == account_id]\n",
        "        received_trans = account_trans[account_trans['Account.1'] == account_id]\n",
        "        \n",
        "        # Amount features\n",
        "        total_sent = sent_trans['Amount Paid'].sum() if len(sent_trans) > 0 else 0\n",
        "        total_received = received_trans['Amount Received'].sum() if len(received_trans) > 0 else 0\n",
        "        avg_amount = account_trans['Amount Paid'].mean()\n",
        "        max_amount = account_trans['Amount Paid'].max()\n",
        "        min_amount = account_trans['Amount Paid'].min()\n",
        "        \n",
        "        # Temporal features\n",
        "        timestamps = pd.to_datetime(account_trans['Timestamp'])\n",
        "        temporal_span = (timestamps.max() - timestamps.min()).days\n",
        "        transaction_frequency = len(account_trans) / max(1, temporal_span)\n",
        "        \n",
        "        # Diversity measures\n",
        "        currency_diversity = account_trans['Payment Currency'].nunique()\n",
        "        bank_diversity = account_trans['To Bank'].nunique()\n",
        "        \n",
        "        # Time-based features\n",
        "        night_transactions = timestamps.dt.hour.isin([22, 23, 0, 1, 2, 3, 4, 5, 6]).sum()\n",
        "        weekend_transactions = timestamps.dt.weekday.isin([5, 6]).sum()\n",
        "        night_ratio = night_transactions / len(account_trans)\n",
        "        weekend_ratio = weekend_transactions / len(account_trans)\n",
        "        \n",
        "        # Risk indicators\n",
        "        is_crypto_bank = 'Crytpo' in str(account_id)\n",
        "        is_international = currency_diversity > 1\n",
        "        is_high_frequency = transaction_frequency > 1.0\n",
        "        \n",
        "        node_features[account_id] = {\n",
        "            'transaction_count': len(account_trans),\n",
        "            'total_sent': total_sent, 'total_received': total_received,\n",
        "            'avg_amount': avg_amount, 'max_amount': max_amount, 'min_amount': min_amount,\n",
        "            'temporal_span': temporal_span, 'transaction_frequency': transaction_frequency,\n",
        "            'currency_diversity': currency_diversity, 'bank_diversity': bank_diversity,\n",
        "            'night_ratio': night_ratio, 'weekend_ratio': weekend_ratio,\n",
        "            'is_crypto_bank': int(is_crypto_bank), 'is_international': int(is_international),\n",
        "            'is_high_frequency': int(is_high_frequency)\n",
        "        }\n",
        "    \n",
        "    return node_features\n",
        "\n",
        "print(\"✓ Enhanced node feature function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Edge Features\n",
        "def create_enhanced_edge_features(transactions):\n",
        "    \"\"\"Create comprehensive edge features\"\"\"\n",
        "    print(\"Creating enhanced edge features...\")\n",
        "    \n",
        "    edge_features = []\n",
        "    edge_labels = []\n",
        "    total_transactions = len(transactions)\n",
        "    \n",
        "    # Progress bar for edge features\n",
        "    from tqdm import tqdm\n",
        "    print(f\"Processing {total_transactions} transactions...\")\n",
        "    \n",
        "    # Prepare encoders\n",
        "    print(\"Preparing encoders...\")\n",
        "    currency_encoder = LabelEncoder()\n",
        "    format_encoder = LabelEncoder()\n",
        "    bank_encoder = LabelEncoder()\n",
        "    \n",
        "    currency_encoder.fit(transactions['Payment Currency'].unique())\n",
        "    format_encoder.fit(transactions['Payment Format'].unique())\n",
        "    bank_encoder.fit(transactions['From Bank'].unique())\n",
        "    print(\"✓ Encoders prepared\")\n",
        "    \n",
        "    for _, transaction in tqdm(transactions.iterrows(), total=total_transactions, desc=\"Edge Features\"):\n",
        "        # Temporal features\n",
        "        timestamp = pd.to_datetime(transaction['Timestamp'])\n",
        "        \n",
        "        # Cyclic temporal encoding\n",
        "        hour_sin = np.sin(2 * np.pi * timestamp.hour / 24)\n",
        "        hour_cos = np.cos(2 * np.pi * timestamp.hour / 24)\n",
        "        day_sin = np.sin(2 * np.pi * timestamp.dayofweek / 7)\n",
        "        day_cos = np.cos(2 * np.pi * timestamp.dayofweek / 7)\n",
        "        month_sin = np.sin(2 * np.pi * timestamp.month / 12)\n",
        "        month_cos = np.cos(2 * np.pi * timestamp.month / 12)\n",
        "        \n",
        "        # Amount features\n",
        "        amount_paid = transaction['Amount Paid']\n",
        "        amount_received = transaction['Amount Received']\n",
        "        \n",
        "        amount_paid_log = np.log1p(amount_paid)\n",
        "        amount_received_log = np.log1p(amount_received)\n",
        "        amount_ratio = amount_paid / max(amount_received, 1)\n",
        "        \n",
        "        # Categorical features\n",
        "        currency_encoded = currency_encoder.transform([transaction['Payment Currency']])[0]\n",
        "        format_encoded = format_encoder.transform([transaction['Payment Format']])[0]\n",
        "        bank_encoded = bank_encoder.transform([transaction['From Bank']])[0]\n",
        "        \n",
        "        # Combine all features\n",
        "        edge_feature = [\n",
        "            hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos,\n",
        "            amount_paid_log, amount_received_log, amount_ratio,\n",
        "            currency_encoded, format_encoded, bank_encoded\n",
        "        ]\n",
        "        \n",
        "        edge_features.append(edge_feature)\n",
        "        edge_labels.append(transaction['Is Laundering'])\n",
        "    \n",
        "    return np.array(edge_features), np.array(edge_labels)\n",
        "\n",
        "print(\"✓ Enhanced edge feature function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Imbalance Handling\n",
        "def handle_class_imbalance(X, y, strategy='smote'):\n",
        "    \"\"\"Handle class imbalance using multiple strategies\"\"\"\n",
        "    print(f\"Handling class imbalance using {strategy}...\")\n",
        "    \n",
        "    # Check class distribution\n",
        "    class_counts = np.bincount(y)\n",
        "    minority_count = min(class_counts)\n",
        "    majority_count = max(class_counts)\n",
        "    \n",
        "    print(f\"  - Original samples: {len(X)}\")\n",
        "    print(f\"  - Original distribution: {class_counts}\")\n",
        "    print(f\"  - Minority class: {minority_count} samples\")\n",
        "    print(f\"  - Majority class: {majority_count} samples\")\n",
        "    \n",
        "    if strategy == 'smote':\n",
        "        # Check if SMOTE can be applied\n",
        "        if minority_count < 2:\n",
        "            print(\"⚠️  Too few minority samples for SMOTE (need at least 2)\")\n",
        "            print(\"🔄 Falling back to cost-sensitive learning only\")\n",
        "            X_resampled, y_resampled = X, y\n",
        "            print(\"✓ Cost-sensitive learning applied (no resampling)\")\n",
        "            \n",
        "        elif minority_count < 4:\n",
        "            print(\"⚠️  Very few minority samples for SMOTE (need at least 4)\")\n",
        "            print(\"🔄 Using reduced k_neighbors for SMOTE\")\n",
        "            try:\n",
        "                smote = SMOTE(random_state=42, k_neighbors=min(3, minority_count-1))\n",
        "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "                print(f\"  - Resampled samples: {len(X_resampled)}\")\n",
        "                print(f\"  - Resampled distribution: {np.bincount(y_resampled)}\")\n",
        "                print(\"✓ SMOTE completed with reduced k_neighbors\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  SMOTE failed: {e}\")\n",
        "                print(\"🔄 Falling back to cost-sensitive learning only\")\n",
        "                X_resampled, y_resampled = X, y\n",
        "                print(\"✓ Cost-sensitive learning applied (no resampling)\")\n",
        "        else:\n",
        "            print(\"Applying SMOTE oversampling...\")\n",
        "            try:\n",
        "                smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "                print(f\"  - Resampled samples: {len(X_resampled)}\")\n",
        "                print(f\"  - Resampled distribution: {np.bincount(y_resampled)}\")\n",
        "                print(\"✓ SMOTE completed\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  SMOTE failed: {e}\")\n",
        "                print(\"🔄 Falling back to cost-sensitive learning only\")\n",
        "                X_resampled, y_resampled = X, y\n",
        "                print(\"✓ Cost-sensitive learning applied (no resampling)\")\n",
        "        \n",
        "    elif strategy == 'none':\n",
        "        X_resampled, y_resampled = X, y\n",
        "        print(\"✓ No resampling applied\")\n",
        "    \n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def create_cost_sensitive_weights(y):\n",
        "    \"\"\"Create cost-sensitive class weights\"\"\"\n",
        "    print(\"Creating cost-sensitive class weights...\")\n",
        "    \n",
        "    # Check class distribution\n",
        "    class_counts = np.bincount(y)\n",
        "    minority_count = min(class_counts)\n",
        "    majority_count = max(class_counts)\n",
        "    \n",
        "    print(f\"  - Class distribution: {class_counts}\")\n",
        "    print(f\"  - Minority class: {minority_count} samples\")\n",
        "    print(f\"  - Majority class: {majority_count} samples\")\n",
        "    \n",
        "    # Compute balanced class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    \n",
        "    # Additional cost for false negatives (missed illicit transactions)\n",
        "    # Use higher multiplier for extreme imbalance\n",
        "    if minority_count < 10:\n",
        "        cost_multiplier = 100.0  # Very high cost for extreme imbalance\n",
        "        print(f\"  - Extreme imbalance detected: using {cost_multiplier}x cost multiplier\")\n",
        "    elif minority_count < 100:\n",
        "        cost_multiplier = 50.0   # High cost for severe imbalance\n",
        "        print(f\"  - Severe imbalance detected: using {cost_multiplier}x cost multiplier\")\n",
        "    else:\n",
        "        cost_multiplier = 10.0   # Standard cost for moderate imbalance\n",
        "        print(f\"  - Moderate imbalance detected: using {cost_multiplier}x cost multiplier\")\n",
        "    \n",
        "    adjusted_weights = class_weights * cost_multiplier\n",
        "    \n",
        "    weight_dict = dict(zip(classes, adjusted_weights))\n",
        "    print(f\"  - Final class weights: {weight_dict}\")\n",
        "    \n",
        "    return weight_dict\n",
        "\n",
        "print(\"✓ Class imbalance handling functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Preprocessing Pipeline\n",
        "def run_simple_preprocessing(data_path, sample_size=10000, resume_from=None, chunk_size=1000):\n",
        "    \"\"\"Run simplified preprocessing pipeline with checkpointing\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Simple Enhanced AML Preprocessing Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = create_checkpoint_dir(data_path, sample_size)\n",
        "    print(f\"📁 Checkpoint directory: {checkpoint_dir}\")\n",
        "    \n",
        "    # Check for existing checkpoints\n",
        "    available_checkpoints = list_available_checkpoints(checkpoint_dir)\n",
        "    if available_checkpoints:\n",
        "        print(f\"📋 Available checkpoints: {available_checkpoints}\")\n",
        "        if resume_from and resume_from in available_checkpoints:\n",
        "            print(f\"🔄 Resuming from checkpoint: {resume_from}\")\n",
        "        else:\n",
        "            print(\"⚠️  Checkpoints found but resume_from not specified. Starting fresh.\")\n",
        "    \n",
        "    # Load data\n",
        "    print(\"\\n📊 STEP 1: Loading Data\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if data is already loaded\n",
        "    if resume_from and resume_from in ['data_loaded', 'node_features', 'edge_features', 'normalized', 'imbalanced', 'weights', 'graph_created']:\n",
        "        print(\"⏭️  Skipping data loading (resuming from later step)\")\n",
        "        transactions, accounts = None, None\n",
        "    else:\n",
        "        transactions, accounts = load_aml_data(data_path, sample_size)\n",
        "        # Save data checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'data_loaded', {\n",
        "            'transactions': transactions,\n",
        "            'accounts': accounts,\n",
        "            'sample_size': sample_size\n",
        "        }, {'step': 'data_loading', 'sample_size': sample_size})\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Data loading completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Create enhanced features\n",
        "    print(\"\\n🔧 STEP 2: Creating Node Features\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if node features already exist\n",
        "    if resume_from and resume_from in ['node_features', 'edge_features', 'normalized', 'imbalanced', 'weights', 'graph_created']:\n",
        "        print(\"⏭️  Loading node features from checkpoint...\")\n",
        "        node_features, _ = load_checkpoint(checkpoint_dir, 'node_features')\n",
        "        if node_features is None:\n",
        "            print(\"⚠️  Node features checkpoint not found, creating new...\")\n",
        "            node_features = create_enhanced_node_features(transactions, accounts)\n",
        "    else:\n",
        "        # Load data if not already loaded\n",
        "        if transactions is None or accounts is None:\n",
        "            data_checkpoint, _ = load_checkpoint(checkpoint_dir, 'data_loaded')\n",
        "            if data_checkpoint:\n",
        "                transactions = data_checkpoint['transactions']\n",
        "                accounts = data_checkpoint['accounts']\n",
        "            else:\n",
        "                raise ValueError(\"No data available and no checkpoint found\")\n",
        "        \n",
        "        # Process in chunks for large datasets\n",
        "        if len(accounts) > chunk_size:\n",
        "            print(f\"📦 Processing {len(accounts)} accounts in chunks of {chunk_size}\")\n",
        "            node_features = process_data_in_chunks(\n",
        "                accounts, chunk_size, \n",
        "                lambda chunk: create_enhanced_node_features(transactions, chunk),\n",
        "                checkpoint_dir, 'node_features'\n",
        "            )\n",
        "            # Combine chunk results\n",
        "            node_features = combine_chunk_results(node_features, lambda chunks: {k: v for chunk in chunks for k, v in chunk.items()})\n",
        "        else:\n",
        "            node_features = create_enhanced_node_features(transactions, accounts)\n",
        "        \n",
        "        # Save node features checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'node_features', node_features, {\n",
        "            'step': 'node_features',\n",
        "            'num_accounts': len(accounts),\n",
        "            'num_features': len(node_features)\n",
        "        })\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Node features completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    print(\"\\n🔧 STEP 3: Creating Edge Features\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if edge features already exist\n",
        "    if resume_from and resume_from in ['edge_features', 'normalized', 'imbalanced', 'weights', 'graph_created']:\n",
        "        print(\"⏭️  Loading edge features from checkpoint...\")\n",
        "        edge_data, _ = load_checkpoint(checkpoint_dir, 'edge_features')\n",
        "        if edge_data:\n",
        "            edge_features, edge_labels = edge_data['features'], edge_data['labels']\n",
        "        else:\n",
        "            print(\"⚠️  Edge features checkpoint not found, creating new...\")\n",
        "            edge_features, edge_labels = create_enhanced_edge_features(transactions)\n",
        "    else:\n",
        "        # Load data if not already loaded\n",
        "        if transactions is None:\n",
        "            data_checkpoint, _ = load_checkpoint(checkpoint_dir, 'data_loaded')\n",
        "            if data_checkpoint:\n",
        "                transactions = data_checkpoint['transactions']\n",
        "            else:\n",
        "                raise ValueError(\"No data available and no checkpoint found\")\n",
        "        \n",
        "        # Process in chunks for large datasets\n",
        "        if len(transactions) > chunk_size:\n",
        "            print(f\"📦 Processing {len(transactions)} transactions in chunks of {chunk_size}\")\n",
        "            edge_results = process_data_in_chunks(\n",
        "                transactions, chunk_size,\n",
        "                lambda chunk: create_enhanced_edge_features(chunk),\n",
        "                checkpoint_dir, 'edge_features'\n",
        "            )\n",
        "            # Combine chunk results\n",
        "            all_features = []\n",
        "            all_labels = []\n",
        "            for chunk_result in edge_results:\n",
        "                all_features.extend(chunk_result[0])\n",
        "                all_labels.extend(chunk_result[1])\n",
        "            edge_features = np.array(all_features)\n",
        "            edge_labels = np.array(all_labels)\n",
        "        else:\n",
        "            edge_features, edge_labels = create_enhanced_edge_features(transactions)\n",
        "        \n",
        "        # Save edge features checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'edge_features', {\n",
        "            'features': edge_features,\n",
        "            'labels': edge_labels\n",
        "        }, {\n",
        "            'step': 'edge_features',\n",
        "            'num_transactions': len(transactions),\n",
        "            'num_features': len(edge_features)\n",
        "        })\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Edge features completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Normalize features\n",
        "    print(\"\\n📊 STEP 4: Normalizing Features\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if normalization already exists\n",
        "    if resume_from and resume_from in ['normalized', 'imbalanced', 'weights', 'graph_created']:\n",
        "        print(\"⏭️  Loading normalized features from checkpoint...\")\n",
        "        normalized_data, _ = load_checkpoint(checkpoint_dir, 'normalized')\n",
        "        if normalized_data:\n",
        "            node_feature_matrix = normalized_data['node_feature_matrix']\n",
        "        else:\n",
        "            print(\"⚠️  Normalized features checkpoint not found, creating new...\")\n",
        "            node_feature_matrix = np.array([list(features.values()) for features in node_features.values()])\n",
        "    else:\n",
        "        node_feature_matrix = np.array([list(features.values()) for features in node_features.values()])\n",
        "        # Save normalization checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'normalized', {\n",
        "            'node_feature_matrix': node_feature_matrix\n",
        "        }, {\n",
        "            'step': 'normalization',\n",
        "            'matrix_shape': node_feature_matrix.shape\n",
        "        })\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Feature normalization completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Handle class imbalance\n",
        "    print(\"\\n⚖️ STEP 5: Handling Class Imbalance\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if imbalance handling already exists\n",
        "    if resume_from and resume_from in ['imbalanced', 'weights', 'graph_created']:\n",
        "        print(\"⏭️  Loading imbalance handling from checkpoint...\")\n",
        "        imbalance_data, _ = load_checkpoint(checkpoint_dir, 'imbalanced')\n",
        "        if imbalance_data:\n",
        "            X_resampled, y_resampled = imbalance_data['X_resampled'], imbalance_data['y_resampled']\n",
        "        else:\n",
        "            print(\"⚠️  Imbalance handling checkpoint not found, creating new...\")\n",
        "            # Try SMOTE first, fallback to cost-sensitive learning if it fails\n",
        "            try:\n",
        "                X_resampled, y_resampled = handle_class_imbalance(edge_features, edge_labels, strategy='smote')\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  SMOTE failed completely: {e}\")\n",
        "                print(\"🔄 Using cost-sensitive learning only\")\n",
        "                X_resampled, y_resampled = edge_features, edge_labels\n",
        "                print(\"✓ Cost-sensitive learning applied (no resampling)\")\n",
        "    else:\n",
        "        # Try SMOTE first, fallback to cost-sensitive learning if it fails\n",
        "        try:\n",
        "            X_resampled, y_resampled = handle_class_imbalance(edge_features, edge_labels, strategy='smote')\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  SMOTE failed completely: {e}\")\n",
        "            print(\"🔄 Using cost-sensitive learning only\")\n",
        "            X_resampled, y_resampled = edge_features, edge_labels\n",
        "            print(\"✓ Cost-sensitive learning applied (no resampling)\")\n",
        "        \n",
        "        # Save imbalance handling checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'imbalanced', {\n",
        "            'X_resampled': X_resampled,\n",
        "            'y_resampled': y_resampled\n",
        "        }, {\n",
        "            'step': 'imbalance_handling',\n",
        "            'original_shape': edge_features.shape,\n",
        "            'resampled_shape': X_resampled.shape\n",
        "        })\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Class imbalance handling completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Create cost-sensitive weights\n",
        "    print(\"\\n🎯 STEP 6: Creating Cost-Sensitive Weights\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if weights already exist\n",
        "    if resume_from and resume_from in ['weights', 'graph_created']:\n",
        "        print(\"⏭️  Loading cost-sensitive weights from checkpoint...\")\n",
        "        weights_data, _ = load_checkpoint(checkpoint_dir, 'weights')\n",
        "        if weights_data:\n",
        "            class_weights = weights_data['class_weights']\n",
        "        else:\n",
        "            print(\"⚠️  Weights checkpoint not found, creating new...\")\n",
        "            class_weights = create_cost_sensitive_weights(edge_labels)\n",
        "    else:\n",
        "        class_weights = create_cost_sensitive_weights(edge_labels)\n",
        "        # Save weights checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'weights', {\n",
        "            'class_weights': class_weights\n",
        "        }, {\n",
        "            'step': 'weights_creation',\n",
        "            'weights': class_weights\n",
        "        })\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Cost-sensitive weights completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Create graph structure\n",
        "    print(\"\\n🕸️ STEP 7: Creating Graph Structure\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Check if graph already exists\n",
        "    if resume_from and resume_from == 'graph_created':\n",
        "        print(\"⏭️  Loading graph from checkpoint...\")\n",
        "        graph_data, _ = load_checkpoint(checkpoint_dir, 'graph_created')\n",
        "        if graph_data:\n",
        "            G = graph_data['graph']\n",
        "        else:\n",
        "            print(\"⚠️  Graph checkpoint not found, creating new...\")\n",
        "            G = nx.DiGraph()\n",
        "            # Add nodes and edges (same as below)\n",
        "    else:\n",
        "        G = nx.DiGraph()\n",
        "        \n",
        "        # Add nodes with features\n",
        "        print(\"Adding nodes...\")\n",
        "        for account_id, features in node_features.items():\n",
        "            G.add_node(account_id, **features)\n",
        "        \n",
        "        # Add edges with features\n",
        "        print(\"Adding edges...\")\n",
        "        from tqdm import tqdm\n",
        "        \n",
        "        # Load transactions if not already loaded\n",
        "        if transactions is None:\n",
        "            data_checkpoint, _ = load_checkpoint(checkpoint_dir, 'data_loaded')\n",
        "            if data_checkpoint:\n",
        "                transactions = data_checkpoint['transactions']\n",
        "            else:\n",
        "                raise ValueError(\"No data available and no checkpoint found\")\n",
        "        \n",
        "        for i, (_, transaction) in enumerate(tqdm(transactions.iterrows(), total=len(transactions), desc=\"Adding Edges\")):\n",
        "            from_account = transaction['Account']\n",
        "            to_account = transaction['Account.1']\n",
        "            \n",
        "            if from_account in G.nodes() and to_account in G.nodes():\n",
        "                G.add_edge(from_account, to_account, \n",
        "                          features=edge_features[i], \n",
        "                          label=edge_labels[i])\n",
        "        \n",
        "        # Save graph checkpoint\n",
        "        save_checkpoint(checkpoint_dir, 'graph_created', {\n",
        "            'graph': G\n",
        "        }, {\n",
        "            'step': 'graph_creation',\n",
        "            'num_nodes': G.number_of_nodes(),\n",
        "            'num_edges': G.number_of_edges()\n",
        "        })\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"✓ Graph structure completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n🎉 Preprocessing completed successfully!\")\n",
        "    print(f\"⏱️ Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "    print(f\"📊 Results:\")\n",
        "    print(f\"  - Nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"  - Edges: {G.number_of_edges()}\")\n",
        "    print(f\"  - Node features: {node_feature_matrix.shape[1]}\")\n",
        "    print(f\"  - Edge features: {edge_features.shape[1]}\")\n",
        "    print(f\"  - Class distribution: {np.bincount(edge_labels)}\")\n",
        "    \n",
        "    return G, node_features, edge_features, edge_labels, class_weights\n",
        "\n",
        "print(\"✓ Main preprocessing pipeline defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Simple Enhanced Preprocessing with Checkpointing\n",
        "print(\"Starting simple enhanced preprocessing with checkpointing...\")\n",
        "\n",
        "# Configuration\n",
        "data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "sample_size = 10000  # Start with sample for testing\n",
        "chunk_size = 1000    # Process in chunks of 1000 for large datasets\n",
        "resume_from = None   # Set to resume from specific checkpoint: 'data_loaded', 'node_features', 'edge_features', 'normalized', 'imbalanced', 'weights', 'graph_created'\n",
        "\n",
        "# Show time estimation\n",
        "estimated_time = estimate_processing_time(sample_size)\n",
        "print(f\"📊 Sample size: {sample_size:,} transactions\")\n",
        "print(f\"⏱️ Estimated processing time: {estimated_time}\")\n",
        "print(f\"💾 Expected memory usage: ~2-4 GB\")\n",
        "print(f\"🔧 Features to create: 15 node features + 12 edge features per transaction\")\n",
        "print(f\"📦 Chunk size: {chunk_size:,} items per chunk\")\n",
        "print(f\"🔄 Resume from: {resume_from or 'Start from beginning'}\")\n",
        "\n",
        "try:\n",
        "    # Run preprocessing with checkpointing\n",
        "    G, node_features, edge_features, edge_labels, class_weights = run_simple_preprocessing(\n",
        "        data_path, sample_size, resume_from=resume_from, chunk_size=chunk_size\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Preprocessing Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"✓ Graph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "    print(f\"✓ Node features: {len(node_features)} accounts with enhanced features\")\n",
        "    print(f\"✓ Edge features: {edge_features.shape[0]} transactions with {edge_features.shape[1]} features\")\n",
        "    print(f\"✓ Class weights: {class_weights}\")\n",
        "    \n",
        "    # Show sample features\n",
        "    print(f\"\\nSample node features for first account:\")\n",
        "    first_account = list(node_features.keys())[0]\n",
        "    print(f\"Account: {first_account}\")\n",
        "    for key, value in list(node_features[first_account].items())[:5]:\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    print(f\"\\nSample edge features:\")\n",
        "    print(f\"  Temporal features: {edge_features[0][:6]}\")\n",
        "    print(f\"  Amount features: {edge_features[0][6:9]}\")\n",
        "    print(f\"  Categorical features: {edge_features[0][9:12]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"✗ Error during preprocessing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
