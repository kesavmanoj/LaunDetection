{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "\n",
        "This notebook implements a simplified version of the enhanced preprocessing pipeline for the IBM AML Synthetic Dataset.\n",
        "\n",
        "## Focus Areas:\n",
        "1. **Enhanced Node Features**: 20+ comprehensive account features\n",
        "2. **Enhanced Edge Features**: 15+ transaction features with cyclic temporal encoding\n",
        "3. **Class Imbalance Handling**: SMOTE + cost-sensitive learning\n",
        "4. **Memory Optimization**: Chunked processing for large datasets\n",
        "\n",
        "## Implementation Strategy:\n",
        "- Start with core features (Phase 1)\n",
        "- Test with sample data first\n",
        "- Gradually scale to full dataset\n",
        "- Monitor memory usage and performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Enhanced Preprocessing for IBM AML Multi-GNN\n",
        "print(\"=\" * 60)\n",
        "print(\"Simple Enhanced AML Preprocessing Pipeline\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ Libraries imported successfully\")\n",
        "\n",
        "# Progress estimation function\n",
        "def estimate_processing_time(sample_size):\n",
        "    \"\"\"Estimate processing time based on sample size\"\"\"\n",
        "    if sample_size <= 1000:\n",
        "        return \"1-2 minutes\"\n",
        "    elif sample_size <= 10000:\n",
        "        return \"5-10 minutes\"\n",
        "    elif sample_size <= 100000:\n",
        "        return \"30-60 minutes\"\n",
        "    else:\n",
        "        return \"2-4 hours\"\n",
        "\n",
        "print(\"âœ“ Progress estimation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and Validate Data\n",
        "def load_aml_data(data_path, sample_size=10000):\n",
        "    \"\"\"Load AML data with optional sampling for testing\"\"\"\n",
        "    print(\"Loading IBM AML dataset...\")\n",
        "    \n",
        "    # Load transactions\n",
        "    trans_file = os.path.join(data_path, 'HI-Small_Trans.csv')\n",
        "    if os.path.exists(trans_file):\n",
        "        # Load with sampling for testing\n",
        "        transactions = pd.read_csv(trans_file, nrows=sample_size)\n",
        "        print(f\"âœ“ Loaded {len(transactions)} transactions (sampled)\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"Transaction file not found: {trans_file}\")\n",
        "    \n",
        "    # Load accounts\n",
        "    accounts_file = os.path.join(data_path, 'HI-Small_accounts.csv')\n",
        "    if os.path.exists(accounts_file):\n",
        "        accounts = pd.read_csv(accounts_file)\n",
        "        print(f\"âœ“ Loaded {len(accounts)} accounts\")\n",
        "    else:\n",
        "        # Create accounts from transactions\n",
        "        print(\"Creating accounts from transaction data...\")\n",
        "        all_accounts = set(transactions['Account'].tolist() + \n",
        "                          transactions['Account.1'].tolist())\n",
        "        \n",
        "        accounts_data = {\n",
        "            'Account Number': list(all_accounts),\n",
        "            'Bank Name': [f\"Bank_{i}\" for i in range(len(all_accounts))],\n",
        "            'Bank ID': [f\"B{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity ID': [f\"E{i}\" for i in range(len(all_accounts))],\n",
        "            'Entity Name': [f\"Entity_{i}\" for i in range(len(all_accounts))]\n",
        "        }\n",
        "        accounts = pd.DataFrame(accounts_data)\n",
        "        print(f\"âœ“ Created {len(accounts)} accounts from transactions\")\n",
        "    \n",
        "    # Validate data\n",
        "    print(\"\\nData Validation:\")\n",
        "    print(f\"Missing values - Transactions: {transactions.isnull().sum().sum()}\")\n",
        "    print(f\"Missing values - Accounts: {accounts.isnull().sum().sum()}\")\n",
        "    \n",
        "    if 'Is Laundering' in transactions.columns:\n",
        "        class_dist = transactions['Is Laundering'].value_counts()\n",
        "        print(f\"Class distribution: {class_dist}\")\n",
        "        print(f\"SAR rate: {class_dist[1] / len(transactions):.4f}\")\n",
        "    \n",
        "    return transactions, accounts\n",
        "\n",
        "print(\"âœ“ Data loading function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Node Features\n",
        "def create_enhanced_node_features(transactions, accounts):\n",
        "    \"\"\"Create comprehensive node features\"\"\"\n",
        "    print(\"Creating enhanced node features...\")\n",
        "    \n",
        "    node_features = {}\n",
        "    total_accounts = len(accounts)\n",
        "    \n",
        "    # Progress bar for node features\n",
        "    from tqdm import tqdm\n",
        "    print(f\"Processing {total_accounts} accounts...\")\n",
        "    \n",
        "    for idx, (_, account) in enumerate(tqdm(accounts.iterrows(), total=total_accounts, desc=\"Node Features\")):\n",
        "        account_id = account['Account Number']\n",
        "        \n",
        "        # Get account transactions\n",
        "        account_trans = transactions[\n",
        "            (transactions['Account'] == account_id) | \n",
        "            (transactions['Account.1'] == account_id)\n",
        "        ]\n",
        "        \n",
        "        if len(account_trans) == 0:\n",
        "            # Default features for accounts with no transactions\n",
        "            node_features[account_id] = {\n",
        "                'transaction_count': 0, 'total_sent': 0, 'total_received': 0,\n",
        "                'avg_amount': 0, 'max_amount': 0, 'min_amount': 0,\n",
        "                'temporal_span': 0, 'transaction_frequency': 0,\n",
        "                'currency_diversity': 0, 'bank_diversity': 0,\n",
        "                'night_ratio': 0, 'weekend_ratio': 0,\n",
        "                'is_crypto_bank': 0, 'is_international': 0, 'is_high_frequency': 0\n",
        "            }\n",
        "            continue\n",
        "        \n",
        "        # Basic transaction features\n",
        "        sent_trans = account_trans[account_trans['Account'] == account_id]\n",
        "        received_trans = account_trans[account_trans['Account.1'] == account_id]\n",
        "        \n",
        "        # Amount features\n",
        "        total_sent = sent_trans['Amount Paid'].sum() if len(sent_trans) > 0 else 0\n",
        "        total_received = received_trans['Amount Received'].sum() if len(received_trans) > 0 else 0\n",
        "        avg_amount = account_trans['Amount Paid'].mean()\n",
        "        max_amount = account_trans['Amount Paid'].max()\n",
        "        min_amount = account_trans['Amount Paid'].min()\n",
        "        \n",
        "        # Temporal features\n",
        "        timestamps = pd.to_datetime(account_trans['Timestamp'])\n",
        "        temporal_span = (timestamps.max() - timestamps.min()).days\n",
        "        transaction_frequency = len(account_trans) / max(1, temporal_span)\n",
        "        \n",
        "        # Diversity measures\n",
        "        currency_diversity = account_trans['Payment Currency'].nunique()\n",
        "        bank_diversity = account_trans['To Bank'].nunique()\n",
        "        \n",
        "        # Time-based features\n",
        "        night_transactions = timestamps.dt.hour.isin([22, 23, 0, 1, 2, 3, 4, 5, 6]).sum()\n",
        "        weekend_transactions = timestamps.dt.weekday.isin([5, 6]).sum()\n",
        "        night_ratio = night_transactions / len(account_trans)\n",
        "        weekend_ratio = weekend_transactions / len(account_trans)\n",
        "        \n",
        "        # Risk indicators\n",
        "        is_crypto_bank = 'Crytpo' in str(account_id)\n",
        "        is_international = currency_diversity > 1\n",
        "        is_high_frequency = transaction_frequency > 1.0\n",
        "        \n",
        "        node_features[account_id] = {\n",
        "            'transaction_count': len(account_trans),\n",
        "            'total_sent': total_sent, 'total_received': total_received,\n",
        "            'avg_amount': avg_amount, 'max_amount': max_amount, 'min_amount': min_amount,\n",
        "            'temporal_span': temporal_span, 'transaction_frequency': transaction_frequency,\n",
        "            'currency_diversity': currency_diversity, 'bank_diversity': bank_diversity,\n",
        "            'night_ratio': night_ratio, 'weekend_ratio': weekend_ratio,\n",
        "            'is_crypto_bank': int(is_crypto_bank), 'is_international': int(is_international),\n",
        "            'is_high_frequency': int(is_high_frequency)\n",
        "        }\n",
        "    \n",
        "    return node_features\n",
        "\n",
        "print(\"âœ“ Enhanced node feature function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Edge Features\n",
        "def create_enhanced_edge_features(transactions):\n",
        "    \"\"\"Create comprehensive edge features\"\"\"\n",
        "    print(\"Creating enhanced edge features...\")\n",
        "    \n",
        "    edge_features = []\n",
        "    edge_labels = []\n",
        "    total_transactions = len(transactions)\n",
        "    \n",
        "    # Progress bar for edge features\n",
        "    from tqdm import tqdm\n",
        "    print(f\"Processing {total_transactions} transactions...\")\n",
        "    \n",
        "    # Prepare encoders\n",
        "    print(\"Preparing encoders...\")\n",
        "    currency_encoder = LabelEncoder()\n",
        "    format_encoder = LabelEncoder()\n",
        "    bank_encoder = LabelEncoder()\n",
        "    \n",
        "    currency_encoder.fit(transactions['Payment Currency'].unique())\n",
        "    format_encoder.fit(transactions['Payment Format'].unique())\n",
        "    bank_encoder.fit(transactions['From Bank'].unique())\n",
        "    print(\"âœ“ Encoders prepared\")\n",
        "    \n",
        "    for _, transaction in tqdm(transactions.iterrows(), total=total_transactions, desc=\"Edge Features\"):\n",
        "        # Temporal features\n",
        "        timestamp = pd.to_datetime(transaction['Timestamp'])\n",
        "        \n",
        "        # Cyclic temporal encoding\n",
        "        hour_sin = np.sin(2 * np.pi * timestamp.hour / 24)\n",
        "        hour_cos = np.cos(2 * np.pi * timestamp.hour / 24)\n",
        "        day_sin = np.sin(2 * np.pi * timestamp.dayofweek / 7)\n",
        "        day_cos = np.cos(2 * np.pi * timestamp.dayofweek / 7)\n",
        "        month_sin = np.sin(2 * np.pi * timestamp.month / 12)\n",
        "        month_cos = np.cos(2 * np.pi * timestamp.month / 12)\n",
        "        \n",
        "        # Amount features\n",
        "        amount_paid = transaction['Amount Paid']\n",
        "        amount_received = transaction['Amount Received']\n",
        "        \n",
        "        amount_paid_log = np.log1p(amount_paid)\n",
        "        amount_received_log = np.log1p(amount_received)\n",
        "        amount_ratio = amount_paid / max(amount_received, 1)\n",
        "        \n",
        "        # Categorical features\n",
        "        currency_encoded = currency_encoder.transform([transaction['Payment Currency']])[0]\n",
        "        format_encoded = format_encoder.transform([transaction['Payment Format']])[0]\n",
        "        bank_encoded = bank_encoder.transform([transaction['From Bank']])[0]\n",
        "        \n",
        "        # Combine all features\n",
        "        edge_feature = [\n",
        "            hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos,\n",
        "            amount_paid_log, amount_received_log, amount_ratio,\n",
        "            currency_encoded, format_encoded, bank_encoded\n",
        "        ]\n",
        "        \n",
        "        edge_features.append(edge_feature)\n",
        "        edge_labels.append(transaction['Is Laundering'])\n",
        "    \n",
        "    return np.array(edge_features), np.array(edge_labels)\n",
        "\n",
        "print(\"âœ“ Enhanced edge feature function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Imbalance Handling\n",
        "def handle_class_imbalance(X, y, strategy='smote'):\n",
        "    \"\"\"Handle class imbalance using multiple strategies\"\"\"\n",
        "    print(f\"Handling class imbalance using {strategy}...\")\n",
        "    \n",
        "    # Check class distribution\n",
        "    class_counts = np.bincount(y)\n",
        "    minority_count = min(class_counts)\n",
        "    majority_count = max(class_counts)\n",
        "    \n",
        "    print(f\"  - Original samples: {len(X)}\")\n",
        "    print(f\"  - Original distribution: {class_counts}\")\n",
        "    print(f\"  - Minority class: {minority_count} samples\")\n",
        "    print(f\"  - Majority class: {majority_count} samples\")\n",
        "    \n",
        "    if strategy == 'smote':\n",
        "        # Check if SMOTE can be applied\n",
        "        if minority_count < 2:\n",
        "            print(\"âš ï¸  Too few minority samples for SMOTE (need at least 2)\")\n",
        "            print(\"ðŸ”„ Falling back to cost-sensitive learning only\")\n",
        "            X_resampled, y_resampled = X, y\n",
        "            print(\"âœ“ Cost-sensitive learning applied (no resampling)\")\n",
        "            \n",
        "        elif minority_count < 4:\n",
        "            print(\"âš ï¸  Very few minority samples for SMOTE (need at least 4)\")\n",
        "            print(\"ðŸ”„ Using reduced k_neighbors for SMOTE\")\n",
        "            try:\n",
        "                smote = SMOTE(random_state=42, k_neighbors=min(3, minority_count-1))\n",
        "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "                print(f\"  - Resampled samples: {len(X_resampled)}\")\n",
        "                print(f\"  - Resampled distribution: {np.bincount(y_resampled)}\")\n",
        "                print(\"âœ“ SMOTE completed with reduced k_neighbors\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  SMOTE failed: {e}\")\n",
        "                print(\"ðŸ”„ Falling back to cost-sensitive learning only\")\n",
        "                X_resampled, y_resampled = X, y\n",
        "                print(\"âœ“ Cost-sensitive learning applied (no resampling)\")\n",
        "        else:\n",
        "            print(\"Applying SMOTE oversampling...\")\n",
        "            try:\n",
        "                smote = SMOTE(random_state=42, k_neighbors=3)\n",
        "                X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "                print(f\"  - Resampled samples: {len(X_resampled)}\")\n",
        "                print(f\"  - Resampled distribution: {np.bincount(y_resampled)}\")\n",
        "                print(\"âœ“ SMOTE completed\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  SMOTE failed: {e}\")\n",
        "                print(\"ðŸ”„ Falling back to cost-sensitive learning only\")\n",
        "                X_resampled, y_resampled = X, y\n",
        "                print(\"âœ“ Cost-sensitive learning applied (no resampling)\")\n",
        "        \n",
        "    elif strategy == 'none':\n",
        "        X_resampled, y_resampled = X, y\n",
        "        print(\"âœ“ No resampling applied\")\n",
        "    \n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "def create_cost_sensitive_weights(y):\n",
        "    \"\"\"Create cost-sensitive class weights\"\"\"\n",
        "    print(\"Creating cost-sensitive class weights...\")\n",
        "    \n",
        "    # Check class distribution\n",
        "    class_counts = np.bincount(y)\n",
        "    minority_count = min(class_counts)\n",
        "    majority_count = max(class_counts)\n",
        "    \n",
        "    print(f\"  - Class distribution: {class_counts}\")\n",
        "    print(f\"  - Minority class: {minority_count} samples\")\n",
        "    print(f\"  - Majority class: {majority_count} samples\")\n",
        "    \n",
        "    # Compute balanced class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    \n",
        "    # Additional cost for false negatives (missed illicit transactions)\n",
        "    # Use higher multiplier for extreme imbalance\n",
        "    if minority_count < 10:\n",
        "        cost_multiplier = 100.0  # Very high cost for extreme imbalance\n",
        "        print(f\"  - Extreme imbalance detected: using {cost_multiplier}x cost multiplier\")\n",
        "    elif minority_count < 100:\n",
        "        cost_multiplier = 50.0   # High cost for severe imbalance\n",
        "        print(f\"  - Severe imbalance detected: using {cost_multiplier}x cost multiplier\")\n",
        "    else:\n",
        "        cost_multiplier = 10.0   # Standard cost for moderate imbalance\n",
        "        print(f\"  - Moderate imbalance detected: using {cost_multiplier}x cost multiplier\")\n",
        "    \n",
        "    adjusted_weights = class_weights * cost_multiplier\n",
        "    \n",
        "    weight_dict = dict(zip(classes, adjusted_weights))\n",
        "    print(f\"  - Final class weights: {weight_dict}\")\n",
        "    \n",
        "    return weight_dict\n",
        "\n",
        "print(\"âœ“ Class imbalance handling functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main Preprocessing Pipeline\n",
        "def run_simple_preprocessing(data_path, sample_size=10000):\n",
        "    \"\"\"Run simplified preprocessing pipeline\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Simple Enhanced AML Preprocessing Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Load data\n",
        "    print(\"\\nðŸ“Š STEP 1: Loading Data\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    transactions, accounts = load_aml_data(data_path, sample_size)\n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Data loading completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Create enhanced features\n",
        "    print(\"\\nðŸ”§ STEP 2: Creating Node Features\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    node_features = create_enhanced_node_features(transactions, accounts)\n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Node features completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    print(\"\\nðŸ”§ STEP 3: Creating Edge Features\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    edge_features, edge_labels = create_enhanced_edge_features(transactions)\n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Edge features completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Normalize features\n",
        "    print(\"\\nðŸ“Š STEP 4: Normalizing Features\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    node_feature_matrix = np.array([list(features.values()) for features in node_features.values()])\n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Feature normalization completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Handle class imbalance\n",
        "    print(\"\\nâš–ï¸ STEP 5: Handling Class Imbalance\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    \n",
        "    # Try SMOTE first, fallback to cost-sensitive learning if it fails\n",
        "    try:\n",
        "        X_resampled, y_resampled = handle_class_imbalance(edge_features, edge_labels, strategy='smote')\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  SMOTE failed completely: {e}\")\n",
        "        print(\"ðŸ”„ Using cost-sensitive learning only\")\n",
        "        X_resampled, y_resampled = edge_features, edge_labels\n",
        "        print(\"âœ“ Cost-sensitive learning applied (no resampling)\")\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Class imbalance handling completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Create cost-sensitive weights\n",
        "    print(\"\\nðŸŽ¯ STEP 6: Creating Cost-Sensitive Weights\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    class_weights = create_cost_sensitive_weights(edge_labels)\n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Cost-sensitive weights completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    # Create graph structure\n",
        "    print(\"\\nðŸ•¸ï¸ STEP 7: Creating Graph Structure\")\n",
        "    print(\"-\" * 30)\n",
        "    step_start = time.time()\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add nodes with features\n",
        "    print(\"Adding nodes...\")\n",
        "    for account_id, features in node_features.items():\n",
        "        G.add_node(account_id, **features)\n",
        "    \n",
        "    # Add edges with features\n",
        "    print(\"Adding edges...\")\n",
        "    from tqdm import tqdm\n",
        "    for i, (_, transaction) in enumerate(tqdm(transactions.iterrows(), total=len(transactions), desc=\"Adding Edges\")):\n",
        "        from_account = transaction['Account']\n",
        "        to_account = transaction['Account.1']\n",
        "        \n",
        "        if from_account in G.nodes() and to_account in G.nodes():\n",
        "            G.add_edge(from_account, to_account, \n",
        "                      features=edge_features[i], \n",
        "                      label=edge_labels[i])\n",
        "    \n",
        "    step_time = time.time() - step_start\n",
        "    print(f\"âœ“ Graph structure completed in {step_time:.2f} seconds\")\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nðŸŽ‰ Preprocessing completed successfully!\")\n",
        "    print(f\"â±ï¸ Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "    print(f\"ðŸ“Š Results:\")\n",
        "    print(f\"  - Nodes: {G.number_of_nodes()}\")\n",
        "    print(f\"  - Edges: {G.number_of_edges()}\")\n",
        "    print(f\"  - Node features: {node_feature_matrix.shape[1]}\")\n",
        "    print(f\"  - Edge features: {edge_features.shape[1]}\")\n",
        "    print(f\"  - Class distribution: {np.bincount(edge_labels)}\")\n",
        "    \n",
        "    return G, node_features, edge_features, edge_labels, class_weights\n",
        "\n",
        "print(\"âœ“ Main preprocessing pipeline defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Simple Enhanced Preprocessing\n",
        "print(\"Starting simple enhanced preprocessing...\")\n",
        "\n",
        "# Configuration\n",
        "data_path = \"/content/drive/MyDrive/LaunDetection/data/raw\"\n",
        "sample_size = 10000  # Start with sample for testing\n",
        "\n",
        "# Show time estimation\n",
        "estimated_time = estimate_processing_time(sample_size)\n",
        "print(f\"ðŸ“Š Sample size: {sample_size:,} transactions\")\n",
        "print(f\"â±ï¸ Estimated processing time: {estimated_time}\")\n",
        "print(f\"ðŸ’¾ Expected memory usage: ~2-4 GB\")\n",
        "print(f\"ðŸ”§ Features to create: 15 node features + 12 edge features per transaction\")\n",
        "\n",
        "try:\n",
        "    # Run preprocessing\n",
        "    G, node_features, edge_features, edge_labels, class_weights = run_simple_preprocessing(\n",
        "        data_path, sample_size\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Preprocessing Summary\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"âœ“ Graph created: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "    print(f\"âœ“ Node features: {len(node_features)} accounts with enhanced features\")\n",
        "    print(f\"âœ“ Edge features: {edge_features.shape[0]} transactions with {edge_features.shape[1]} features\")\n",
        "    print(f\"âœ“ Class weights: {class_weights}\")\n",
        "    \n",
        "    # Show sample features\n",
        "    print(f\"\\nSample node features for first account:\")\n",
        "    first_account = list(node_features.keys())[0]\n",
        "    print(f\"Account: {first_account}\")\n",
        "    for key, value in list(node_features[first_account].items())[:5]:\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    print(f\"\\nSample edge features:\")\n",
        "    print(f\"  Temporal features: {edge_features[0][:6]}\")\n",
        "    print(f\"  Amount features: {edge_features[0][6:9]}\")\n",
        "    print(f\"  Categorical features: {edge_features[0][9:12]}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âœ— Error during preprocessing: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
